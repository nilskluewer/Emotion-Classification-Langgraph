{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T12:53:00.051205Z",
     "start_time": "2024-08-22T12:53:00.011965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import tiktoken\n",
    "from typing import Dict, List, Optional, Tuple, Set\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, file_path: str):\n",
    "        \"\"\"\n",
    "        Initialize the DataPreprocessor with the file path.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the CSV file containing the data.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.df = None\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\"\n",
    "        Process the data by loading it from the CSV file and applying various preprocessing steps.\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(self.file_path)\n",
    "        self._convert_dates()\n",
    "        self._handle_missing_values()\n",
    "        self._convert_id_columns()\n",
    "        self._create_new_features()\n",
    "\n",
    "    def _convert_dates(self):\n",
    "        \"\"\"\n",
    "        Convert date columns to datetime format.\n",
    "        \"\"\"\n",
    "        date_columns = ['PostingCreatedAt', 'ArticlePublishingDate', 'UserCreatedAt']\n",
    "        for col in date_columns:\n",
    "            self.df[col] = pd.to_datetime(self.df[col])\n",
    "\n",
    "    def _handle_missing_values(self):\n",
    "        \"\"\"\n",
    "        Handle missing values in the data by filling them with appropriate values.\n",
    "        \"\"\"\n",
    "        self.df['PostingHeadline'] = self.df['PostingHeadline'].fillna('No Headline')\n",
    "        self.df['PostingComment'] = self.df['PostingComment'].fillna('No Comment')\n",
    "        self.df['UserGender'] = self.df['UserGender'].fillna('Unknown')\n",
    "        self.df['UserCommunityName'] = self.df['UserCommunityName'].fillna('Unknown')\n",
    "\n",
    "    def _convert_id_columns(self):\n",
    "        \"\"\"\n",
    "        Convert ID columns to integer type.\n",
    "        \"\"\"\n",
    "        id_columns = ['ID_Posting', 'ID_Posting_Parent', 'ID_CommunityIdentity', 'ID_Article']\n",
    "        for col in id_columns:\n",
    "            self.df[col] = self.df[col].fillna(0).astype(int)\n",
    "\n",
    "    def _create_new_features(self):\n",
    "        \"\"\"\n",
    "        Create new features based on existing columns.\n",
    "        \"\"\"\n",
    "        self.df['CommentLength'] = self.df['PostingComment'].str.len()\n",
    "        self.df['DaysSinceUserCreation'] = (self.df['PostingCreatedAt'] - self.df['UserCreatedAt']).dt.days\n",
    "        self.df['IsReply'] = self.df['ID_Posting_Parent'] != 0\n",
    "        self.df['PostingHour'] = self.df['PostingCreatedAt'].dt.hour\n",
    "        self.df['PostingDayOfWeek'] = self.df['PostingCreatedAt'].dt.dayofweek\n",
    "\n",
    "    def save_preprocessed_data(self, output_path: str):\n",
    "        \"\"\"\n",
    "        Save the preprocessed data to a pickle file.\n",
    "\n",
    "        Args:\n",
    "            output_path (str): The path where the preprocessed data will be saved.\n",
    "        \"\"\"\n",
    "        with open(output_path, 'wb') as f:\n",
    "            pickle.dump(self.df, f)\n",
    "        print(f\"Preprocessed data saved to {output_path}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load_preprocessed_data(cls, input_path: str):\n",
    "        \"\"\"\n",
    "        Load the preprocessed data from a pickle file.\n",
    "\n",
    "        Args:\n",
    "            input_path (str): The path to the pickle file containing the preprocessed data.\n",
    "\n",
    "        Returns:\n",
    "            DataPreprocessor: An instance of DataPreprocessor with the loaded data.\n",
    "        \"\"\"\n",
    "        with open(input_path, 'rb') as f:\n",
    "            df = pickle.load(f)\n",
    "        preprocessor = cls(None)\n",
    "        preprocessor.df = df\n",
    "        print(f\"Preprocessed data loaded from {input_path}\")\n",
    "        return preprocessor\n",
    "\n",
    "class CommentThreadManager:\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Initialize the CommentThreadManager with the preprocessed data.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The preprocessed data containing comment information.\n",
    "        \"\"\"\n",
    "        self.article_comments = {article_id: group for article_id, group in df.groupby('ID_Article')}\n",
    "\n",
    "    def build_comment_thread(self, comments: pd.DataFrame, parent_id: int, depth: int = 0) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Build a hierarchical structure of comments and their replies.\n",
    "\n",
    "        Args:\n",
    "            comments (pd.DataFrame): The comments data for a specific article.\n",
    "            parent_id (int): The ID of the parent comment.\n",
    "            depth (int): The depth of the current comment in the thread.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict]: A list of dictionaries representing the comment thread.\n",
    "        \"\"\"\n",
    "        replies = comments[comments['ID_Posting_Parent'] == parent_id]\n",
    "        return [{\n",
    "            'id': int(reply['ID_Posting']),\n",
    "            'parent_id': int(reply['ID_Posting_Parent']) if pd.notnull(reply['ID_Posting_Parent']) else None,\n",
    "            'user_id': int(reply['ID_CommunityIdentity']),\n",
    "            'user_name': reply['UserCommunityName'],\n",
    "            'user_gender': reply['UserGender'],\n",
    "            'user_created_at': reply['UserCreatedAt'].isoformat() if pd.notnull(reply['UserCreatedAt']) else None,\n",
    "            'comment_headline': reply['PostingHeadline'],\n",
    "            'comment_text': reply['PostingComment'],\n",
    "            'comment_created_at': reply['PostingCreatedAt'].isoformat() if pd.notnull(reply['PostingCreatedAt']) else None,\n",
    "            'comment_length': int(reply['CommentLength']),\n",
    "            'depth': depth,\n",
    "            'replies': self.build_comment_thread(comments, int(reply['ID_Posting']), depth + 1)\n",
    "        } for _, reply in replies.iterrows()]\n",
    "\n",
    "    def get_article_threads(self, article_id: int) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Get the comment threads for a specific article.\n",
    "\n",
    "        Args:\n",
    "            article_id (int): The ID of the article.\n",
    "\n",
    "        Returns:\n",
    "            Optional[Dict]: A dictionary representing the article's comment threads, or None if the article is not found.\n",
    "        \"\"\"\n",
    "        if article_id not in self.article_comments:\n",
    "            return None\n",
    "\n",
    "        article_df = self.article_comments[article_id]\n",
    "        root_comments = article_df[article_df['ID_Posting_Parent'].isnull() | (article_df['ID_Posting_Parent'] == 0)]\n",
    "\n",
    "        threads = self.build_comment_thread(article_df, 0)\n",
    "        article_meta = article_df.iloc[0]\n",
    "\n",
    "        return {\n",
    "            'article_id': int(article_id),\n",
    "            'article_title': article_meta['ArticleTitle'],\n",
    "            'article_publish_date': article_meta['ArticlePublishingDate'].isoformat() if pd.notnull(article_meta['ArticlePublishingDate']) else None,\n",
    "            'article_channel': article_meta['ArticleChannel'],\n",
    "            'article_ressort_name': article_meta['ArticleRessortName'],\n",
    "            'total_comments': len(article_df),\n",
    "            'root_comments': len(root_comments),\n",
    "            'comment_threads': threads\n",
    "        }\n",
    "\n",
    "    def get_article_ids(self) -> List[int]:\n",
    "        \"\"\"\n",
    "        Get the list of article IDs.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: A list of article IDs.\n",
    "        \"\"\"\n",
    "        return list(self.article_comments.keys())\n",
    "\n",
    "class UserContextSphere:\n",
    "    def __init__(self, df: pd.DataFrame, thread_manager: CommentThreadManager):\n",
    "        self.df = df\n",
    "        self.thread_manager = thread_manager\n",
    "        self.user_comments = {user_id: group for user_id, group in df.groupby('ID_CommunityIdentity')}\n",
    "\n",
    "\n",
    "    def get_user_context(self, user_id: int) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Get the context information for a specific user, including only the threads where the user has written a comment.\n",
    "    \n",
    "        Args:\n",
    "            user_id (int): The ID of the user.\n",
    "    \n",
    "        Returns:\n",
    "            Optional[Dict]: A dictionary representing the user's context, or None if the user is not found.\n",
    "        \"\"\"\n",
    "        if user_id not in self.user_comments:\n",
    "            return None\n",
    "    \n",
    "        user_df = self.user_comments[user_id]\n",
    "        total_comments = len(user_df)\n",
    "        total_replies = len(user_df[user_df['ID_Posting_Parent'].notnull()])\n",
    "    \n",
    "        user_context = {\n",
    "            'user_id': int(user_id),\n",
    "            'user_name': user_df['UserCommunityName'].iloc[0],\n",
    "            'user_gender': user_df['UserGender'].iloc[0],\n",
    "            'user_created_at': user_df['UserCreatedAt'].iloc[0].isoformat(),\n",
    "            'total_comments': total_comments,\n",
    "            'total_replies': total_replies,\n",
    "            'articles': {}\n",
    "        }\n",
    "    \n",
    "        for article_id, article_comments in user_df.groupby('ID_Article'):\n",
    "            article_id = int(article_id)\n",
    "            article_thread = self.thread_manager.get_article_threads(article_id)\n",
    "    \n",
    "            if article_thread:\n",
    "                user_threads = []\n",
    "                for _, comment in article_comments.iterrows():\n",
    "                    thread = self.find_thread_for_comment(article_thread['comment_threads'], int(comment['ID_Posting']))\n",
    "                    if thread:\n",
    "                        user_threads.append(thread)\n",
    "    \n",
    "                if user_threads:\n",
    "                    user_context['articles'][article_id] = {\n",
    "                        'article_title': article_thread['article_title'],\n",
    "                        'article_publish_date': article_thread['article_publish_date'],\n",
    "                        'user_comments_count': len(article_comments),\n",
    "                        'user_replies_count': len(article_comments[article_comments['ID_Posting_Parent'].notnull()]),\n",
    "                        'threads': user_threads\n",
    "                    }\n",
    "    \n",
    "        return user_context\n",
    "\n",
    "    def find_thread_for_comment(self, threads: List[Dict], comment_id: int) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Find the thread that contains a specific comment.\n",
    "\n",
    "        Args:\n",
    "            threads (List[Dict]): A list of comment threads.\n",
    "            comment_id (int): The ID of the comment to find.\n",
    "\n",
    "        Returns:\n",
    "            Optional[Dict]: The thread containing the comment, or None if the comment is not found.\n",
    "        \"\"\"\n",
    "        for thread in threads:\n",
    "            if thread['id'] == comment_id:\n",
    "                return thread\n",
    "            result = self.find_thread_for_comment(thread['replies'], comment_id)\n",
    "            if result:\n",
    "                return thread\n",
    "        return None\n",
    "\n",
    "\n",
    "    def filter_user_threads(self, user_context: Dict, user_id: int) -> Tuple[Dict, int]:\n",
    "        removed_threads = 0\n",
    "        filtered_context = user_context.copy()\n",
    "        filtered_context['articles'] = {}\n",
    "    \n",
    "        for article_id, article_data in user_context['articles'].items():\n",
    "            user_threads = []\n",
    "            for thread in article_data['threads']:\n",
    "                filtered_thread = self.filter_thread_for_user(thread, user_id)\n",
    "                if filtered_thread:\n",
    "                    user_threads.append(filtered_thread)\n",
    "                else:\n",
    "                    removed_threads += 1\n",
    "    \n",
    "            if user_threads:\n",
    "                filtered_context['articles'][article_id] = article_data.copy()\n",
    "                filtered_context['articles'][article_id]['threads'] = user_threads\n",
    "                filtered_context['articles'][article_id]['user_comments_count'] = sum(self.count_user_comments(thread, user_id) for thread in user_threads)\n",
    "            else:\n",
    "                removed_threads += 1\n",
    "    \n",
    "        return filtered_context, removed_threads\n",
    "\n",
    "    def filter_thread_for_user(self, thread: Dict, user_id: int) -> Optional[Dict]:\n",
    "        if thread['user_id'] == user_id:\n",
    "            filtered_replies = [self.filter_thread_for_user(reply, user_id) for reply in thread['replies']]\n",
    "            filtered_replies = [reply for reply in filtered_replies if reply]\n",
    "            filtered_thread = thread.copy()\n",
    "            filtered_thread['replies'] = filtered_replies\n",
    "            return filtered_thread\n",
    "        else:\n",
    "            for reply in thread['replies']:\n",
    "                filtered_reply = self.filter_thread_for_user(reply, user_id)\n",
    "                if filtered_reply:\n",
    "                    return thread.copy()\n",
    "        return None\n",
    "\n",
    "    def count_user_comments(self, thread: Dict, user_id: int) -> int:\n",
    "        count = 1 if thread['user_id'] == user_id else 0\n",
    "        for reply in thread['replies']:\n",
    "            count += self.count_user_comments(reply, user_id)\n",
    "        return count\n",
    "\n",
    "    def user_has_comment_in_thread(self, thread: Dict, user_id: int) -> bool:\n",
    "        \"\"\"\n",
    "        Check if the user has a comment in the given thread or its replies.\n",
    "\n",
    "        Args:\n",
    "            thread (Dict): The thread to check.\n",
    "            user_id (int): The ID of the user.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the user has a comment in the thread, False otherwise.\n",
    "        \"\"\"\n",
    "        if thread['user_id'] == user_id:\n",
    "            return True\n",
    "        for reply in thread['replies']:\n",
    "            if self.user_has_comment_in_thread(reply, user_id):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def generate_user_report_with_cutoff(self, user_id: int) -> Tuple[str, int, int]:\n",
    "        \"\"\"\n",
    "        Generate a user report with cutoff, keeping only threads where the user has commented.\n",
    "\n",
    "        Args:\n",
    "            user_id (int): The ID of the user.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[str, int, int]: A tuple containing the user report, token count, and the number of removed threads.\n",
    "        \"\"\"\n",
    "        user_context = self.get_user_context(user_id)\n",
    "        if not user_context:\n",
    "            return f\"No data found for user ID {user_id}\", 0, 0\n",
    "\n",
    "        filtered_context, removed_threads = self.filter_user_threads(user_context, user_id)\n",
    "\n",
    "        report = json.dumps(filtered_context, indent=2)\n",
    "        token_count = self.count_tokens(report)\n",
    "\n",
    "        return report, token_count, removed_threads\n",
    "\n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"\n",
    "        Count the number of tokens in a given text.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of tokens in the text.\n",
    "        \"\"\"\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        tokens_lst = encoding.encode(text)\n",
    "        return len(tokens_lst)"
   ],
   "id": "daa4e0d56b24bee5",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T12:40:20.049360Z",
     "start_time": "2024-08-22T12:40:19.579614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Main execution\n",
    "preprocessed_file = \"../data/preprocessed/preprocessed_data.pkl\"\n",
    "\n",
    "if not os.path.exists(preprocessed_file):\n",
    "    print(\"Preprocessed data not found. Preprocessing...\")\n",
    "    preprocessor = DataPreprocessor('./data/raw_csv/Postings_01052019_31052019.csv')\n",
    "    preprocessor.process()\n",
    "    preprocessor.save_preprocessed_data(preprocessed_file)\n",
    "else:\n",
    "    print(\"Loading preprocessed data...\")\n",
    "    preprocessor = DataPreprocessor.load_preprocessed_data(preprocessed_file)"
   ],
   "id": "3a39da486099f4a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data...\n",
      "Preprocessed data loaded from ../data/preprocessed/preprocessed_data.pkl\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T12:53:08.561487Z",
     "start_time": "2024-08-22T12:53:03.923599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "thread_manager = CommentThreadManager(preprocessor.df)\n",
    "user_context_sphere = UserContextSphere(preprocessor.df, thread_manager)\n",
    "\n",
    "# Create the spheres directories if they don't exist\n",
    "spheres_dir_no_cutoff = \"spheres/no_cutoff\"\n",
    "spheres_dir_cutoff = \"spheres/cutoff\"\n",
    "os.makedirs(spheres_dir_no_cutoff, exist_ok=True)\n",
    "os.makedirs(spheres_dir_cutoff, exist_ok=True)\n",
    "\n",
    "user_id = 22231  # Replace with the desired user ID\n",
    "\n",
    "# Generate and save context without cutoff\n",
    "user_context_no_cutoff = user_context_sphere.get_user_context(user_id)\n",
    "if user_context_no_cutoff:\n",
    "    token_count_no_cutoff = user_context_sphere.count_tokens(json.dumps(user_context_no_cutoff))\n",
    "    user_context_no_cutoff['token_count'] = token_count_no_cutoff\n",
    "\n",
    "    filename_no_cutoff = f\"{spheres_dir_no_cutoff}/{user_id}.json\"\n",
    "    with open(filename_no_cutoff, 'w', encoding='utf-8') as f:\n",
    "        json.dump(user_context_no_cutoff, f, indent=2)\n",
    "    print(f\"User context without cutoff saved to {filename_no_cutoff}\")\n",
    "\n",
    "    # Generate and save context with cutoff\n",
    "    # Generate and save context with cutoff\n",
    "    user_report_cutoff, token_count_cutoff, removed_threads = user_context_sphere.generate_user_report_with_cutoff(user_id)\n",
    "    user_context_cutoff = json.loads(user_report_cutoff)\n",
    "    user_context_cutoff['token_count'] = token_count_cutoff\n",
    "    user_context_cutoff['removed_threads'] = removed_threads\n",
    "    \n",
    "    filename_cutoff = f\"{spheres_dir_cutoff}/{user_id}.json\"\n",
    "    with open(filename_cutoff, 'w', encoding='utf-8') as f:\n",
    "        json.dump(user_context_cutoff, f, indent=2)\n",
    "    print(f\"User context with cutoff saved to {filename_cutoff}\")\n",
    "    \n",
    "    print(f\"Token count without cutoff: {token_count_no_cutoff}\")\n",
    "    print(f\"Token count with cutoff: {token_count_cutoff}\")\n",
    "    print(f\"Removed threads: {removed_threads}\")\n",
    "else:\n",
    "    error_message = {\"error\": f\"No data found for user ID {user_id}\"}\n",
    "\n",
    "    # Save error message to both directories\n",
    "    for dir_path in [spheres_dir_no_cutoff, spheres_dir_cutoff]:\n",
    "        filename = f\"{dir_path}/{user_id}_error.json\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(error_message, f, indent=2)\n",
    "        print(f\"Error message saved to {filename}\")"
   ],
   "id": "4e968013fc06201",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User context without cutoff saved to spheres/no_cutoff/22231.json\n",
      "User context with cutoff saved to spheres/cutoff/22231.json\n",
      "Token count without cutoff: 17528\n",
      "Token count with cutoff: 18842\n",
      "Removed threads: 0\n"
     ]
    }
   ],
   "execution_count": 30
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
