{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-22T10:28:16.954168Z",
     "start_time": "2024-08-22T10:28:16.578713Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import tiktoken\n",
    "from typing import Dict, List, Tuple\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n",
    "from collections import defaultdict\n",
    "from data_preprocessor import DataPreprocessor\n",
    "\n",
    "'''\n",
    "The modified cutoff_after_last_interaction method now better aligns with your requirements. Here's how it works:\n",
    "\n",
    "a. We first traverse all threads to find the user's last interaction time.\n",
    "b. We then remove any comments (including root comments) that occur after the user's last interaction.\n",
    "c. This approach ensures that the context sphere only contains information up to the user's last interaction, assuming that anything after that should not be part of the context.\n",
    "\n",
    "This is a good approach because:\n",
    "- It preserves the chronological context of the user's interactions.\n",
    "- It removes potentially irrelevant information that the user hasn't seen or interacted with.\n",
    "- It helps in creating a more focused and relevant context sphere for the user.\n",
    "'''\n",
    "\n",
    "class CommentThreadManager:\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.article_comments = {article_id: group for article_id, group in df.groupby('ID_Article')}\n",
    "\n",
    "    def build_comment_thread(self, comments: pd.DataFrame, parent_id: int, depth: int = 0) -> List[Dict]:\n",
    "        replies = comments[comments['ID_Posting_Parent'] == parent_id]\n",
    "        return [{\n",
    "            'id': int(reply['ID_Posting']),\n",
    "            'parent_id': int(reply['ID_Posting_Parent']) if pd.notnull(reply['ID_Posting_Parent']) else None,\n",
    "            'user_id': int(reply['ID_CommunityIdentity']),\n",
    "            'user_name': reply['UserCommunityName'],\n",
    "            'user_gender': reply['UserGender'],\n",
    "            'user_created_at': reply['UserCreatedAt'].isoformat() if pd.notnull(reply['UserCreatedAt']) else None,\n",
    "            'comment_headline': reply['PostingHeadline'],\n",
    "            'comment_text': reply['PostingComment'],\n",
    "            'comment_created_at': reply['PostingCreatedAt'].isoformat() if pd.notnull(reply['PostingCreatedAt']) else None,\n",
    "            'comment_length': int(reply['CommentLength']),\n",
    "            'depth': depth,\n",
    "            'replies': self.build_comment_thread(comments, int(reply['ID_Posting']), depth + 1)\n",
    "        } for _, reply in replies.iterrows()]\n",
    "\n",
    "    def get_article_threads(self, article_id: int) -> Dict:\n",
    "        if article_id not in self.article_comments:\n",
    "            return None\n",
    "\n",
    "        article_df = self.article_comments[article_id]\n",
    "        root_comments = article_df[article_df['ID_Posting_Parent'].isnull() | (article_df['ID_Posting_Parent'] == 0)]\n",
    "\n",
    "        threads = self.build_comment_thread(article_df, 0)\n",
    "        article_meta = article_df.iloc[0]\n",
    "\n",
    "        return {\n",
    "            'article_id': int(article_id),\n",
    "            'article_title': article_meta['ArticleTitle'],\n",
    "            'article_publish_date': article_meta['ArticlePublishingDate'].isoformat() if pd.notnull(article_meta['ArticlePublishingDate']) else None,\n",
    "            'article_channel': article_meta['ArticleChannel'],\n",
    "            'article_ressort_name': article_meta['ArticleRessortName'],\n",
    "            'total_comments': len(article_df),\n",
    "            'root_comments': len(root_comments),\n",
    "            'comment_threads': threads\n",
    "        }\n",
    "\n",
    "class UserContextSphere:\n",
    "    def __init__(self, df: pd.DataFrame, thread_manager: CommentThreadManager):\n",
    "        self.df = df\n",
    "        self.thread_manager = thread_manager\n",
    "        self.user_comments = defaultdict(list)\n",
    "        self._populate_user_comments()\n",
    "\n",
    "    def _populate_user_comments(self):\n",
    "        for _, row in self.df.iterrows():\n",
    "            self.user_comments[row['ID_CommunityIdentity']].append(row)\n",
    "\n",
    "    def get_user_context(self, user_id: int) -> Dict:\n",
    "        if user_id not in self.user_comments:\n",
    "            return None\n",
    "\n",
    "        user_df = pd.DataFrame(self.user_comments[user_id])\n",
    "        total_comments = len(user_df)\n",
    "        total_replies = len(user_df[user_df['ID_Posting_Parent'].notnull()])\n",
    "\n",
    "        user_context = {\n",
    "            'user_id': int(user_id),\n",
    "            'user_name': user_df['UserCommunityName'].iloc[0],\n",
    "            'user_gender': user_df['UserGender'].iloc[0],\n",
    "            'user_created_at': user_df['UserCreatedAt'].iloc[0].isoformat(),\n",
    "            'total_comments': total_comments,\n",
    "            'total_replies': total_replies,\n",
    "            'articles': {}\n",
    "        }\n",
    "\n",
    "        for article_id, article_comments in user_df.groupby('ID_Article'):\n",
    "            article_id = int(article_id)\n",
    "            article_thread = self.thread_manager.get_article_threads(article_id)\n",
    "\n",
    "            if article_thread:\n",
    "                user_context['articles'][article_id] = {\n",
    "                    'article_title': article_thread['article_title'],\n",
    "                    'article_publish_date': article_thread['article_publish_date'],\n",
    "                    'user_comments_count': len(article_comments),\n",
    "                    'user_replies_count': len(article_comments[article_comments['ID_Posting_Parent'].notnull()]),\n",
    "                    'threads': [self.find_thread_for_comment(article_thread['comment_threads'], int(comment['ID_Posting']))\n",
    "                                for _, comment in article_comments.iterrows()]\n",
    "                }\n",
    "\n",
    "        return user_context\n",
    "\n",
    "    def find_thread_for_comment(self, threads: List[Dict], comment_id: int) -> Dict:\n",
    "        for thread in threads:\n",
    "            if thread['id'] == comment_id:\n",
    "                return thread\n",
    "            result = self.find_thread_for_comment(thread['replies'], comment_id)\n",
    "            if result:\n",
    "                return thread\n",
    "        return None\n",
    "\n",
    "    def cutoff_after_last_interaction(self, user_context: Dict, user_id: int) -> Tuple[Dict, int]:\n",
    "        removed_comments = 0\n",
    "        last_interaction_time = None\n",
    "\n",
    "        def process_thread(thread: Dict) -> Tuple[Dict, int]:\n",
    "            nonlocal removed_comments, last_interaction_time\n",
    "            if thread is None or 'comment_created_at' not in thread:\n",
    "                return None, 1  # Remove this thread as it's invalid\n",
    "\n",
    "            thread_time = pd.to_datetime(thread['comment_created_at'])\n",
    "            if last_interaction_time and thread_time > last_interaction_time:\n",
    "                return None, 1  # Remove this thread and all its replies\n",
    "\n",
    "            if thread['user_id'] == user_id:\n",
    "                if last_interaction_time is None or thread_time > last_interaction_time:\n",
    "                    last_interaction_time = thread_time\n",
    "\n",
    "            new_replies = []\n",
    "            for reply in thread.get('replies', []):\n",
    "                processed_reply, removed_count = process_thread(reply)\n",
    "                removed_comments += removed_count\n",
    "                if processed_reply:\n",
    "                    new_replies.append(processed_reply)\n",
    "\n",
    "            thread['replies'] = new_replies\n",
    "            return thread, 0\n",
    "\n",
    "        for article_id in user_context['articles']:\n",
    "            new_threads = []\n",
    "            for thread in user_context['articles'][article_id]['threads']:\n",
    "                processed_thread, _ = process_thread(thread)\n",
    "                if processed_thread:\n",
    "                    new_threads.append(processed_thread)\n",
    "\n",
    "            user_context['articles'][article_id]['threads'] = new_threads\n",
    "\n",
    "        # Remove comments after the last interaction\n",
    "        if last_interaction_time:\n",
    "            for article_id in user_context['articles']:\n",
    "                user_context['articles'][article_id]['threads'] = [\n",
    "                    thread for thread in user_context['articles'][article_id]['threads']\n",
    "                    if pd.to_datetime(thread['comment_created_at']) <= last_interaction_time\n",
    "                ]\n",
    "                removed_comments += len(user_context['articles'][article_id]['threads'])\n",
    "\n",
    "        return user_context, removed_comments\n",
    "\n",
    "\n",
    "    def escape_markdown(self, text: str) -> str:\n",
    "        escape_chars = ['\\\\', '`', '*', '_', '{', '}', '[', ']', '(', ')', '#', '+', '-', '.', '!']\n",
    "        for char in escape_chars:\n",
    "            text = text.replace(char, '\\\\' + char)\n",
    "        return text.replace('`', \"'\")\n",
    "\n",
    "    def format_comment_thread(self, comment: Dict) -> str:\n",
    "        root = ET.Element(\"comment_thread\")\n",
    "        self.add_comment_to_xml(root, comment)\n",
    "        xml_str = ET.tostring(root, encoding='unicode')\n",
    "        pretty_xml = minidom.parseString(xml_str).toprettyxml(indent=\"  \")\n",
    "        return pretty_xml\n",
    "\n",
    "    def add_comment_to_xml(self, parent: ET.Element, comment: Dict):\n",
    "        comment_elem = ET.SubElement(parent, \"comment\")\n",
    "        author_elem = ET.SubElement(comment_elem, \"author\")\n",
    "        author_elem.text = self.escape_markdown(comment['user_name'])\n",
    "        content_elem = ET.SubElement(comment_elem, \"content\")\n",
    "        content_elem.text = self.escape_markdown(comment['comment_text'])\n",
    "        if comment['replies']:\n",
    "            replies_elem = ET.SubElement(comment_elem, \"replies\")\n",
    "            for reply in comment['replies']:\n",
    "                self.add_comment_to_xml(replies_elem, reply)\n",
    "\n",
    "    def generate_formatted_user_context(self, user_id: int) -> str:\n",
    "        user_context = self.get_user_context(user_id)\n",
    "        if not user_context:\n",
    "            return f\"No data found for user ID {user_id}\"\n",
    "\n",
    "        output = []\n",
    "\n",
    "        output.append(\"# User Context\\n\")\n",
    "        output.append(f\"- **User ID:** {user_context['user_id']}\\n\")\n",
    "        output.append(f\"- **Username:** {self.escape_markdown(user_context['user_name'])}\\n\")\n",
    "        output.append(f\"- **Gender:** {self.escape_markdown(user_context['user_gender'])}\\n\")\n",
    "        output.append(f\"- **Created At:** {user_context['user_created_at']}\\n\")\n",
    "        output.append(f\"- **Total Comments:** {user_context['total_comments']}\\n\")\n",
    "        output.append(f\"- **Total Replies:** {user_context['total_replies']}\\n\")\n",
    "        output.append(\"\\n---\\n\\n\")\n",
    "\n",
    "        for article_id, article_data in user_context['articles'].items():\n",
    "            output.append(\"# Article Context\\n\")\n",
    "            output.append(f\"- **Article ID:** {article_id}\\n\")\n",
    "            output.append(f\"- **Article Title:** {self.escape_markdown(article_data['article_title'])}\\n\")\n",
    "            output.append(f\"- **Article Publish Date:** {article_data['article_publish_date']}\\n\")\n",
    "            output.append(f\"- **User Comments Count:** {article_data['user_comments_count']}\\n\")\n",
    "            output.append(f\"- **User Replies Count:** {article_data['user_replies_count']}\\n\")\n",
    "            output.append(\"\\n---\\n\\n\")\n",
    "\n",
    "            output.append(\"# Comment Threads\\n\\n\")\n",
    "            for i, thread in enumerate(article_data['threads'], 1):\n",
    "                output.append(f\"## Thread {i}\\n\\n\")\n",
    "                output.append(self.format_comment_thread(thread))\n",
    "                output.append(\"\\n---\\n\\n\")\n",
    "\n",
    "        output.append(\"# End of Context\")\n",
    "        return \"\".join(output)\n",
    "\n",
    "    def generate_user_report_with_cutoff(self, user_id: int) -> Tuple[str, int, int]:\n",
    "        user_context = self.get_user_context(user_id)\n",
    "        if not user_context:\n",
    "            return f\"No data found for user ID {user_id}\", 0, 0\n",
    "\n",
    "        user_context, removed_comments = self.cutoff_after_last_interaction(user_context, user_id)\n",
    "\n",
    "        report = self.generate_formatted_user_context(user_id)\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        token_count = len(encoding.encode(report))\n",
    "\n",
    "        return report, token_count, removed_comments\n",
    "\n",
    "    def find_users_with_few_comments(self, min_comments: int = 1, max_comments: int = 5) -> List[int]:\n",
    "        user_comment_counts = self.df['ID_CommunityIdentity'].value_counts()\n",
    "        users_with_few_comments = user_comment_counts[\n",
    "            (user_comment_counts >= min_comments) & (user_comment_counts <= max_comments)\n",
    "            ].index.tolist()\n",
    "        return users_with_few_comments"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T10:29:04.786155Z",
     "start_time": "2024-08-22T10:29:03.964172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load File\n",
    "preprocessed_file = \"../data/preprocessed/preprocessed_data.pkl\"\n",
    "\n",
    "if not os.path.exists(preprocessed_file):\n",
    "    print(\"Preprocessed data not found. Preprocessing...\")\n",
    "    preprocessor = DataPreprocessor('../data/raw_csv/Postings_01052019_31052019.csv')\n",
    "    preprocessor.process()\n",
    "    with open(preprocessed_file, 'wb') as f:\n",
    "        pickle.dump(preprocessor.df, f)\n",
    "    print(f\"Preprocessed data saved to {preprocessed_file}\")\n",
    "else:\n",
    "    print(\"Loading preprocessed data...\")\n",
    "    preprocessor = DataPreprocessor.load_preprocessed_data(preprocessed_file)"
   ],
   "id": "5638c993d7e141ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data...\n",
      "Preprocessed data loaded from ../data/preprocessed/preprocessed_data.pkl\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T10:31:36.566056Z",
     "start_time": "2024-08-22T10:29:26.462223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Main execution\n",
    "thread_manager = CommentThreadManager(preprocessor.df)\n",
    "user_context_sphere = UserContextSphere(preprocessor.df, thread_manager)\n",
    "\n",
    "spheres_dir_no_cutoff = \"spheres/no_cutoff\"\n",
    "spheres_dir_cutoff = \"spheres/cutoff\"\n",
    "os.makedirs(spheres_dir_no_cutoff, exist_ok=True)\n",
    "os.makedirs(spheres_dir_cutoff, exist_ok=True)\n",
    "\n",
    "user_id = 26373  # Replace with the desired user ID\n",
    "\n",
    "formatted_context = user_context_sphere.generate_formatted_user_context(user_id)\n",
    "\n",
    "if formatted_context != f\"No data found for user ID {user_id}\":\n",
    "    filename_no_cutoff = f\"{spheres_dir_no_cutoff}/{user_id}.md\"\n",
    "    with open(filename_no_cutoff, 'w', encoding='utf-8') as f:\n",
    "        f.write(formatted_context)\n",
    "    print(f\"User context without cutoff saved to {filename_no_cutoff}\")\n",
    "\n",
    "    report, token_count, removed_comments = user_context_sphere.generate_user_report_with_cutoff(user_id)\n",
    "\n",
    "    filename_cutoff = f\"{spheres_dir_cutoff}/{user_id}.md\"\n",
    "    with open(filename_cutoff, 'w', encoding='utf-8') as f:\n",
    "        f.write(report)\n",
    "    print(f\"User context with cutoff saved to {filename_cutoff}\")\n",
    "\n",
    "    print(f\"Token count: {token_count}\")\n",
    "    print(f\"Removed comments: {removed_comments}\")\n",
    "else:\n",
    "    error_message = f\"# Error\\n\\nNo data found for user ID {user_id}\"\n",
    "\n",
    "    for dir_path in [spheres_dir_no_cutoff, spheres_dir_cutoff]:\n",
    "        filename = f\"{dir_path}/{user_id}_error.md\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(error_message)\n",
    "        print(f\"Error message saved to {filename}\")"
   ],
   "id": "a2f918746ff110b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User context without cutoff saved to spheres/no_cutoff/26373.md\n",
      "User context with cutoff saved to spheres/cutoff/26373.md\n",
      "Token count: 713856\n",
      "Removed comments: 13\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-08-22T10:31:51.905469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "thread_manager = CommentThreadManager(preprocessor.df)\n",
    "user_context_sphere = UserContextSphere(preprocessor.df, thread_manager)\n",
    "\n",
    "# Directory setup\n",
    "output_dir = \"user_reports\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Find users with few comments\n",
    "min_comments, max_comments = 2, 5\n",
    "users_with_few_comments = user_context_sphere.find_users_with_few_comments(min_comments, max_comments)\n",
    "\n",
    "# Save list of users with few comments\n",
    "with open(os.path.join(output_dir, \"users_with_few_comments.md\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"# Users with {min_comments}-{max_comments} comments\\n\\n\")\n",
    "    for user_id in users_with_few_comments:\n",
    "        f.write(f\"- User ID: {user_id}\\n\")\n",
    "\n",
    "# Process and save reports for users with few comments\n",
    "summary = []\n",
    "for user_id in users_with_few_comments:\n",
    "    report, token_count, removed_comments = user_context_sphere.generate_user_report_with_cutoff(user_id)\n",
    "\n",
    "    if report != f\"No data found for user ID {user_id}\":\n",
    "        filename = os.path.join(output_dir, f\"user_{user_id}_report.md\")\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(report)\n",
    "        summary.append(f\"User {user_id}: {token_count} tokens, {removed_comments} comments removed\")\n",
    "    else:\n",
    "        summary.append(f\"User {user_id}: No data found\")\n",
    "\n",
    "# Save summary\n",
    "with open(os.path.join(output_dir, \"summary.md\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"# Summary of User Reports\\n\\n\")\n",
    "    for line in summary:\n",
    "        f.write(f\"- {line}\\n\")\n",
    "\n",
    "print(f\"Reports and summary saved in {output_dir}\")"
   ],
   "id": "2197b96ecbd3f54c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
