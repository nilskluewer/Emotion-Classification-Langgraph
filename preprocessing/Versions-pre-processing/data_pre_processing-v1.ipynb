{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Things I did in preprocessing\n",
    "- Building the threads and a mechanic to query for users\n",
    "- Build the context sphere by having all comments a user wrote + the full thread the user wrote it in\n",
    "- Reduce the overall context length by excluding non relevant comments -> which are after a user comment written"
   ],
   "id": "6a8b49a5750d12d4"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-20T08:39:18.155980Z",
     "start_time": "2024-09-20T08:39:17.943565Z"
    }
   },
   "source": [
    "# Merge csv Posting and Votes\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Replace 'your_file.csv' with the path to your CSV file\n",
    "df = pd.read_csv(r'./data/raw_csv/Postings_01052019_31052019.csv')\n",
    "\n",
    "\n",
    "# Display the first 10 rows\n",
    "display(df.head(10))\n",
    "print(df.to_html())"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/raw_csv/Postings_01052019_31052019.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mIPython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdisplay\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m display, HTML\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Replace 'your_file.csv' with the path to your CSV file\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m./data/raw_csv/Postings_01052019_31052019.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Display the first 10 rows\u001B[39;00m\n\u001B[1;32m     10\u001B[0m display(df\u001B[38;5;241m.\u001B[39mhead(\u001B[38;5;241m10\u001B[39m))\n",
      "File \u001B[0;32m~/Documents/GitHub/Emotion-Classification-Langgraph/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[1;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m   1014\u001B[0m     dialect,\n\u001B[1;32m   1015\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[1;32m   1023\u001B[0m )\n\u001B[1;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/Emotion-Classification-Langgraph/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    617\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[1;32m    619\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[0;32m--> 620\u001B[0m parser \u001B[38;5;241m=\u001B[39m \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    622\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[1;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[0;32m~/Documents/GitHub/Emotion-Classification-Langgraph/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m   1617\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   1619\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1620\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/Emotion-Classification-Langgraph/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[0;34m(self, f, engine)\u001B[0m\n\u001B[1;32m   1878\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[1;32m   1879\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1880\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1881\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1882\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1883\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1884\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1885\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1886\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1887\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1888\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1889\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1890\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1891\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[0;32m~/Documents/GitHub/Emotion-Classification-Langgraph/venv/lib/python3.12/site-packages/pandas/io/common.py:873\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    868\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    869\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[1;32m    870\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[1;32m    871\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[1;32m    872\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[0;32m--> 873\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    874\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    875\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    876\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    877\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    878\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    879\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    880\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    881\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[1;32m    882\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './data/raw_csv/Postings_01052019_31052019.csv'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T07:48:15.187134Z",
     "start_time": "2024-08-22T07:48:15.072418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pickle\n",
    "import tiktoken\n",
    "\n",
    "class DataPreprocessor:\n",
    "    \"\"\"\n",
    "    A class to preprocess the raw comment data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path):\n",
    "        \"\"\"\n",
    "        Initialize the DataPreprocessor with the path to the raw data file.\n",
    "        \n",
    "        :param file_path: str, path to the raw CSV file\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.df = None\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Load the raw data from CSV file.\"\"\"\n",
    "        self.df = pd.read_csv(self.file_path)\n",
    "\n",
    "    def preprocess(self):\n",
    "        \"\"\"Apply all preprocessing steps to the data.\"\"\"\n",
    "        self._convert_dates()\n",
    "        self._handle_missing_values()\n",
    "        self._convert_id_columns()\n",
    "        self._create_new_features()\n",
    "\n",
    "    def _convert_dates(self):\n",
    "        \"\"\"Convert date columns to datetime format.\"\"\"\n",
    "        date_columns = ['PostingCreatedAt', 'ArticlePublishingDate', 'UserCreatedAt']\n",
    "        for col in date_columns:\n",
    "            self.df[col] = pd.to_datetime(self.df[col])\n",
    "\n",
    "    def _handle_missing_values(self):\n",
    "        \"\"\"Handle missing values in the dataset.\"\"\"\n",
    "        self.df['PostingHeadline'] = self.df['PostingHeadline'].fillna('No Headline')\n",
    "        self.df['PostingComment'] = self.df['PostingComment'].fillna('No Comment')\n",
    "        self.df['UserGender'] = self.df['UserGender'].fillna('Unknown')\n",
    "        self.df['UserCommunityName'] = self.df['UserCommunityName'].fillna('Unknown')\n",
    "\n",
    "    def _convert_id_columns(self):\n",
    "        \"\"\"Convert ID columns to integers.\"\"\"\n",
    "        id_columns = ['ID_Posting', 'ID_Posting_Parent', 'ID_CommunityIdentity', 'ID_Article']\n",
    "        for col in id_columns:\n",
    "            self.df[col] = self.df[col].fillna(0).astype(int)\n",
    "\n",
    "    def _create_new_features(self):\n",
    "        \"\"\"Create new features from existing data.\"\"\"\n",
    "        self.df['CommentLength'] = self.df['PostingComment'].str.len()\n",
    "        self.df['DaysSinceUserCreation'] = (self.df['PostingCreatedAt'] - self.df['UserCreatedAt']).dt.days\n",
    "        self.df['IsReply'] = self.df['ID_Posting_Parent'] != 0\n",
    "        self.df['PostingHour'] = self.df['PostingCreatedAt'].dt.hour\n",
    "        self.df['PostingDayOfWeek'] = self.df['PostingCreatedAt'].dt.dayofweek\n",
    "\n",
    "    def save_preprocessed_data(self, output_path):\n",
    "        \"\"\"\n",
    "        Save the preprocessed data to a CSV file.\n",
    "        \n",
    "        :param output_path: str, path to save the preprocessed CSV file\n",
    "        \"\"\"\n",
    "        self.df.to_csv(output_path, index=False)\n",
    "\n",
    "        def save_preprocessed_data(self, output_path):\n",
    "            \"\"\"\n",
    "            Save the preprocessed data to a pickle file.\n",
    "            \n",
    "            :param output_path: str, path to save the preprocessed pickle file\n",
    "            \"\"\"\n",
    "        with open(output_path, 'wb') as f:\n",
    "            pickle.dump(self.df, f)\n",
    "        print(f\"Preprocessed data saved to {output_path}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load_preprocessed_data(cls, input_path):\n",
    "        \"\"\"\n",
    "        Load preprocessed data from a pickle file.\n",
    "        \n",
    "        :param input_path: str, path to the preprocessed pickle file\n",
    "        :return: DataPreprocessor instance with loaded data\n",
    "        \"\"\"\n",
    "        with open(input_path, 'rb') as f:\n",
    "            df = pickle.load(f)\n",
    "\n",
    "        preprocessor = cls(None)  # Create instance without file path\n",
    "        preprocessor.df = df\n",
    "        print(f\"Preprocessed data loaded from {input_path}\")\n",
    "        return preprocessor"
   ],
   "id": "5ff3d01d90bf2763",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T07:48:19.019604Z",
     "start_time": "2024-08-22T07:48:18.985021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json \n",
    "\n",
    "class CommentThreadManager:\n",
    "    \"\"\"\n",
    "    A class to manage and structure comment threads for articles.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Initialize the CommentThreadManager with a preprocessed DataFrame.\n",
    "        \n",
    "        :param df: pandas DataFrame, preprocessed comment data\n",
    "        \"\"\"\n",
    "        self.article_comments = {article_id: group for article_id, group in df.groupby('ID_Article')}\n",
    "\n",
    "    def build_comment_thread(self, comments, parent_id, depth=0):\n",
    "        thread = []\n",
    "        replies = comments[comments['ID_Posting_Parent'] == parent_id]\n",
    "        for _, reply in replies.iterrows():\n",
    "            sub_thread = self.build_comment_thread(comments, int(reply['ID_Posting']), depth + 1)\n",
    "            thread_item = {\n",
    "                'id': int(reply['ID_Posting']),\n",
    "                'parent_id': int(reply['ID_Posting_Parent']) if pd.notnull(reply['ID_Posting_Parent']) else None,\n",
    "                'user_id': int(reply['ID_CommunityIdentity']),\n",
    "                'user_name': reply['UserCommunityName'],\n",
    "                'user_gender': reply['UserGender'],\n",
    "                'user_created_at': reply['UserCreatedAt'].isoformat() if pd.notnull(reply['UserCreatedAt']) else None,\n",
    "                'comment_headline': reply['PostingHeadline'],\n",
    "                'comment_text': reply['PostingComment'],\n",
    "                'comment_created_at': reply['PostingCreatedAt'].isoformat() if pd.notnull(reply['PostingCreatedAt']) else None,\n",
    "                'comment_length': int(reply['CommentLength']),\n",
    "                'depth': depth,\n",
    "                'replies': sub_thread,\n",
    "                'thread_stats': {\n",
    "                    'total_comments': 1 + sum(r['thread_stats']['total_comments'] for r in sub_thread),\n",
    "                    'max_depth': depth + max([r['thread_stats']['max_depth'] for r in sub_thread] + [0]),\n",
    "                }\n",
    "            }\n",
    "            thread.append(thread_item)\n",
    "        return thread\n",
    "\n",
    "    def get_article_threads(self, article_id):\n",
    "        \"\"\"\n",
    "        Get the structured comment threads for a specific article.\n",
    "        \n",
    "        :param article_id: int, ID of the article\n",
    "        :return: dict, structured article data with comment threads\n",
    "        \"\"\"\n",
    "        if article_id not in self.article_comments:\n",
    "            return None\n",
    "\n",
    "        article_df = self.article_comments[article_id]\n",
    "        root_comments = article_df[article_df['ID_Posting_Parent'].isnull() | (article_df['ID_Posting_Parent'] == 0)]\n",
    "\n",
    "        threads = []\n",
    "        for _, comment in root_comments.iterrows():\n",
    "            thread = {\n",
    "                'id': int(comment['ID_Posting']),\n",
    "                'parent_id': None,\n",
    "                'user_id': int(comment['ID_CommunityIdentity']),\n",
    "                'user_name': comment['UserCommunityName'],\n",
    "                'user_gender': comment['UserGender'],\n",
    "                'user_created_at': comment['UserCreatedAt'].isoformat() if pd.notnull(comment['UserCreatedAt']) else None,\n",
    "                'comment_headline': comment['PostingHeadline'],\n",
    "                'comment_text': comment['PostingComment'],\n",
    "                'comment_created_at': comment['PostingCreatedAt'].isoformat() if pd.notnull(comment['PostingCreatedAt']) else None,\n",
    "                'comment_length': int(comment['CommentLength']),\n",
    "                'depth': 0,\n",
    "                'replies': self.build_comment_thread(article_df, int(comment['ID_Posting']), 1)\n",
    "            }\n",
    "            threads.append(thread)\n",
    "\n",
    "        article_meta = article_df.iloc[0]\n",
    "\n",
    "        return {\n",
    "            'article_id': int(article_id),\n",
    "            'article_title': article_meta['ArticleTitle'],\n",
    "            'article_publish_date': article_meta['ArticlePublishingDate'].isoformat() if pd.notnull(article_meta['ArticlePublishingDate']) else None,\n",
    "            'article_channel': article_meta['ArticleChannel'],\n",
    "            'article_ressort_name': article_meta['ArticleRessortName'],\n",
    "            'total_comments': len(article_df),\n",
    "            'root_comments': len(root_comments),\n",
    "            'comment_threads': threads\n",
    "        }\n",
    "\n",
    "    def get_article_ids(self):\n",
    "        \"\"\"\n",
    "        Get a list of all article IDs in the dataset.\n",
    "        \n",
    "        :return: list of int, article IDs\n",
    "        \"\"\"\n",
    "        return list(self.article_comments.keys())\n",
    "\n",
    "    def get_user_ids(self):\n",
    "        \"\"\"\n",
    "        Get a list of all user IDs in the dataset.\n",
    "        \n",
    "        :return: list of int, user IDs\n",
    "        \"\"\"\n",
    "        return self.df['ID_CommunityIdentity'].unique().tolist()"
   ],
   "id": "699e3c4c32450565",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Now lets start building the context sphere\n",
    "I understand your point. You're looking to create a more comprehensive view of a user's activity and interactions within the comment threads. Instead of just seeing isolated comments made by a user, you want to see the full context of their engagement. This includes:\n",
    "\n",
    "1. All comments made by the user across different articles.\n",
    "2. For each comment, you want to see the entire thread it belongs to, not just the user's comment in isolation.\n",
    "3. This will show what the user was responding to and how their comment fits into the larger conversation.\n",
    "4. It will also reveal any subsequent responses to the user's comments.\n",
    "\n",
    "The goal is to build a \"context sphere\" around each user, showing their complete interaction history within the comment ecosystem. This approach will provide a more nuanced understanding of the user's behavior, opinions, and how they engage with others in discussions.\n",
    "\n",
    "Is this interpretation correct? If so, I can suggest how to modify your existing code to achieve this goal."
   ],
   "id": "45feb9caccc478ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T07:48:25.815074Z",
     "start_time": "2024-08-22T07:48:25.807439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class UserContextSphere:\n",
    "    def __init__(self, df, thread_manager):\n",
    "        self.df = df\n",
    "        self.thread_manager = thread_manager\n",
    "        self.user_comments = {user_id: group for user_id, group in df.groupby('ID_CommunityIdentity')}\n",
    "\n",
    "    def get_user_context(self, user_id):\n",
    "        if user_id not in self.user_comments:\n",
    "            return None\n",
    "\n",
    "        user_df = self.user_comments[user_id]\n",
    "\n",
    "        # Calculate user statistics\n",
    "        total_comments = len(user_df)\n",
    "        total_replies = len(user_df[user_df['ID_Posting_Parent'].notnull()])\n",
    "\n",
    "        user_context = {\n",
    "            'user_id': int(user_id),\n",
    "            'user_name': user_df['UserCommunityName'].iloc[0],\n",
    "            'user_gender': user_df['UserGender'].iloc[0],\n",
    "            'user_created_at': user_df['UserCreatedAt'].iloc[0].isoformat(),\n",
    "            'total_comments': total_comments,\n",
    "            'total_replies': total_replies,\n",
    "            'articles': {}\n",
    "        }\n",
    "\n",
    "        # Group comments by article\n",
    "        for article_id, article_comments in user_df.groupby('ID_Article'):\n",
    "            article_id = int(article_id)\n",
    "            article_thread = self.thread_manager.get_article_threads(article_id)\n",
    "\n",
    "            if article_thread:\n",
    "                user_context['articles'][article_id] = {\n",
    "                    'article_title': article_thread['article_title'],\n",
    "                    'article_publish_date': article_thread['article_publish_date'],\n",
    "                    'user_comments_count': len(article_comments),\n",
    "                    'user_replies_count': len(article_comments[article_comments['ID_Posting_Parent'].notnull()]),\n",
    "                    'threads': []\n",
    "                }\n",
    "\n",
    "                for _, comment in article_comments.iterrows():\n",
    "                    thread = self.find_thread_for_comment(article_thread['comment_threads'], int(comment['ID_Posting']))\n",
    "                    if thread:\n",
    "                        user_context['articles'][article_id]['threads'].append(thread)\n",
    "\n",
    "        return user_context\n",
    "\n",
    "    def find_thread_for_comment(self, threads, comment_id):\n",
    "        for thread in threads:\n",
    "            if thread['id'] == comment_id:\n",
    "                return thread\n",
    "            if thread['replies']:\n",
    "                result = self.find_thread_for_comment(thread['replies'], comment_id)\n",
    "                if result:\n",
    "                    return thread  # Return the whole thread, not just the subthread\n",
    "        return None\n",
    "\n",
    "    def count_tokens(self, text):\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        tokens_lst = encoding.encode(text)\n",
    "        return len(tokens_lst)\n",
    "\n",
    "    def generate_user_report_with_token_count(self, user_id):\n",
    "        report = self.generate_user_report(user_id)\n",
    "        token_count = self.count_tokens(report)\n",
    "        report += f\"\\nTotal token count: {token_count}\"\n",
    "        return report, token_count\n",
    "\n",
    "    def format_thread(self, thread, user_id, depth):\n",
    "        indent = \"â”‚   \" * depth\n",
    "        is_user = thread['user_id'] == user_id\n",
    "        user_indicator = \"ðŸ‘¤\" if is_user else \"  \"\n",
    "    \n",
    "        formatted = f\"{indent}{'â””â”€ ' if depth > 0 else ''}{user_indicator} {thread['user_name']} \"\n",
    "        formatted += f\"({thread['user_gender']}) - {thread['comment_created_at']}\\n\"\n",
    "        formatted += f\"{indent}{'   ' if depth > 0 else ''}  {thread['comment_text']}\\n\"\n",
    "        formatted += f\"{indent}{'   ' if depth > 0 else ''}  Length: {thread['comment_length']} chars\\n\"\n",
    "    \n",
    "        if thread['replies']:\n",
    "            formatted += f\"{indent}{'   ' if depth > 0 else ''}  Replies: {len(thread['replies'])}\\n\"\n",
    "    \n",
    "        formatted += \"\\n\"\n",
    "    \n",
    "        for reply in thread['replies']:\n",
    "            formatted += self.format_thread(reply, user_id, depth + 1)\n",
    "    \n",
    "        return formatted\n",
    "\n",
    "    def generate_user_report(self, user_id):\n",
    "        user_context = self.get_user_context(user_id)\n",
    "        if not user_context:\n",
    "            return f\"No data found for user ID {user_id}\"\n",
    "\n",
    "        report = f\"User Report for ID: {user_context['user_id']}\\n\"\n",
    "        report += f\"Name: {user_context['user_name']}\\n\"\n",
    "        report += f\"Gender: {user_context['user_gender']}\\n\"\n",
    "        report += f\"Created At: {user_context['user_created_at']}\\n\"\n",
    "        report += f\"Total Comments: {user_context['total_comments']}\\n\"\n",
    "        report += f\"Total Replies: {user_context['total_replies']}\\n\\n\"\n",
    "\n",
    "        for article_id, article_data in user_context['articles'].items():\n",
    "            report += f\"Article: {article_data['article_title']}\\n\"\n",
    "            report += f\"Published: {article_data['article_publish_date']}\\n\"\n",
    "            report += f\"User Comments on this Article: {article_data['user_comments_count']}\\n\"\n",
    "            report += f\"User Replies on this Article: {article_data['user_replies_count']}\\n\\n\"\n",
    "\n",
    "            report += \"Comment Threads:\\n\"\n",
    "            for thread in article_data['threads']:\n",
    "                report += self.format_thread(thread, user_id, 0)\n",
    "                report += \"\\n\"\n",
    "\n",
    "        return report"
   ],
   "id": "3552ed89bc5bd17d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T07:55:30.691893Z",
     "start_time": "2024-08-22T07:55:30.604212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocessed_file = \"data/preprocessed/preprocessed_data.pkl\"\n",
    "\n",
    "    if not os.path.exists(preprocessed_file):\n",
    "        print(\"Preprocessed data not found. Preprocessing...\")\n",
    "        preprocessor = DataPreprocessor('./data/raw_csv/Postings_01052019_31052019.csv')\n",
    "        preprocessor.load_data()\n",
    "        preprocessor.preprocess()\n",
    "        preprocessor.save_preprocessed_data(preprocessed_file)\n",
    "    else:\n",
    "        print(\"Loading preprocessed data...\")\n",
    "        preprocessor = DataPreprocessor.load_preprocessed_data(preprocessed_file)\n",
    "\n",
    "    # Create CommentThreadManager\n",
    "    thread_manager = CommentThreadManager(preprocessor.df)\n",
    "\n",
    "    # Create UserContextSphere\n",
    "    user_context_sphere = UserContextSphere(preprocessor.df, thread_manager)\n",
    "\n",
    "    # Example: Generate report for a specific user\n",
    "    user_id = 633859  # Replace with the desired user ID\n",
    "    user_report, token_count = user_context_sphere.generate_user_report_with_token_count(user_id)\n",
    "\n",
    "\n",
    "print(user_report)\n"
   ],
   "id": "c0704389e8145f76",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data not found. Preprocessing...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/raw_csv/Postings_01052019_31052019.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 9\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPreprocessed data not found. Preprocessing...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      8\u001B[0m preprocessor \u001B[38;5;241m=\u001B[39m DataPreprocessor(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./data/raw_csv/Postings_01052019_31052019.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 9\u001B[0m \u001B[43mpreprocessor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m preprocessor\u001B[38;5;241m.\u001B[39mpreprocess()\n\u001B[1;32m     11\u001B[0m preprocessor\u001B[38;5;241m.\u001B[39msave_preprocessed_data(preprocessed_file)\n",
      "Cell \u001B[0;32mIn[2], line 24\u001B[0m, in \u001B[0;36mDataPreprocessor.load_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m     23\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Load the raw data from CSV file.\"\"\"\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdf \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfile_path\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/Emotion-Classification-Langgraph/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[1;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m   1014\u001B[0m     dialect,\n\u001B[1;32m   1015\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[1;32m   1023\u001B[0m )\n\u001B[1;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/Emotion-Classification-Langgraph/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    617\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[1;32m    619\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[0;32m--> 620\u001B[0m parser \u001B[38;5;241m=\u001B[39m \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    622\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[1;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[0;32m~/Documents/GitHub/Emotion-Classification-Langgraph/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m   1617\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   1619\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1620\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/Emotion-Classification-Langgraph/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[0;34m(self, f, engine)\u001B[0m\n\u001B[1;32m   1878\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[1;32m   1879\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1880\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1881\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1882\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1883\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1884\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1885\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1886\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1887\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1888\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1889\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1890\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1891\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[0;32m~/Documents/GitHub/Emotion-Classification-Langgraph/venv/lib/python3.12/site-packages/pandas/io/common.py:873\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    868\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    869\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[1;32m    870\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[1;32m    871\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[1;32m    872\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[0;32m--> 873\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    874\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    875\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    876\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    877\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    878\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    879\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    880\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    881\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[1;32m    882\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './data/raw_csv/Postings_01052019_31052019.csv'"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T08:43:25.533643Z",
     "start_time": "2024-07-25T08:43:24.172311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocessed_file = \"data/preprocessed/preprocessed_data.pkl\"\n",
    "\n",
    "    if not os.path.exists(preprocessed_file):\n",
    "        print(\"Preprocessed data not found. Preprocessing...\")\n",
    "        preprocessor = DataPreprocessor('./data/raw_csv/Postings_01052019_31052019.csv')\n",
    "        preprocessor.load_data()\n",
    "        preprocessor.preprocess()\n",
    "        preprocessor.save_preprocessed_data(preprocessed_file)\n",
    "    else:\n",
    "        print(\"Loading preprocessed data...\")\n",
    "        preprocessor = DataPreprocessor.load_preprocessed_data(preprocessed_file)\n",
    "\n",
    "        thread_manager = CommentThreadManager(preprocessor.df)\n",
    "        article_id = 12345  # Beispiel-Artikel-ID\n",
    "        user_id = 67890     # Optional: Ziel-Benutzer-ID\n",
    "        \n",
    "        # FÃ¼r alle Threads im Artikel\n",
    "        full_thread_json = thread_manager.get_full_article_thread_json(article_id)\n",
    "        \n",
    "        # Nur fÃ¼r Threads, die den Ziel-Benutzer enthalten\n",
    "        user_thread_json = thread_manager.get_full_article_thread_json(article_id, user_id)\n",
    "        \n",
    "        print(full_thread_json)  # oder user_thread_json\n",
    "\n"
   ],
   "id": "620d705f1f1685d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data...\n",
      "Preprocessed data loaded from data/preprocessed/preprocessed_data.pkl\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T07:50:10.033163Z",
     "start_time": "2024-07-25T07:50:10.017915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tokens_lst = encoding.encode(\"tiktoken is great!\")\n",
    "print(len(tokens_lst))\n"
   ],
   "id": "28c0f98b5176b32c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T08:19:10.691252Z",
     "start_time": "2024-07-25T08:17:24.595132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class UserAnalyzer:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    def count_users(self):\n",
    "        return self.df['ID_CommunityIdentity'].nunique()\n",
    "\n",
    "    def count_tokens(self, text):\n",
    "        tokens_lst = self.encoding.encode(text)\n",
    "        return len(tokens_lst)\n",
    "\n",
    "    def get_token_distribution(self):\n",
    "        user_token_counts = defaultdict(int)\n",
    "\n",
    "        for _, row in self.df.iterrows():\n",
    "            user_id = row['ID_CommunityIdentity']\n",
    "            comment = row['PostingComment']\n",
    "            if isinstance(comment, str):\n",
    "                user_token_counts[user_id] += self.count_tokens(comment)\n",
    "\n",
    "        return dict(user_token_counts)\n",
    "\n",
    "    def analyze_token_distribution(self):\n",
    "        token_distribution = self.get_token_distribution()\n",
    "\n",
    "        total_users = len(token_distribution)\n",
    "        total_tokens = sum(token_distribution.values())\n",
    "        avg_tokens = total_tokens / total_users if total_users > 0 else 0\n",
    "\n",
    "        sorted_distribution = sorted(token_distribution.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return {\n",
    "            'total_users': total_users,\n",
    "            'total_tokens': total_tokens,\n",
    "            'avg_tokens_per_user': avg_tokens,\n",
    "            'top_10_users': sorted_distribution[:10],\n",
    "            'bottom_10_users': sorted_distribution[-10:],\n",
    "        }\n",
    "\n",
    "    def analyze_activity_patterns(self):\n",
    "        self.df['PostingHour'] = self.df['PostingCreatedAt'].dt.hour\n",
    "        self.df['PostingDayOfWeek'] = self.df['PostingCreatedAt'].dt.dayofweek\n",
    "\n",
    "        hourly_activity = self.df['PostingHour'].value_counts().sort_index()\n",
    "        daily_activity = self.df['PostingDayOfWeek'].value_counts().sort_index()\n",
    "\n",
    "        return {\n",
    "            'hourly_activity': hourly_activity,\n",
    "            'daily_activity': daily_activity\n",
    "        }\n",
    "\n",
    "    def segment_users(self, n_clusters=3):\n",
    "        user_activity = self.df.groupby('ID_CommunityIdentity').agg({\n",
    "            'ID_Posting': 'count',\n",
    "            'CommentLength': 'mean',\n",
    "            'PostingHour': 'mean',\n",
    "            'PostingDayOfWeek': 'mean'\n",
    "        }).reset_index()\n",
    "\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        user_activity['Cluster'] = kmeans.fit_predict(user_activity.drop('ID_CommunityIdentity', axis=1))\n",
    "\n",
    "        return user_activity\n",
    "\n",
    "    def visualize_activity_patterns(self):\n",
    "        activity_patterns = self.analyze_activity_patterns()\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "        sns.barplot(x=activity_patterns['hourly_activity'].index,\n",
    "                    y=activity_patterns['hourly_activity'].values, ax=ax1)\n",
    "        ax1.set_title('Hourly Activity Pattern')\n",
    "        ax1.set_xlabel('Hour of Day')\n",
    "        ax1.set_ylabel('Number of Comments')\n",
    "\n",
    "        sns.barplot(x=activity_patterns['daily_activity'].index,\n",
    "                    y=activity_patterns['daily_activity'].values, ax=ax2)\n",
    "        ax2.set_title('Daily Activity Pattern')\n",
    "        ax2.set_xlabel('Day of Week (0 = Monday, 6 = Sunday)')\n",
    "        ax2.set_ylabel('Number of Comments')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('activity_patterns.png')\n",
    "        plt.close()\n",
    "\n",
    "    def visualize_token_distribution(self):\n",
    "        token_distribution = self.get_token_distribution()\n",
    "        token_counts = list(token_distribution.values())\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(token_counts, kde=True)\n",
    "        plt.title('Distribution of Token Counts per User')\n",
    "        plt.xlabel('Token Count')\n",
    "        plt.ylabel('Number of Users')\n",
    "        plt.savefig('token_distribution.png')\n",
    "        plt.close()\n",
    "\n",
    "    def visualize_user_segments(self):\n",
    "        user_segments = self.segment_users()\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.scatterplot(data=user_segments, x='ID_Posting', y='CommentLength', hue='Cluster', palette='deep')\n",
    "        plt.title('User Segments based on Activity and Comment Length')\n",
    "        plt.xlabel('Number of Comments')\n",
    "        plt.ylabel('Average Comment Length')\n",
    "        plt.savefig('user_segments.png')\n",
    "        plt.close()\n",
    "\n",
    "def main_analysis():\n",
    "    preprocessed_file = \"data/preprocessed/preprocessed_data.pkl\"\n",
    "\n",
    "    if not os.path.exists(preprocessed_file):\n",
    "        print(\"Preprocessed data not found. Preprocessing...\")\n",
    "        preprocessor = DataPreprocessor('./data/raw_csv/Postings_01052019_31052019.csv')\n",
    "        preprocessor.load_data()\n",
    "        preprocessor.preprocess()\n",
    "\n",
    "        os.makedirs(os.path.dirname(preprocessed_file), exist_ok=True)\n",
    "        with open(preprocessed_file, 'wb') as f:\n",
    "            pickle.dump(preprocessor.df, f)\n",
    "\n",
    "        df = preprocessor.df\n",
    "    else:\n",
    "        print(\"Loading preprocessed data...\")\n",
    "        with open(preprocessed_file, 'rb') as f:\n",
    "            df = pickle.load(f)\n",
    "\n",
    "    user_analyzer = UserAnalyzer(df)\n",
    "\n",
    "    total_users = user_analyzer.count_users()\n",
    "    print(f\"Total number of unique users: {total_users}\")\n",
    "\n",
    "    print(\"Analyzing token distribution...\")\n",
    "    analysis_results = user_analyzer.analyze_token_distribution()\n",
    "\n",
    "    print(f\"Total users analyzed: {analysis_results['total_users']}\")\n",
    "    print(f\"Total tokens across all users: {analysis_results['total_tokens']}\")\n",
    "    print(f\"Average tokens per user: {analysis_results['avg_tokens_per_user']:.2f}\")\n",
    "\n",
    "    print(\"\\nTop 10 users by token count:\")\n",
    "    for user_id, token_count in analysis_results['top_10_users']:\n",
    "        print(f\"User ID: {user_id}, Token count: {token_count}\")\n",
    "\n",
    "    print(\"\\nBottom 10 users by token count:\")\n",
    "    for user_id, token_count in analysis_results['bottom_10_users']:\n",
    "        print(f\"User ID: {user_id}, Token count: {token_count}\")\n",
    "\n",
    "    print(\"\\nAnalyzing activity patterns...\")\n",
    "    activity_patterns = user_analyzer.analyze_activity_patterns()\n",
    "    print(\"Hourly activity pattern:\", activity_patterns['hourly_activity'])\n",
    "    print(\"Daily activity pattern:\", activity_patterns['daily_activity'])\n",
    "\n",
    "    print(\"\\nSegmenting users...\")\n",
    "    user_segments = user_analyzer.segment_users()\n",
    "    print(user_segments.groupby('Cluster').mean())\n",
    "\n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    user_analyzer.visualize_activity_patterns()\n",
    "    user_analyzer.visualize_token_distribution()\n",
    "    user_analyzer.visualize_user_segments()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_analysis()"
   ],
   "id": "b64c4e440575cee9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data...\n",
      "Total number of unique users: 23925\n",
      "Analyzing token distribution...\n",
      "Total users analyzed: 23925\n",
      "Total tokens across all users: 38282469\n",
      "Average tokens per user: 1600.10\n",
      "\n",
      "Top 10 users by token count:\n",
      "User ID: 499749, Token count: 104725\n",
      "User ID: 518647, Token count: 91048\n",
      "User ID: 601254, Token count: 79884\n",
      "User ID: 521362, Token count: 75729\n",
      "User ID: 678541, Token count: 72960\n",
      "User ID: 1782, Token count: 70133\n",
      "User ID: 59327, Token count: 69724\n",
      "User ID: 665847, Token count: 67760\n",
      "User ID: 686372, Token count: 66608\n",
      "User ID: 633859, Token count: 65870\n",
      "\n",
      "Bottom 10 users by token count:\n",
      "User ID: 512297, Token count: 1\n",
      "User ID: 565387, Token count: 1\n",
      "User ID: 188861, Token count: 1\n",
      "User ID: 669059, Token count: 1\n",
      "User ID: 173522, Token count: 1\n",
      "User ID: 537330, Token count: 1\n",
      "User ID: 499935, Token count: 1\n",
      "User ID: 694186, Token count: 1\n",
      "User ID: 539016, Token count: 1\n",
      "User ID: 670012, Token count: 1\n",
      "\n",
      "Analyzing activity patterns...\n",
      "Hourly activity pattern: PostingHour\n",
      "0     16347\n",
      "1      8268\n",
      "2      4510\n",
      "3      2867\n",
      "4      2628\n",
      "5      5022\n",
      "6     14870\n",
      "7     29793\n",
      "8     40778\n",
      "9     46727\n",
      "10    47500\n",
      "11    46395\n",
      "12    46139\n",
      "13    46788\n",
      "14    43427\n",
      "15    41439\n",
      "16    40073\n",
      "17    38322\n",
      "18    40218\n",
      "19    41544\n",
      "20    38655\n",
      "21    37160\n",
      "22    33352\n",
      "23    26272\n",
      "Name: count, dtype: int64\n",
      "Daily activity pattern: PostingDayOfWeek\n",
      "0    102365\n",
      "1    109218\n",
      "2    112051\n",
      "3    118709\n",
      "4    126037\n",
      "5     86078\n",
      "6     84636\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Segmenting users...\n",
      "         ID_CommunityIdentity  ID_Posting  CommentLength  PostingHour  \\\n",
      "Cluster                                                                 \n",
      "0               431687.202429  376.136302     182.398757    13.768470   \n",
      "1               432568.639827   20.819941     123.537672    13.797637   \n",
      "2               453800.084442   16.697154     397.478619    13.592789   \n",
      "\n",
      "         PostingDayOfWeek  \n",
      "Cluster                    \n",
      "0                2.900004  \n",
      "1                2.894039  \n",
      "2                2.849367  \n",
      "\n",
      "Generating visualizations...\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
