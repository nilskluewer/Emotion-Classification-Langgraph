{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T09:00:28.817547Z",
     "start_time": "2024-08-22T09:00:28.439754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import tiktoken\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import os\n",
    "import json\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, file_path: str):\n",
    "        self.file_path = file_path\n",
    "        self.df = None\n",
    "\n",
    "    def process(self):\n",
    "        self.df = pd.read_csv(self.file_path)\n",
    "        self._convert_dates()\n",
    "        self._handle_missing_values()\n",
    "        self._convert_id_columns()\n",
    "        self._create_new_features()\n",
    "\n",
    "    def _convert_dates(self):\n",
    "        date_columns = ['PostingCreatedAt', 'ArticlePublishingDate', 'UserCreatedAt']\n",
    "        for col in date_columns:\n",
    "            self.df[col] = pd.to_datetime(self.df[col])\n",
    "\n",
    "    def _handle_missing_values(self):\n",
    "        self.df['PostingHeadline'] = self.df['PostingHeadline'].fillna('No Headline')\n",
    "        self.df['PostingComment'] = self.df['PostingComment'].fillna('No Comment')\n",
    "        self.df['UserGender'] = self.df['UserGender'].fillna('Unknown')\n",
    "        self.df['UserCommunityName'] = self.df['UserCommunityName'].fillna('Unknown')\n",
    "\n",
    "    def _convert_id_columns(self):\n",
    "        id_columns = ['ID_Posting', 'ID_Posting_Parent', 'ID_CommunityIdentity', 'ID_Article']\n",
    "        for col in id_columns:\n",
    "            self.df[col] = self.df[col].fillna(0).astype(int)\n",
    "\n",
    "    def _create_new_features(self):\n",
    "        self.df['CommentLength'] = self.df['PostingComment'].str.len()\n",
    "        self.df['DaysSinceUserCreation'] = (self.df['PostingCreatedAt'] - self.df['UserCreatedAt']).dt.days\n",
    "        self.df['IsReply'] = self.df['ID_Posting_Parent'] != 0\n",
    "        self.df['PostingHour'] = self.df['PostingCreatedAt'].dt.hour\n",
    "        self.df['PostingDayOfWeek'] = self.df['PostingCreatedAt'].dt.dayofweek\n",
    "\n",
    "    def save_preprocessed_data(self, output_path: str):\n",
    "        with open(output_path, 'wb') as f:\n",
    "            pickle.dump(self.df, f)\n",
    "        print(f\"Preprocessed data saved to {output_path}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load_preprocessed_data(cls, input_path: str):\n",
    "        with open(input_path, 'rb') as f:\n",
    "            df = pickle.load(f)\n",
    "        preprocessor = cls(None)\n",
    "        preprocessor.df = df\n",
    "        print(f\"Preprocessed data loaded from {input_path}\")\n",
    "        return preprocessor\n",
    "\n",
    "class CommentThreadManager:\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.article_comments = {article_id: group for article_id, group in df.groupby('ID_Article')}\n",
    "\n",
    "    def build_comment_thread(self, comments: pd.DataFrame, parent_id: int, depth: int = 0) -> List[Dict]:\n",
    "        replies = comments[comments['ID_Posting_Parent'] == parent_id]\n",
    "        return [{\n",
    "            'id': int(reply['ID_Posting']),\n",
    "            'parent_id': int(reply['ID_Posting_Parent']) if pd.notnull(reply['ID_Posting_Parent']) else None,\n",
    "            'user_id': int(reply['ID_CommunityIdentity']),\n",
    "            'user_name': reply['UserCommunityName'],\n",
    "            'user_gender': reply['UserGender'],\n",
    "            'user_created_at': reply['UserCreatedAt'].isoformat() if pd.notnull(reply['UserCreatedAt']) else None,\n",
    "            'comment_headline': reply['PostingHeadline'],\n",
    "            'comment_text': reply['PostingComment'],\n",
    "            'comment_created_at': reply['PostingCreatedAt'].isoformat() if pd.notnull(reply['PostingCreatedAt']) else None,\n",
    "            'comment_length': int(reply['CommentLength']),\n",
    "            'depth': depth,\n",
    "            'replies': self.build_comment_thread(comments, int(reply['ID_Posting']), depth + 1)\n",
    "        } for _, reply in replies.iterrows()]\n",
    "\n",
    "    def get_article_threads(self, article_id: int) -> Optional[Dict]:\n",
    "        if article_id not in self.article_comments:\n",
    "            return None\n",
    "\n",
    "        article_df = self.article_comments[article_id]\n",
    "        root_comments = article_df[article_df['ID_Posting_Parent'].isnull() | (article_df['ID_Posting_Parent'] == 0)]\n",
    "\n",
    "        threads = self.build_comment_thread(article_df, 0)\n",
    "        article_meta = article_df.iloc[0]\n",
    "\n",
    "        return {\n",
    "            'article_id': int(article_id),\n",
    "            'article_title': article_meta['ArticleTitle'],\n",
    "            'article_publish_date': article_meta['ArticlePublishingDate'].isoformat() if pd.notnull(article_meta['ArticlePublishingDate']) else None,\n",
    "            'article_channel': article_meta['ArticleChannel'],\n",
    "            'article_ressort_name': article_meta['ArticleRessortName'],\n",
    "            'total_comments': len(article_df),\n",
    "            'root_comments': len(root_comments),\n",
    "            'comment_threads': threads\n",
    "        }\n",
    "\n",
    "    def get_article_ids(self) -> List[int]:\n",
    "        return list(self.article_comments.keys())\n",
    "\n",
    "class UserContextSphere:\n",
    "    def __init__(self, df: pd.DataFrame, thread_manager: CommentThreadManager):\n",
    "        self.df = df\n",
    "        self.thread_manager = thread_manager\n",
    "        self.user_comments = {user_id: group for user_id, group in df.groupby('ID_CommunityIdentity')}\n",
    "\n",
    "    def get_user_context(self, user_id: int) -> Optional[Dict]:\n",
    "        if user_id not in self.user_comments:\n",
    "            return None\n",
    "\n",
    "        user_df = self.user_comments[user_id]\n",
    "        total_comments = len(user_df)\n",
    "        total_replies = len(user_df[user_df['ID_Posting_Parent'].notnull()])\n",
    "\n",
    "        user_context = {\n",
    "            'user_id': int(user_id),\n",
    "            'user_name': user_df['UserCommunityName'].iloc[0],\n",
    "            'user_gender': user_df['UserGender'].iloc[0],\n",
    "            'user_created_at': user_df['UserCreatedAt'].iloc[0].isoformat(),\n",
    "            'total_comments': total_comments,\n",
    "            'total_replies': total_replies,\n",
    "            'articles': {}\n",
    "        }\n",
    "\n",
    "        for article_id, article_comments in user_df.groupby('ID_Article'):\n",
    "            article_id = int(article_id)\n",
    "            article_thread = self.thread_manager.get_article_threads(article_id)\n",
    "\n",
    "            if article_thread:\n",
    "                user_context['articles'][article_id] = {\n",
    "                    'article_title': article_thread['article_title'],\n",
    "                    'article_publish_date': article_thread['article_publish_date'],\n",
    "                    'user_comments_count': len(article_comments),\n",
    "                    'user_replies_count': len(article_comments[article_comments['ID_Posting_Parent'].notnull()]),\n",
    "                    'threads': [self.find_thread_for_comment(article_thread['comment_threads'], int(comment['ID_Posting']))\n",
    "                                for _, comment in article_comments.iterrows()]\n",
    "                }\n",
    "\n",
    "        return user_context\n",
    "\n",
    "    def find_thread_for_comment(self, threads: List[Dict], comment_id: int) -> Optional[Dict]:\n",
    "        for thread in threads:\n",
    "            if thread['id'] == comment_id:\n",
    "                return thread\n",
    "            result = self.find_thread_for_comment(thread['replies'], comment_id)\n",
    "            if result:\n",
    "                return thread\n",
    "        return None\n",
    "\n",
    "    def cutoff_after_last_interaction(self, user_context: Dict, user_id: int) -> Tuple[Dict, int]:\n",
    "        removed_comments = 0\n",
    "\n",
    "        def process_thread(thread: Dict, last_interaction_time: Optional[str]) -> Tuple[Optional[Dict], int, Optional[str]]:\n",
    "            nonlocal removed_comments\n",
    "            if thread['user_id'] == user_id:\n",
    "                last_interaction_time = thread['comment_created_at']\n",
    "\n",
    "            if last_interaction_time and thread['comment_created_at'] > last_interaction_time:\n",
    "                removed_comments += 1\n",
    "                return None, removed_comments, last_interaction_time\n",
    "\n",
    "            new_replies = []\n",
    "            for reply in thread['replies']:\n",
    "                processed_reply, removed_comments, last_interaction_time = process_thread(reply, last_interaction_time)\n",
    "                if processed_reply:\n",
    "                    new_replies.append(processed_reply)\n",
    "\n",
    "            thread['replies'] = new_replies\n",
    "            return thread, removed_comments, last_interaction_time\n",
    "\n",
    "        for article_id in user_context['articles']:\n",
    "            new_threads = []\n",
    "            last_interaction_time = None\n",
    "            for thread in user_context['articles'][article_id]['threads']:\n",
    "                processed_thread, removed_comments, last_interaction_time = process_thread(thread, last_interaction_time)\n",
    "                if processed_thread:\n",
    "                    new_threads.append(processed_thread)\n",
    "            user_context['articles'][article_id]['threads'] = new_threads\n",
    "\n",
    "        return user_context, removed_comments\n",
    "\n",
    "    def generate_user_report_with_cutoff(self, user_id: int) -> Tuple[str, int, int]:\n",
    "        user_context = self.get_user_context(user_id)\n",
    "        if not user_context:\n",
    "            return f\"No data found for user ID {user_id}\", 0, 0\n",
    "\n",
    "        user_context, removed_comments = self.cutoff_after_last_interaction(user_context, user_id)\n",
    "\n",
    "        report = json.dumps(user_context, indent=2)\n",
    "        token_count = self.count_tokens(report)\n",
    "\n",
    "        return report, token_count, removed_comments\n",
    "\n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        tokens_lst = encoding.encode(text)\n",
    "        return len(tokens_lst)\n"
   ],
   "id": "daa4e0d56b24bee5",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T09:01:23.780558Z",
     "start_time": "2024-08-22T09:00:31.391485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    preprocessed_file = \"../data/preprocessed/preprocessed_data.pkl\"\n",
    "\n",
    "    if not os.path.exists(preprocessed_file):\n",
    "        print(\"Preprocessed data not found. Preprocessing...\")\n",
    "        preprocessor = DataPreprocessor('../data/raw_csv/Postings_01052019_31052019.csv')\n",
    "        preprocessor.process()\n",
    "        preprocessor.save_preprocessed_data(preprocessed_file)\n",
    "    else:\n",
    "        print(\"Loading preprocessed data...\")\n",
    "        preprocessor = DataPreprocessor.load_preprocessed_data(preprocessed_file)\n",
    "\n",
    "    thread_manager = CommentThreadManager(preprocessor.df)\n",
    "    user_context_sphere = UserContextSphere(preprocessor.df, thread_manager)\n",
    "\n",
    "    # Create the spheres directories if they don't exist\n",
    "    spheres_dir_no_cutoff = \"spheres/no_cutoff\"\n",
    "    spheres_dir_cutoff = \"spheres/cutoff\"\n",
    "    os.makedirs(spheres_dir_no_cutoff, exist_ok=True)\n",
    "    os.makedirs(spheres_dir_cutoff, exist_ok=True)\n",
    "\n",
    "    user_id = 499749  # Replace with the desired user ID\n",
    "\n",
    "    # Generate and save context without cutoff\n",
    "    user_context_no_cutoff = user_context_sphere.get_user_context(user_id)\n",
    "    if user_context_no_cutoff:\n",
    "        token_count_no_cutoff = user_context_sphere.count_tokens(json.dumps(user_context_no_cutoff))\n",
    "        user_context_no_cutoff['token_count'] = token_count_no_cutoff\n",
    "\n",
    "        filename_no_cutoff = f\"{spheres_dir_no_cutoff}/{user_id}.json\"\n",
    "        with open(filename_no_cutoff, 'w', encoding='utf-8') as f:\n",
    "            json.dump(user_context_no_cutoff, f, indent=2)\n",
    "        print(f\"User context without cutoff saved to {filename_no_cutoff}\")\n",
    "\n",
    "        # Generate and save context with cutoff\n",
    "        user_context_cutoff, removed_comments = user_context_sphere.cutoff_after_last_interaction(user_context_no_cutoff.copy(), user_id)\n",
    "        token_count_cutoff = user_context_sphere.count_tokens(json.dumps(user_context_cutoff))\n",
    "        user_context_cutoff['token_count'] = token_count_cutoff\n",
    "        user_context_cutoff['removed_comments'] = removed_comments\n",
    "\n",
    "        filename_cutoff = f\"{spheres_dir_cutoff}/{user_id}.json\"\n",
    "        with open(filename_cutoff, 'w', encoding='utf-8') as f:\n",
    "            json.dump(user_context_cutoff, f, indent=2)\n",
    "        print(f\"User context with cutoff saved to {filename_cutoff}\")\n",
    "\n",
    "        print(f\"Token count without cutoff: {token_count_no_cutoff}\")\n",
    "        print(f\"Token count with cutoff: {token_count_cutoff}\")\n",
    "        print(f\"Removed comments: {removed_comments}\")\n",
    "    else:\n",
    "        error_message = {\"error\": f\"No data found for user ID {user_id}\"}\n",
    "\n",
    "        # Save error message to both directories\n",
    "        for dir_path in [spheres_dir_no_cutoff, spheres_dir_cutoff]:\n",
    "            filename = f\"{dir_path}/{user_id}_error.json\"\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(error_message, f, indent=2)\n",
    "            print(f\"Error message saved to {filename}\")"
   ],
   "id": "4e968013fc06201",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data...\n",
      "Preprocessed data loaded from ../data/preprocessed/preprocessed_data.pkl\n",
      "User context without cutoff saved to spheres/no_cutoff/499749.json\n",
      "User context with cutoff saved to spheres/cutoff/499749.json\n",
      "Token count without cutoff: 3551063\n",
      "Token count with cutoff: 1315251\n",
      "Removed comments: 1593\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
