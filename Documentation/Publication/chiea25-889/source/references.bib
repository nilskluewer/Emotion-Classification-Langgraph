
@article{kambhampati_can_2024,
	title = {Can {Large} {Language} {Models} {Reason} and {Plan}?},
	issn = {0077-8923, 1749-6632},
	url = {http://arxiv.org/abs/2403.04121},
	doi = {10.1111/nyas.15125},
	abstract = {While humans sometimes do show the capability of correcting their own erroneous guesses with self-critiquing, there seems to be no basis for that assumption in the case of LLMs.},
	urldate = {2024-03-18},
	journal = {Annals of the New York Academy of Sciences},
	author = {Kambhampati, Subbarao},
	month = mar,
	year = {2024},
	note = {arXiv:2403.04121 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	pages = {nyas.15125},
}

@article{hevner_design_2004,
	title = {Design science in information systems research},
	journal = {MIS quarterly},
	author = {Hevner, Alan R and March, Salvatore T and Park, Jinsoo and Ram, Sudha},
	year = {2004},
	note = {Publisher: JSTOR},
	pages = {75--105},
}

@book{barrett_how_2017,
	title = {How {Emotions} {Are} {Made}: {The} {Secret} {Life} of the {Brain}},
	isbn = {978-1-5098-3751-9},
	shorttitle = {How {Emotions} {Are} {Made}},
	url = {https://books.google.at/books?id=vjIvDQAAQBAJ},
	abstract = {'How Emotions Are Made did what all great books do. It took a subject I thought I understood and turned my understanding upside down' – Malcolm Gladwell, author of The Tipping PointWhen you feel anxious, angry, happy, or surprised, what's really going on inside of you?Uncover fascinating insights into the human mind with How Emotions Are Made by Lisa Feldman Barrett, a pioneer in neuroscience and psychology. This profound book will dismantle and reconstruct your understanding of your own emotions.The world perceives our emotions as automatic and reactive, a response to the world around us. But How Emotions Are Made poses a compelling new perspective, suggesting emotions aren't universally pre-installed, rather they are unique psychological experiences constructed through our personal history, physiology, and environment.This new view of emotions has serious implications:- when judges issue lesser sentences for crimes of passion- when police officers fire at threatening suspects- when doctors choose between one diagnosis and anotherThey're all, in some way, relying on the ancient assumption that emotions are hardwired into our brains and bodies. Revising that conception of emotion isn't just good science, Barrett shows; it's vital to our well-being and the health of society itself.},
	language = {en},
	publisher = {Pan Macmillan},
	author = {Barrett, Lisa Feldman},
	month = mar,
	year = {2017},
	note = {Google-Books-ID: vjIvDQAAQBAJ},
	keywords = {Psychology / Cognitive Psychology \& Cognition, Psychology / Emotions, Science / Cognitive Science, Science / General, Science / Life Sciences / Neuroscience},
}

@article{mayring_qualitative_2014,
	title = {Qualitative content analysis: theoretical foundation, basic procedures and software solution},
	author = {Mayring, Philipp},
	year = {2014},
	note = {Publisher: AUT},
}

@article{kohlbacher_use_2006,
	title = {The {Use} of {Qualitative} {Content} {Analysis} in {Case} {Study} {Research}},
	volume = {Vol 7},
	copyright = {This work is licensed under a Creative Commons Attribution 4.0 International License.},
	url = {http://www.qualitative-research.net/index.php/fqs/article/view/75},
	doi = {10.17169/FQS-7.1.75},
	abstract = {This paper aims at exploring and discussing the possibilities of applying qualitative content analysis as a (text) interpretation method in case study research. First, case study research as a research strategy within qualitative social research is briefly presented. Then, a basic introduction to (qualitative) content analysis as an interpretation method for qualitative interviews and other data material is given. Finally the use of qualitative content analysis for developing case studies is examined and evaluated. The author argues in favor of both case study research as a research strategy and qualitative content analysis as a method of examination of data material and seeks to encourage the integration of qualitative content analysis into the data analysis in case study research.},
	language = {en},
	urldate = {2025-02-17},
	journal = {Forum Qualitative Sozialforschung / Forum: Qualitative Social Research},
	author = {Kohlbacher, Florian},
	month = jan,
	year = {2006},
	note = {Publisher: Forum Qualitative Sozialforschung / Forum: Qualitative Social Research},
	keywords = {case study research; content analysis; qualitative content analysis; qualitative research},
	pages = {No 1 (2006): Learning About Risk},
}

@misc{google_safety_2025,
	title = {Safety and content filters {\textbar} {Generative} {AI}},
	url = {https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters},
	language = {en},
	urldate = {2025-02-15},
	journal = {Google Cloud},
	author = {Google},
	year = {2025},
}

@misc{langsmith_langsmith_2024,
	title = {{LangSmith}},
	url = {https://www.langchain.com/langsmith},
	abstract = {Get your LLM app from prototype to production.},
	language = {en},
	urldate = {2024-09-13},
	author = {Langsmith},
	month = sep,
	year = {2024},
}

@article{handa_which_2025,
	title = {Which {Economic} {Tasks} are {Performed} with {AI}? {Evidence} from {Millions} of {Claude} {Conversations}},
	abstract = {Despite widespread speculation about artificial intelligence’s impact on the future of work, we lack systematic empirical evidence about how these systems are actually being used for different tasks. Here, we present a novel framework for measuring AI usage patterns across the economy. We leverage a recent privacy-preserving system [Tamkin et al., 2024] to analyze over four million Claude.ai conversations through the lens of tasks and occupations in the U.S. Department of Labor’s O*NET Database. Our analysis reveals that AI usage primarily concentrates in software development and writing tasks, which together account for nearly half of all total usage. However, usage of AI extends more broadly across the economy, with ∼ 36\% of occupations using AI for at least a quarter of their associated tasks. We also analyze how AI is being used for tasks, finding 57\% of usage suggests augmentation of human capabilities (e.g., learning or iterating on an output) while 43\% suggests automation (e.g., fulfilling a request with minimal human involvement). While our data and methods face important limitations and only paint a picture of AI usage on a single platform, they provide an automated, granular approach for tracking AI’s evolving role in the economy and identifying leading indicators of future impact as these technologies continue to advance.},
	language = {en},
	author = {Handa, Kunal and Tamkin, Alex and McCain, Miles and Huang, Saffron and Durmus, Esin and Heck, Sarah and Mueller, Jared and Hong, Jerry and Ritchie, Stuart and Belonax, Tim and Troy, Kevin K and Amodei, Dario and Kaplan, Jared and Clark, Jack and Ganguli, Deep},
	month = feb,
	year = {2025},
}

@misc{tornberg_chatgpt-4_2023,
	title = {{ChatGPT}-4 {Outperforms} {Experts} and {Crowd} {Workers} in {Annotating} {Political} {Twitter} {Messages} with {Zero}-{Shot} {Learning}},
	url = {http://arxiv.org/abs/2304.06588},
	doi = {10.48550/arXiv.2304.06588},
	abstract = {This paper assesses the accuracy, reliability and bias of the Large Language Model (LLM) ChatGPT-4 on the text analysis task of classifying the political affiliation of a Twitter poster based on the content of a tweet. The LLM is compared to manual annotation by both expert classifiers and crowd workers, generally considered the gold standard for such tasks. We use Twitter messages from United States politicians during the 2020 election, providing a ground truth against which to measure accuracy. The paper finds that ChatGPT-4 has achieves higher accuracy, higher reliability, and equal or lower bias than the human classifiers. The LLM is able to correctly annotate messages that require reasoning on the basis of contextual knowledge, and inferences around the author's intentions - traditionally seen as uniquely human abilities. These findings suggest that LLM will have substantial impact on the use of textual data in the social sciences, by enabling interpretive research at a scale.},
	urldate = {2025-02-12},
	publisher = {arXiv},
	author = {Törnberg, Petter},
	month = apr,
	year = {2023},
	note = {arXiv:2304.06588 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Social and Information Networks},
}

@misc{noauthor_emotional_nodate,
	title = {Emotional intelligence of {Large} {Language} {Models}},
	url = {https://journals.sagepub.com/doi/epub/10.1177/18344909231213958},
	language = {en},
	urldate = {2025-02-12},
	doi = {10.1177/18344909231213958},
}

@misc{noauthor_emotional_nodate-1,
	title = {Emotional intelligence of {Large} {Language} {Models} - {Xuena} {Wang}, {Xueting} {Li}, {Zi} {Yin}, {Yue} {Wu}, {Jia} {Liu}, 2023},
	url = {https://journals.sagepub.com/doi/full/10.1177/18344909231213958},
	urldate = {2025-02-12},
}

@misc{li_large_2023,
	title = {Large {Language} {Models} {Understand} and {Can} be {Enhanced} by {Emotional} {Stimuli}},
	url = {http://arxiv.org/abs/2307.11760},
	doi = {10.48550/arXiv.2307.11760},
	abstract = {Emotional intelligence significantly impacts our daily behaviors and interactions. Although Large Language Models (LLMs) are increasingly viewed as a stride toward artificial general intelligence, exhibiting impressive performance in numerous tasks, it is still uncertain if LLMs can genuinely grasp psychological emotional stimuli. Understanding and responding to emotional cues gives humans a distinct advantage in problem-solving. In this paper, we take the first step towards exploring the ability of LLMs to understand emotional stimuli. To this end, we first conduct automatic experiments on 45 tasks using various LLMs, including Flan-T5-Large, Vicuna, Llama 2, BLOOM, ChatGPT, and GPT-4. Our tasks span deterministic and generative applications that represent comprehensive evaluation scenarios. Our automatic experiments show that LLMs have a grasp of emotional intelligence, and their performance can be improved with emotional prompts (which we call "EmotionPrompt" that combines the original prompt with emotional stimuli), e.g., 8.00\% relative performance improvement in Instruction Induction and 115\% in BIG-Bench. In addition to those deterministic tasks that can be automatically evaluated using existing metrics, we conducted a human study with 106 participants to assess the quality of generative tasks using both vanilla and emotional prompts. Our human study results demonstrate that EmotionPrompt significantly boosts the performance of generative tasks (10.9\% average improvement in terms of performance, truthfulness, and responsibility metrics). We provide an in-depth discussion regarding why EmotionPrompt works for LLMs and the factors that may influence its performance. We posit that EmotionPrompt heralds a novel avenue for exploring interdisciplinary knowledge for human-LLMs interaction.},
	urldate = {2025-02-12},
	publisher = {arXiv},
	author = {Li, Cheng and Wang, Jindong and Zhang, Yixuan and Zhu, Kaijie and Hou, Wenxin and Lian, Jianxun and Luo, Fang and Yang, Qiang and Xie, Xing},
	month = nov,
	year = {2023},
	note = {arXiv:2307.11760 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
}

@misc{beltz_qualitative_nodate,
	title = {Qualitative {Interviewforschung}},
	url = {https://www.beltz.de/fachmedien/soziologie/produkte/details/15051-qualitative-interviewforschung.html},
	abstract = {In dem Methodenbuch wird forschungsphasenorientiert sowohl methodologisch umfassend als auch praxisnah in die zentralen Aspekte qualitativer Interviewforschung eingeführt. Dabei wird ein integrativer Ansatz verfolgt, der in den verschiedenen Forschungsphasen und -dimensionen versucht, ein zentrales Ziel nicht aus den Augen zu verlieren: die Offenheit gegenüber den Forschungsgegenständen und den Forschungsprozessen vor dem Hintergrund der methodischen Herausforderungen und Problemen qualitativer Sozial-/Interviewforschung.},
	language = {de-DE},
	urldate = {2025-02-12},
	author = {beltz},
}

@article{merton_focused_1946,
	title = {The {Focused} {Interview}},
	volume = {51},
	issn = {0002-9602},
	url = {https://www.journals.uchicago.edu/doi/abs/10.1086/219886},
	doi = {10.1086/219886},
	abstract = {The focused interview is designed to determine the responses of persons exposed to a situation previously analyzed by the investigator. Its chief functions are to discover: (1) the significant aspects of the total situation to which response has occurred; (2) discrepancies between anticipated and actual effects; (3) responses of deviant subgroups in the population; and (4) the processes involved in experimentally induced effects. Procedures for satisfying the criteria of specificity, range, and depth in the interview are described.},
	number = {6},
	urldate = {2025-02-12},
	journal = {American Journal of Sociology},
	author = {Merton, Robert K. and Kendall, Patricia L.},
	month = may,
	year = {1946},
	note = {Publisher: The University of Chicago Press},
	pages = {541--557},
}

@misc{team_gemini_2024,
	title = {Gemini 1.5: {Unlocking} multimodal understanding across millions of tokens of context},
	shorttitle = {Gemini 1.5},
	url = {http://arxiv.org/abs/2403.05530},
	doi = {10.48550/arXiv.2403.05530},
	abstract = {In this report, we present the latest model of the Gemini family, Gemini 1.5 Pro, a highly compute-efficient multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. Gemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasks across modalities, improves the state-of-the-art in long-document QA, long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5 Pro's long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval ({\textgreater}99\%) up to at least 10M tokens, a generational leap over existing models such as Claude 2.1 (200k) and GPT-4 Turbo (128k). Finally, we highlight surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content.},
	urldate = {2025-02-10},
	publisher = {arXiv},
	author = {Team, Gemini and Reid, Machel and Savinov, Nikolay and Teplyashin, Denis and Dmitry and Lepikhin and Lillicrap, Timothy and Alayrac, Jean-baptiste and Soricut, Radu and Lazaridou, Angeliki and Firat, Orhan and Schrittwieser, Julian and Antonoglou, Ioannis and Anil, Rohan and Borgeaud, Sebastian and Dai, Andrew and Millican, Katie and Dyer, Ethan and Glaese, Mia and Sottiaux, Thibault and Lee, Benjamin and Viola, Fabio and Reynolds, Malcolm and Xu, Yuanzhong and Molloy, James and Chen, Jilin and Isard, Michael and Barham, Paul and Hennigan, Tom and McIlroy, Ross and Johnson, Melvin and Schalkwyk, Johan and Collins, Eli and Rutherford, Eliza and Moreira, Erica and Ayoub, Kareem and Goel, Megha and Meyer, Clemens and Thornton, Gregory and Yang, Zhen and Michalewski, Henryk and Abbas, Zaheer and Schucher, Nathan and Anand, Ankesh and Ives, Richard and Keeling, James and Lenc, Karel and Haykal, Salem and Shakeri, Siamak and Shyam, Pranav and Chowdhery, Aakanksha and Ring, Roman and Spencer, Stephen and Sezener, Eren and Vilnis, Luke and Chang, Oscar and Morioka, Nobuyuki and Tucker, George and Zheng, Ce and Woodman, Oliver and Attaluri, Nithya and Kocisky, Tomas and Eltyshev, Evgenii and Chen, Xi and Chung, Timothy and Selo, Vittorio and Brahma, Siddhartha and Georgiev, Petko and Slone, Ambrose and Zhu, Zhenkai and Lottes, James and Qiao, Siyuan and Caine, Ben and Riedel, Sebastian and Tomala, Alex and Chadwick, Martin and Love, Juliette and Choy, Peter and Mittal, Sid and Houlsby, Neil and Tang, Yunhao and Lamm, Matthew and Bai, Libin and Zhang, Qiao and He, Luheng and Cheng, Yong and Humphreys, Peter and Li, Yujia and Brin, Sergey and Cassirer, Albin and Miao, Yingjie and Zilka, Lukas and Tobin, Taylor and Xu, Kelvin and Proleev, Lev and Sohn, Daniel and Magni, Alberto and Hendricks, Lisa Anne and Gao, Isabel and Ontanon, Santiago and Bunyan, Oskar and Byrd, Nathan and Sharma, Abhanshu and Zhang, Biao and Pinto, Mario and Sinha, Rishika and Mehta, Harsh and Jia, Dawei and Caelles, Sergi and Webson, Albert and Morris, Alex and Roelofs, Becca and Ding, Yifan and Strudel, Robin and Xiong, Xuehan and Ritter, Marvin and Dehghani, Mostafa and Chaabouni, Rahma and Karmarkar, Abhijit and Lai, Guangda and Mentzer, Fabian and Xu, Bibo and Li, YaGuang and Zhang, Yujing and Paine, Tom Le and Goldin, Alex and Neyshabur, Behnam and Baumli, Kate and Levskaya, Anselm and Laskin, Michael and Jia, Wenhao and Rae, Jack W. and Xiao, Kefan and He, Antoine and Giordano, Skye and Yagati, Lakshman and Lespiau, Jean-Baptiste and Natsev, Paul and Ganapathy, Sanjay and Liu, Fangyu and Martins, Danilo and Chen, Nanxin and Xu, Yunhan and Barnes, Megan and May, Rhys and Vezer, Arpi and Oh, Junhyuk and Franko, Ken and Bridgers, Sophie and Zhao, Ruizhe and Wu, Boxi and Mustafa, Basil and Sechrist, Sean and Parisotto, Emilio and Pillai, Thanumalayan Sankaranarayana and Larkin, Chris and Gu, Chenjie and Sorokin, Christina and Krikun, Maxim and Guseynov, Alexey and Landon, Jessica and Datta, Romina and Pritzel, Alexander and Thacker, Phoebe and Yang, Fan and Hui, Kevin and Hauth, Anja and Yeh, Chih-Kuan and Barker, David and Mao-Jones, Justin and Austin, Sophia and Sheahan, Hannah and Schuh, Parker and Svensson, James and Jain, Rohan and Ramasesh, Vinay and Briukhov, Anton and Chung, Da-Woon and Glehn, Tamara von and Butterfield, Christina and Jhakra, Priya and Wiethoff, Matthew and Frye, Justin and Grimstad, Jordan and Changpinyo, Beer and Lan, Charline Le and Bortsova, Anna and Wu, Yonghui and Voigtlaender, Paul and Sainath, Tara and Gu, Shane and Smith, Charlotte and Hawkins, Will and Cao, Kris and Besley, James and Srinivasan, Srivatsan and Omernick, Mark and Gaffney, Colin and Surita, Gabriela and Burnell, Ryan and Damoc, Bogdan and Ahn, Junwhan and Brock, Andrew and Pajarskas, Mantas and Petrushkina, Anastasia and Noury, Seb and Blanco, Lorenzo and Swersky, Kevin and Ahuja, Arun and Avrahami, Thi and Misra, Vedant and Liedekerke, Raoul de and Iinuma, Mariko and Polozov, Alex and York, Sarah and Driessche, George van den and Michel, Paul and Chiu, Justin and Blevins, Rory and Gleicher, Zach and Recasens, Adrià and Rrustemi, Alban and Gribovskaya, Elena and Roy, Aurko and Gworek, Wiktor and Arnold, Sébastien M. R. and Lee, Lisa and Lee-Thorp, James and Maggioni, Marcello and Piqueras, Enrique and Badola, Kartikeya and Vikram, Sharad and Gonzalez, Lucas and Baddepudi, Anirudh and Senter, Evan and Devlin, Jacob and Qin, James and Azzam, Michael and Trebacz, Maja and Polacek, Martin and Krishnakumar, Kashyap and Chang, Shuo-yiin and Tung, Matthew and Penchev, Ivo and Joshi, Rishabh and Olszewska, Kate and Muir, Carrie and Wirth, Mateo and Hartman, Ale Jakse and Newlan, Josh and Kashem, Sheleem and Bolina, Vijay and Dabir, Elahe and Amersfoort, Joost van and Ahmed, Zafarali and Cobon-Kerr, James and Kamath, Aishwarya and Hrafnkelsson, Arnar Mar and Hou, Le and Mackinnon, Ian and Frechette, Alexandre and Noland, Eric and Si, Xiance and Taropa, Emanuel and Li, Dong and Crone, Phil and Gulati, Anmol and Cevey, Sébastien and Adler, Jonas and Ma, Ada and Silver, David and Tokumine, Simon and Powell, Richard and Lee, Stephan and Vodrahalli, Kiran and Hassan, Samer and Mincu, Diana and Yang, Antoine and Levine, Nir and Brennan, Jenny and Wang, Mingqiu and Hodkinson, Sarah and Zhao, Jeffrey and Lipschultz, Josh and Pope, Aedan and Chang, Michael B. and Li, Cheng and Shafey, Laurent El and Paganini, Michela and Douglas, Sholto and Bohnet, Bernd and Pardo, Fabio and Odoom, Seth and Rosca, Mihaela and Santos, Cicero Nogueira dos and Soparkar, Kedar and Guez, Arthur and Hudson, Tom and Hansen, Steven and Asawaroengchai, Chulayuth and Addanki, Ravi and Yu, Tianhe and Stokowiec, Wojciech and Khan, Mina and Gilmer, Justin and Lee, Jaehoon and Bostock, Carrie Grimes and Rong, Keran and Caton, Jonathan and Pejman, Pedram and Pavetic, Filip and Brown, Geoff and Sharma, Vivek and Lučić, Mario and Samuel, Rajkumar and Djolonga, Josip and Mandhane, Amol and Sjösund, Lars Lowe and Buchatskaya, Elena and White, Elspeth and Clay, Natalie and Jiang, Jiepu and Lim, Hyeontaek and Hemsley, Ross and Cankara, Zeyncep and Labanowski, Jane and Cao, Nicola De and Steiner, David and Hashemi, Sayed Hadi and Austin, Jacob and Gergely, Anita and Blyth, Tim and Stanton, Joe and Shivakumar, Kaushik and Siddhant, Aditya and Andreassen, Anders and Araya, Carlos and Sethi, Nikhil and Shivanna, Rakesh and Hand, Steven and Bapna, Ankur and Khodaei, Ali and Miech, Antoine and Tanzer, Garrett and Swing, Andy and Thakoor, Shantanu and Aroyo, Lora and Pan, Zhufeng and Nado, Zachary and Sygnowski, Jakub and Winkler, Stephanie and Yu, Dian and Saleh, Mohammad and Maggiore, Loren and Bansal, Yamini and Garcia, Xavier and Kazemi, Mehran and Patil, Piyush and Dasgupta, Ishita and Barr, Iain and Giang, Minh and Kagohara, Thais and Danihelka, Ivo and Marathe, Amit and Feinberg, Vladimir and Elhawaty, Mohamed and Ghelani, Nimesh and Horgan, Dan and Miller, Helen and Walker, Lexi and Tanburn, Richard and Tariq, Mukarram and Shrivastava, Disha and Xia, Fei and Wang, Qingze and Chiu, Chung-Cheng and Ashwood, Zoe and Baatarsukh, Khuslen and Samangooei, Sina and Kaufman, Raphaël Lopez and Alcober, Fred and Stjerngren, Axel and Komarek, Paul and Tsihlas, Katerina and Boral, Anudhyan and Comanescu, Ramona and Chen, Jeremy and Liu, Ruibo and Welty, Chris and Bloxwich, Dawn and Chen, Charlie and Sun, Yanhua and Feng, Fangxiaoyu and Mauger, Matthew and Dotiwalla, Xerxes and Hellendoorn, Vincent and Sharman, Michael and Zheng, Ivy and Haridasan, Krishna and Barth-Maron, Gabe and Swanson, Craig and Rogozińska, Dominika and Andreev, Alek and Rubenstein, Paul Kishan and Sang, Ruoxin and Hurt, Dan and Elsayed, Gamaleldin and Wang, Renshen and Lacey, Dave and Ilić, Anastasija and Zhao, Yao and Iwanicki, Adam and Lince, Alejandro and Chen, Alexander and Lyu, Christina and Lebsack, Carl and Griffith, Jordan and Gaba, Meenu and Sandhu, Paramjit and Chen, Phil and Koop, Anna and Rajwar, Ravi and Yeganeh, Soheil Hassas and Chang, Solomon and Zhu, Rui and Radpour, Soroush and Davoodi, Elnaz and Lei, Ving Ian and Xu, Yang and Toyama, Daniel and Segal, Constant and Wicke, Martin and Lin, Hanzhao and Bulanova, Anna and Badia, Adrià Puigdomènech and Rakićević, Nemanja and Sprechmann, Pablo and Filos, Angelos and Hou, Shaobo and Campos, Víctor and Kassner, Nora and Sachan, Devendra and Fortunato, Meire and Iwuanyanwu, Chimezie and Nikolaev, Vitaly and Lakshminarayanan, Balaji and Jazayeri, Sadegh and Varadarajan, Mani and Tekur, Chetan and Fritz, Doug and Khalman, Misha and Reitter, David and Dasgupta, Kingshuk and Sarcar, Shourya and Ornduff, Tina and Snaider, Javier and Huot, Fantine and Jia, Johnson and Kemp, Rupert and Trdin, Nejc and Vijayakumar, Anitha and Kim, Lucy and Angermueller, Christof and Lao, Li and Liu, Tianqi and Zhang, Haibin and Engel, David and Greene, Somer and White, Anaïs and Austin, Jessica and Taylor, Lilly and Ashraf, Shereen and Liu, Dangyi and Georgaki, Maria and Cai, Irene and Kulizhskaya, Yana and Goenka, Sonam and Saeta, Brennan and Xu, Ying and Frank, Christian and Cesare, Dario de and Robenek, Brona and Richardson, Harry and Alnahlawi, Mahmoud and Yew, Christopher and Ponnapalli, Priya and Tagliasacchi, Marco and Korchemniy, Alex and Kim, Yelin and Li, Dinghua and Rosgen, Bill and Levin, Kyle and Wiesner, Jeremy and Banzal, Praseem and Srinivasan, Praveen and Yu, Hongkun and Ünlü, Çağlar and Reid, David and Tung, Zora and Finchelstein, Daniel and Kumar, Ravin and Elisseeff, Andre and Huang, Jin and Zhang, Ming and Aguilar, Ricardo and Giménez, Mai and Xia, Jiawei and Dousse, Olivier and Gierke, Willi and Yates, Damion and Jalan, Komal and Li, Lu and Latorre-Chimoto, Eri and Nguyen, Duc Dung and Durden, Ken and Kallakuri, Praveen and Liu, Yaxin and Johnson, Matthew and Tsai, Tomy and Talbert, Alice and Liu, Jasmine and Neitz, Alexander and Elkind, Chen and Selvi, Marco and Jasarevic, Mimi and Soares, Livio Baldini and Cui, Albert and Wang, Pidong and Wang, Alek Wenjiao and Ye, Xinyu and Kallarackal, Krystal and Loher, Lucia and Lam, Hoi and Broder, Josef and Holtmann-Rice, Dan and Martin, Nina and Ramadhana, Bramandia and Shukla, Mrinal and Basu, Sujoy and Mohan, Abhi and Fernando, Nick and Fiedel, Noah and Paterson, Kim and Li, Hui and Garg, Ankush and Park, Jane and Choi, DongHyun and Wu, Diane and Singh, Sankalp and Zhang, Zhishuai and Globerson, Amir and Yu, Lily and Carpenter, John and Quitry, Félix de Chaumont and Radebaugh, Carey and Lin, Chu-Cheng and Tudor, Alex and Shroff, Prakash and Garmon, Drew and Du, Dayou and Vats, Neera and Lu, Han and Iqbal, Shariq and Yakubovich, Alex and Tripuraneni, Nilesh and Manyika, James and Qureshi, Haroon and Hua, Nan and Ngani, Christel and Raad, Maria Abi and Forbes, Hannah and Stanway, Jeff and Sundararajan, Mukund and Ungureanu, Victor and Bishop, Colton and Li, Yunjie and Venkatraman, Balaji and Li, Bo and Thornton, Chloe and Scellato, Salvatore and Gupta, Nishesh and Wang, Yicheng and Tenney, Ian and Wu, Xihui and Shenoy, Ashish and Carvajal, Gabriel and Wright, Diana Gage and Bariach, Ben and Xiao, Zhuyun and Hawkins, Peter and Dalmia, Sid and Farabet, Clement and Valenzuela, Pedro and Yuan, Quan and Agarwal, Ananth and Chen, Mia and Kim, Wooyeol and Hulse, Brice and Dukkipati, Nandita and Paszke, Adam and Bolt, Andrew and Choo, Kiam and Beattie, Jennifer and Prendki, Jennifer and Vashisht, Harsha and Santamaria-Fernandez, Rebeca and Cobo, Luis C. and Wilkiewicz, Jarek and Madras, David and Elqursh, Ali and Uy, Grant and Ramirez, Kevin and Harvey, Matt and Liechty, Tyler and Zen, Heiga and Seibert, Jeff and Hu, Clara Huiyi and Khorlin, Andrey and Le, Maigo and Aharoni, Asaf and Li, Megan and Wang, Lily and Kumar, Sandeep and Casagrande, Norman and Hoover, Jay and Badawy, Dalia El and Soergel, David and Vnukov, Denis and Miecnikowski, Matt and Simsa, Jiri and Kumar, Praveen and Sellam, Thibault and Vlasic, Daniel and Daruki, Samira and Shabat, Nir and Zhang, John and Su, Guolong and Zhang, Jiageng and Liu, Jeremiah and Sun, Yi and Palmer, Evan and Ghaffarkhah, Alireza and Xiong, Xi and Cotruta, Victor and Fink, Michael and Dixon, Lucas and Sreevatsa, Ashwin and Goedeckemeyer, Adrian and Dimitriev, Alek and Jafari, Mohsen and Crocker, Remi and FitzGerald, Nicholas and Kumar, Aviral and Ghemawat, Sanjay and Philips, Ivan and Liu, Frederick and Liang, Yannie and Sterneck, Rachel and Repina, Alena and Wu, Marcus and Knight, Laura and Georgiev, Marin and Lee, Hyo and Askham, Harry and Chakladar, Abhishek and Louis, Annie and Crous, Carl and Cate, Hardie and Petrova, Dessie and Quinn, Michael and Owusu-Afriyie, Denese and Singhal, Achintya and Wei, Nan and Kim, Solomon and Vincent, Damien and Nasr, Milad and Choquette-Choo, Christopher A. and Tojo, Reiko and Lu, Shawn and Casas, Diego de Las and Cheng, Yuchung and Bolukbasi, Tolga and Lee, Katherine and Fatehi, Saaber and Ananthanarayanan, Rajagopal and Patel, Miteyan and Kaed, Charbel and Li, Jing and Belle, Shreyas Rammohan and Chen, Zhe and Konzelmann, Jaclyn and Põder, Siim and Garg, Roopal and Koverkathu, Vinod and Brown, Adam and Dyer, Chris and Liu, Rosanne and Nova, Azade and Xu, Jun and Walton, Alanna and Parrish, Alicia and Epstein, Mark and McCarthy, Sara and Petrov, Slav and Hassabis, Demis and Kavukcuoglu, Koray and Dean, Jeffrey and Vinyals, Oriol},
	month = apr,
	year = {2024},
	note = {arXiv:2403.05530 [cs]
version: 2},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{google_gemini_2025,
	title = {Gemini 2.0 is now available to everyone},
	url = {https://blog.google/technology/google-deepmind/gemini-model-updates-february-2025/},
	abstract = {We’re announcing new updates to Gemini 2.0 Flash, plus introducing Gemini 2.0 Flash-Lite and Gemini 2.0 Pro Experimental.},
	language = {en-us},
	urldate = {2025-02-10},
	journal = {Google},
	author = {Google},
	month = feb,
	year = {2025},
}

@misc{google_gemini_2024,
	title = {Gemini 1.5 {Pro} {2M} context window, code execution capabilities, and {Gemma} 2 are available today- {Google} {Developers} {Blog}},
	url = {https://developers.googleblog.com/en/new-features-for-the-gemini-api-and-google-ai-studio/},
	abstract = {Developers will now be able to give Gemini 1.5 Flash a set of pre-trained data of their own and tune it to improve model performance significantly.},
	language = {en},
	urldate = {2025-02-10},
	author = {Google},
	month = jun,
	year = {2024},
}

@article{braun_using_2006,
	title = {Using thematic analysis in psychology},
	volume = {3},
	issn = {1478-0887},
	url = {https://www.tandfonline.com/doi/abs/10.1191/1478088706qp063oa},
	doi = {10.1191/1478088706qp063oa},
	abstract = {Thematic analysis is a poorly demarcated, rarely acknowledged, yet widely used qualitative analytic method within psychology. In this paper, we argue that it offers an accessible and theoretically flexible approach to analysing qualitative data. We outline what thematic analysis is, locating it in relation to other qualitative analytic methods that search for themes or patterns, and in relation to different epistemological and ontological positions. We then provide clear guidelines to those wanting to start thematic analysis, or conduct it in a more deliberate and rigorous way, and consider potential pitfalls in conducting thematic analysis. Finally, we outline the disadvantages and advantages of thematic analysis. We conclude by advocating thematic analysis as a useful and flexible method for qualitative research in and beyond psychology.},
	number = {2},
	urldate = {2025-02-05},
	journal = {Qualitative Research in Psychology},
	author = {Braun, Virginia and Clarke, Victoria},
	month = jan,
	year = {2006},
	note = {Publisher: Routledge
\_eprint: https://www.tandfonline.com/doi/pdf/10.1191/1478088706qp063oa},
	keywords = {epistemology, flexibility, patterns, qualitative psychology, thematic analysis},
	pages = {77--101},
}

@misc{noauthor_introducing_nodate,
	title = {Introducing {Structured} {Outputs} in the {API}},
	url = {https://openai.com/index/introducing-structured-outputs-in-the-api/},
	abstract = {We are introducing Structured Outputs in the API—model outputs now reliably adhere to developer-supplied JSON Schemas.},
	language = {en-US},
	urldate = {2025-02-10},
}

@misc{wang_multimodal_2024,
	title = {Multimodal {Needle} in a {Haystack}: {Benchmarking} {Long}-{Context} {Capability} of {Multimodal} {Large} {Language} {Models}},
	shorttitle = {Multimodal {Needle} in a {Haystack}},
	url = {http://arxiv.org/abs/2406.11230},
	doi = {10.48550/arXiv.2406.11230},
	abstract = {Multimodal Large Language Models (MLLMs) have shown significant promise in various applications, leading to broad interest from researchers and practitioners alike. However, a comprehensive evaluation of their long-context capabilities remains underexplored. To address these gaps, we introduce the MultiModal Needle-in-a-haystack (MMNeedle) benchmark, specifically designed to assess the long-context capabilities of MLLMs. Besides multi-image input, we employ image stitching to further increase the input context length, and develop a protocol to automatically generate labels for sub-image level retrieval. Essentially, MMNeedle evaluates MLLMs by stress-testing their capability to locate a target sub-image (needle) within a set of images (haystack) based on textual instructions and descriptions of image contents. This setup necessitates an advanced understanding of extensive visual contexts and effective information retrieval within long-context image inputs. With this benchmark, we evaluate state-of-the-art MLLMs, encompassing both API-based and open-source models. The findings reveal that GPT-4o consistently surpasses other models in long-context scenarios, but suffers from hallucination problems in negative samples, i.e., when needles are not in the haystacks. Our comprehensive long-context evaluation of MLLMs also sheds lights on the considerable performance gap between API-based and open-source models. All the code, data, and instructions required to reproduce the main results are available at https://github.com/Wang-ML-Lab/multimodal-needle-in-a-haystack.},
	urldate = {2025-02-10},
	publisher = {arXiv},
	author = {Wang, Hengyi and Shi, Haizhou and Tan, Shiwei and Qin, Weiyi and Wang, Wenyuan and Zhang, Tunyu and Nambi, Akshay and Ganu, Tanuja and Wang, Hao},
	month = jun,
	year = {2024},
	note = {arXiv:2406.11230 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{casey_long_2024,
	title = {Long context models in the enterprise: benchmarks and beyond},
	shorttitle = {Long context models in the enterprise},
	url = {https://snorkel.ai/blog/long-context-models-in-the-enterprise-benchmarks-and-beyond/},
	abstract = {Snorkel researchers devised a new way to evaluate long context models and address their "lost-in-the-middle" challenges with mediod voting.},
	language = {en-US},
	urldate = {2025-02-10},
	journal = {Snorkel AI},
	author = {Casey, Matthew},
	month = jun,
	year = {2024},
}

@misc{koo_automata-based_2024,
	title = {Automata-based constraints for language model decoding},
	url = {http://arxiv.org/abs/2407.08103},
	doi = {10.48550/arXiv.2407.08103},
	abstract = {Language models (LMs) are often expected to generate strings in some formal language; for example, structured data, API calls, or code snippets. Although LMs can be tuned to improve their adherence to formal syntax, this does not guarantee conformance, especially with smaller LMs suitable for large-scale deployment. In addition, tuning requires significant resources, making it impractical for uncommon or task-specific formats. To prevent downstream parsing errors we would ideally constrain the LM to only produce valid output, but this is severely complicated by tokenization, which is typically both ambiguous and misaligned with the formal grammar. We solve these issues through the application of automata theory, deriving an efficient closed-form solution for the regular languages, a broad class of formal languages with many practical applications, including API calls or schema-guided JSON and YAML. We also discuss pragmatic extensions for coping with the issue of high branching factor, and extend our techniques to deterministic context-free languages, which similarly admit an efficient closed-form solution. Previous work on this topic (Willard and Louf, 2023) layers bespoke solutions onto automata, leading to problems with speed, correctness, and extensibility. Instead, we reformulate the entire task in terms of automata so we can leverage well-studied and well-optimized algorithms. Our system compiles constraints {\textasciitilde}7,000x faster, is provably correct, and can be extended in a modular fashion.},
	urldate = {2025-02-10},
	publisher = {arXiv},
	author = {Koo, Terry and Liu, Frederick and He, Luheng},
	month = aug,
	year = {2024},
	note = {arXiv:2407.08103 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Formal Languages and Automata Theory},
}

@misc{willard_efficient_2023,
	title = {Efficient {Guided} {Generation} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2307.09702},
	doi = {10.48550/arXiv.2307.09702},
	abstract = {In this article we show how the problem of neural text generation can be constructively reformulated in terms of transitions between the states of a finite-state machine. This framework leads to an efficient approach to guiding text generation with regular expressions and context-free grammars by allowing the construction of an index over a language model's vocabulary. The approach is model agnostic, allows one to enforce domain-specific knowledge and constraints, and enables the construction of reliable interfaces by guaranteeing the structure of the generated text. It adds little overhead to the token sequence generation process and significantly outperforms existing solutions. An implementation is provided in the open source Python library Outlines},
	urldate = {2025-02-10},
	publisher = {arXiv},
	author = {Willard, Brandon T. and Louf, Rémi},
	month = aug,
	year = {2023},
	note = {arXiv:2307.09702 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{tam_let_2024,
	title = {Let {Me} {Speak} {Freely}? {A} {Study} on the {Impact} of {Format} {Restrictions} on {Performance} of {Large} {Language} {Models}},
	shorttitle = {Let {Me} {Speak} {Freely}?},
	url = {http://arxiv.org/abs/2408.02442},
	doi = {10.48550/arXiv.2408.02442},
	abstract = {Structured generation, the process of producing content in standardized formats like JSON and XML, is widely utilized in real-world applications to extract key output information from large language models (LLMs). This study investigates whether such constraints on generation space impact LLMs abilities, including reasoning and domain knowledge comprehension. Specifically, we evaluate LLMs performance when restricted to adhere to structured formats versus generating free-form responses across various common tasks. Surprisingly, we observe a significant decline in LLMs reasoning abilities under format restrictions. Furthermore, we find that stricter format constraints generally lead to greater performance degradation in reasoning tasks.},
	urldate = {2025-02-10},
	publisher = {arXiv},
	author = {Tam, Zhi Rui and Wu, Cheng-Kuang and Tsai, Yi-Lin and Lin, Chieh-Yen and Lee, Hung-yi and Chen, Yun-Nung},
	month = oct,
	year = {2024},
	note = {arXiv:2408.02442 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@book{albert_measuring_2013,
	title = {Measuring the {User} {Experience}: {Collecting}, {Analyzing}, and {Presenting} {Usability} {Metrics}},
	isbn = {978-0-12-415792-7},
	shorttitle = {Measuring the {User} {Experience}},
	abstract = {Measuring the User Experience was the first book that focused on how to quantify the user experience. Now in the second edition, the authors include new material on how recent technologies have made it easier and more effective to collect a broader range of data about the user experience. As more UX and web professionals need to justify their design decisions with solid, reliable data, Measuring the User Experience provides the quantitative analysis training that these professionals need. The second edition presents new metrics such as emotional engagement, personas, keystroke analysis, and net promoter score. It also examines how new technologies coming from neuro-marketing and online market research can refine user experience measurement, helping usability and user experience practitioners make business cases to stakeholders. The book also contains new research and updated examples, including tips on writing online survey questions, six new case studies, and examples using the most recent version of Excel. - Learn which metrics to select for every case, including behavioral, physiological, emotional, aesthetic, gestural, verbal, and physical, as well as more specialized metrics such as eye-tracking and clickstream data - Find a vendor-neutral examination of how to measure the user experience with web sites, digital products, and virtually any other type of product or system - Discover in-depth global case studies showing how organizations have successfully used metrics and the information they revealed - Companion site, www.measuringux.com, includes articles, tools, spreadsheets, presentations, and other resources to help you effectively measure the user experience},
	language = {en},
	publisher = {Newnes},
	author = {Albert, Bill and Tullis, Tom},
	month = may,
	year = {2013},
	note = {Google-Books-ID: bPhLeMBLEkAC},
	keywords = {Computers / Human-Computer Interaction (HCI), Computers / Software Development \& Engineering / Computer Graphics},
}

@article{likert_technique_1932,
	title = {A technique for the measurement of attitudes},
	journal = {Archives of Psychology},
	author = {Likert, Rensis},
	year = {1932},
}

@book{fink_conducting_2019,
	title = {Conducting {Research} {Literature} {Reviews}: {From} the {Internet} to {Paper}},
	isbn = {978-1-5443-1848-6},
	shorttitle = {Conducting {Research} {Literature} {Reviews}},
	abstract = {Providing readers with an accessible, in-depth look at how to synthesize research literature, Conducting Research Literature Reviews: From the Internet to Paper is perfect for students, researchers, marketers, planners, and policymakers who design and manage public and private agencies, conduct research studies, and prepare strategic plans and grant proposals. Bestselling author Arlene Fink shows readers how to explain the need for and significance of research, as well as how to explain a study’s findings.   Offering a step-by-step approach to conducting literature reviews, the Fifth Edition features new research, examples, and references from the social, behavioral, and health sciences, expanded coverage of qualitative research, updated and revised meta-analysis procedures, a brand new glossary of key terms, double the number of exercises, and additional examples of how to write reviews.},
	language = {en},
	publisher = {SAGE Publications},
	author = {Fink, Arlene},
	month = jan,
	year = {2019},
	note = {Google-Books-ID: 0z1\_DwAAQBAJ},
	keywords = {Literary Criticism / Semiotics \& Theory, Social Science / Research},
}

@book{creswell_research_2017,
	title = {Research design: {Qualitative}, quantitative, and mixed methods approaches},
	publisher = {Sage publications},
	author = {Creswell, John W and Creswell, J David},
	year = {2017},
}

@book{bryman_social_2016,
	title = {Social {Research} {Methods}},
	isbn = {978-0-19-968945-3},
	abstract = {This introduction to research methods provides students and researchers with unrivalled coverage of both quantitative and qualitative methods, making it invaluable for anyone embarking on social research.  Bridging the gap between theory and practice, Social Research Methods, Fifth Edition, is packed full of engaging examples and practical tips to equip students with the tools and knowledge needed for them to complete their own research projects. In addition to providing practical advice, author Alan Bryman deftly explores the nature of social research and the wider issues impinging on it.},
	language = {en},
	publisher = {Oxford University Press},
	author = {Bryman, Alan},
	year = {2016},
	note = {Google-Books-ID: N2zQCgAAQBAJ},
	keywords = {Reference / Research, Social Science / Research, Social Science / Statistics},
}

@article{meinke_frontier_2024,
	title = {Frontier models are capable of in-context scheming},
	journal = {arXiv preprint arXiv:2412.04984},
	author = {Meinke, Alexander and Schoen, Bronson and Scheurer, Jérémy and Balesni, Mikita and Shah, Rusheb and Hobbhahn, Marius},
	year = {2024},
}

@misc{snell_scaling_2024,
	title = {Scaling {LLM} {Test}-{Time} {Compute} {Optimally} can be {More} {Effective} than {Scaling} {Model} {Parameters}},
	url = {http://arxiv.org/abs/2408.03314},
	doi = {10.48550/arXiv.2408.03314},
	abstract = {Enabling LLMs to improve their outputs by using more test-time computation is a critical step towards building generally self-improving agents that can operate on open-ended natural language. In this paper, we study the scaling of inference-time computation in LLMs, with a focus on answering the question: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on the achievable performance of LLMs, but also on the future of LLM pretraining and how one should tradeoff inference-time and pre-training compute. Despite its importance, little research attempted to understand the scaling behaviors of various test-time inference methods. Moreover, current work largely provides negative results for a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models; and (2) updating the model's distribution over a response adaptively, given the prompt at test time. We find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a "compute-optimal" scaling strategy, which acts to most effectively allocate test-time compute adaptively per prompt. Using this compute-optimal strategy, we can improve the efficiency of test-time compute scaling by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute can be used to outperform a 14x larger model.},
	urldate = {2025-01-31},
	publisher = {arXiv},
	author = {Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
	month = aug,
	year = {2024},
	note = {arXiv:2408.03314 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{noauthor_multimodal_nodate,
	title = {Multimodal {Prompting} with {Gemini} 1.5: {Working} with {Images} - {Google} {Cloud} {Applied} {AI} {Engineering}},
	url = {https://googlecloudplatform.github.io/applied-ai-engineering-samples/genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_image/},
	urldate = {2025-01-30},
}

@misc{voronov_mind_2024,
	title = {Mind {Your} {Format}: {Towards} {Consistent} {Evaluation} of {In}-{Context} {Learning} {Improvements}},
	shorttitle = {Mind {Your} {Format}},
	url = {http://arxiv.org/abs/2401.06766},
	doi = {10.48550/arXiv.2401.06766},
	abstract = {Large language models demonstrate a remarkable capability for learning to solve new tasks from a few examples. The prompt template, or the way the input examples are formatted to obtain the prompt, is an important yet often overlooked aspect of in-context learning. In this work, we conduct a comprehensive study of the template format's influence on the in-context learning performance. We evaluate the impact of the prompt template across 21 models (from 770M to 70B parameters) and 4 standard classification datasets. We show that a poor choice of the template can reduce the performance of the strongest models and inference methods to a random guess level. More importantly, the best templates do not transfer between different setups and even between models of the same family. Our findings show that the currently prevalent approach to evaluation, which ignores template selection, may give misleading results due to different templates in different works. As a first step towards mitigating this issue, we propose Template Ensembles that aggregate model predictions across several templates. This simple test-time augmentation boosts average performance while being robust to the choice of random set of templates.},
	urldate = {2025-01-30},
	publisher = {arXiv},
	author = {Voronov, Anton and Wolf, Lena and Ryabinin, Max},
	month = jun,
	year = {2024},
	note = {arXiv:2401.06766 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{sclar_quantifying_2024,
	title = {Quantifying {Language} {Models}' {Sensitivity} to {Spurious} {Features} in {Prompt} {Design} or: {How} {I} learned to start worrying about prompt formatting},
	shorttitle = {Quantifying {Language} {Models}' {Sensitivity} to {Spurious} {Features} in {Prompt} {Design} or},
	url = {http://arxiv.org/abs/2310.11324},
	doi = {10.48550/arXiv.2310.11324},
	abstract = {As large language models (LLMs) are adopted as a fundamental component of language technologies, it is crucial to accurately characterize their performance. Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern pre-trained generative language model. In this work, we focus on LLM sensitivity to a quintessential class of meaning-preserving design choices: prompt formatting. We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B. Sensitivity remains even when increasing model size, the number of few-shot examples, or performing instruction tuning. Our analysis suggests that work evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible prompt formats, instead of the currently-standard practice of reporting performance on a single format. We also show that format performance only weakly correlates between models, which puts into question the methodological validity of comparing models with an arbitrarily chosen, fixed prompt format. To facilitate systematic analysis we propose FormatSpread, an algorithm that rapidly evaluates a sampled set of plausible prompt formats for a given task, and reports the interval of expected performance without accessing model weights. Furthermore, we present a suite of analyses that characterize the nature of this sensitivity, including exploring the influence of particular atomic perturbations and the internal representation of particular formats.},
	urldate = {2025-01-30},
	publisher = {arXiv},
	author = {Sclar, Melanie and Choi, Yejin and Tsvetkov, Yulia and Suhr, Alane},
	month = jul,
	year = {2024},
	note = {arXiv:2310.11324 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{sui_table_2024,
	address = {Merida Mexico},
	title = {Table {Meets} {LLM}: {Can} {Large} {Language} {Models} {Understand} {Structured} {Table} {Data}? {A} {Benchmark} and {Empirical} {Study}},
	isbn = {9798400703713},
	shorttitle = {Table {Meets} {LLM}},
	url = {https://dl.acm.org/doi/10.1145/3616855.3635752},
	doi = {10.1145/3616855.3635752},
	language = {en},
	urldate = {2025-01-30},
	booktitle = {Proceedings of the 17th {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {ACM},
	author = {Sui, Yuan and Zhou, Mengyu and Zhou, Mingjie and Han, Shi and Zhang, Dongmei},
	month = mar,
	year = {2024},
	pages = {645--654},
}

@book{t_affective_2004,
	title = {Affective {Neuroscience}: {The} {Foundations} of {Human} and {Animal} {Emotions}},
	isbn = {978-0-19-802567-2},
	shorttitle = {Affective {Neuroscience}},
	abstract = {Some investigators have argued that emotions, especially animal emotions, are illusory concepts outside the realm of scientific inquiry. However, with advances in neurobiology and neuroscience, researchers are demonstrating that this position is wrong as they move closer to a lasting understanding of the biology and psychology of emotion. In Affective Neuroscience, Jaak Panksepp provides the most up-to-date information about the brain-operating systems that organize the fundamental emotional tendencies of all mammals. Presenting complex material in a readable manner, the book offers a comprehensive summary of the fundamental neural sources of human and animal feelings, as well as a conceptual framework for studying emotional systems of the brain. Panksepp approaches emotions from the perspective of basic emotion theory but does not fail to address the complex issues raised by constructionist approaches. These issues include relations to human consciousness and the psychiatric implications of this knowledge. The book includes chapters on sleep and arousal, pleasure and fear systems, the sources of rage and anger, and the neural control of sexuality, as well as the more subtle emotions related to maternal care, social loss, and playfulness. Representing a synthetic integration of vast amounts of neurobehavioral knowledge, including relevant neuroanatomy, neurophysiology, and neurochemistry, this book will be one of the most important contributions to understanding the biology of emotions since Darwins The Expression of the Emotions in Man and Animals},
	language = {en},
	publisher = {Oxford University Press},
	author = {T, Jaak},
	month = sep,
	year = {2004},
	note = {Google-Books-ID: qqcRGagyEuAC},
	keywords = {Medical / Neuroscience, Psychology / Emotions, Psychology / Neuropsychology, Science / Life Sciences / Neuroscience},
}

@misc{dong_survey_2024,
	title = {A {Survey} on {In}-context {Learning}},
	url = {http://arxiv.org/abs/2301.00234},
	doi = {10.48550/arXiv.2301.00234},
	abstract = {With the increasing capabilities of large language models (LLMs), in-context learning (ICL) has emerged as a new paradigm for natural language processing (NLP), where LLMs make predictions based on contexts augmented with a few examples. It has been a significant trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, prompt designing strategies, and related analysis. Additionally, we explore various ICL application scenarios, such as data engineering and knowledge updating. Finally, we address the challenges of ICL and suggest potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.},
	urldate = {2025-01-29},
	publisher = {arXiv},
	author = {Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Ma, Jingyuan and Li, Rui and Xia, Heming and Xu, Jingjing and Wu, Zhiyong and Liu, Tianyu and Chang, Baobao and Sun, Xu and Li, Lei and Sui, Zhifang},
	month = oct,
	year = {2024},
	note = {arXiv:2301.00234 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@book{tomkins_affect_1962,
	title = {Affect {Imagery} {Consciousness}: {Volume} {I}: {The} {Positive} {Affects}},
	isbn = {978-0-8261-0442-7},
	shorttitle = {Affect {Imagery} {Consciousness}},
	abstract = {Tomkins' magnum opus, Affect, Imagery, Consciousness, was published by Springer Publishing Company in four volumes over 30 years. When Tomkins began writing the book in the 1950's, American psychology was dominated by psychoanalytic and behaviorist theories - neither of which placed much importance on the role of basic emotions in everyday human behavior. Tomkins challenged the status quo by developing - over the span of nearly 2,000 pages -- a theory of consciousness and motivation that placed emotion at the core of the human experience. Because so few psychologists were studying emotion at that time, Tomkins drew liberally from other academic disciplines to help formulate his ideas and support his arguments: evolutionary biology, ethology, cybernetics, literature, philosophy, psychoanalysis, and neurophysiology, among others. In the process, Tomkins practically invented the field of "nonverbal behavior" through close observation of emotional expressions in people, including his own infant son. His work was a brilliantly eccentric pastiche of ideas that adhered to no strict disciplinary or ideological boundaries. In time, however, AIC came to prominence through the research of his disciples, notably Paul Ekman and Carroll Izzard, who went on to become major researchers in the psychology of emotion. Today, Tomkins's book is influential not just in psychology but in philosophy, sociology, communication studies, even in "affective computing. Springer Publishing Company is pleased to continue to offer this magisterial work in four volumes.},
	language = {en},
	publisher = {Springer Publishing Company},
	author = {Tomkins, Silvan},
	month = jan,
	year = {1962},
	note = {Google-Books-ID: WIpgNerqaIkC},
	keywords = {Medical / Diseases, Psychology / Emotions, Psychology / General, Psychology / Movements / General, Psychology / Personality, Psychology / Physiological Psychology},
}

@book{izard_human_2013,
	title = {Human {Emotions}},
	isbn = {978-1-4899-2209-0},
	abstract = {In recent years-especially the past decade, in sharp contrast to preceding decades-knowledge in the field of emotions has been steadily increasing. This knowledge comes from many different specialties: Emotion is a truly interdisciplinary subject. Workers in the fields of physiology, neurology, ethology, physiological psychology, personality and social psychology, clinical psychology and psychiatry, medicine, nursing, social work, and the clergy are all directly concerned with emotion. Professions such as law and architecture have an obvious concern with emotions as they affect human motives and needs. The various branches of art, especially the performing arts, certainly deal with the emotions, especially with the expression of emotions. Constantine Stanislavsky, the Russian theatrical genius, revolu tionized modem theater by developing a training method for actors and actresses that emphasized creating genuine emotion on the stage, the emotion appropriate to the character and the life situation being depicted. Indeed, one can hardly think of any human activity that is not related in some way to the field of emotion. Since the contributions to the subject of emotions come from so many different disciplines, it is difficult to find the important common themes that can yield an understanding of the field as a whole. This volume will attempt to make that task easier, but I recognize that no one can treat all of the diverse material expertly and in detail. My aim will be to represent all important types of contributions and perhaps point the way for further and more intensive study of special topics.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Izard, Carroll E.},
	month = nov,
	year = {2013},
	note = {Google-Books-ID: DYoHCAAAQBAJ},
	keywords = {Psychology / Applied Psychology, Psychology / Clinical Psychology, Psychology / Cognitive Psychology \& Cognition, Psychology / Personality},
}

@article{izard_stability_1993,
	title = {Stability of emotion experiences and their relations to traits of personality},
	volume = {64},
	issn = {1939-1315},
	doi = {10.1037/0022-3514.64.5.847},
	abstract = {Presents a theoretical framework for studying emotion–personality relations and an empirical study of the stability of 88 normal middle-class mothers' emotion experiences and their relations to personality during the 3 yrs after childbirth. Ss completed the Differential Emotions Scale (DES), Eysenck's Personality Questionnaire, D. N. Jackson's Personality Research Form, and M. Zuckerman's Sensation Seeking Scale. The DES demonstrated stability over 3 yrs. There was individual stability despite changes in group means during the postpartum period. Positive emotionality, as well as the discrete emotions of interest, enjoyment, and shyness, predicted extraversion. Negative emotionality and the discrete negative emotions were significant predictors of neuroticism. Positive emotionality was inversely related to neuroticism. There were expectable correlations among specific emotions and primary traits of personality. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {5},
	journal = {Journal of Personality and Social Psychology},
	author = {Izard, Carroll E. and Libero, Deborah Z. and Putnam, Priscilla and Haynes, O. Maurice},
	year = {1993},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Emotions, Longitudinal Studies, Mothers, Personality Traits},
	pages = {847--860},
}

@article{werthner_digital_2024,
	title = {Digital {Transformation}, {Digital} {Humanism}: {What} {Needs} to {Be} {Done}},
	journal = {Hannes Werthner· Carlo Ghezzi· Jeff Kramer· Julian Nida-Rümelin· Bashar Nuseibeh· Erich Prem·},
	author = {Werthner, Hannes},
	year = {2024},
	pages = {115},
}

@article{plutchik_nature_2001,
	title = {The {Nature} of {Emotions}: {Human} emotions have deep evolutionary roots, a fact that may explain their complexity and provide tools for clinical practice},
	volume = {89},
	issn = {0003-0996},
	shorttitle = {The {Nature} of {Emotions}},
	url = {https://www.jstor.org/stable/27857503},
	number = {4},
	urldate = {2025-01-23},
	journal = {American Scientist},
	author = {Plutchik, Robert},
	year = {2001},
	note = {Publisher: Sigma Xi, The Scientific Research Society},
	pages = {344--350},
}

@incollection{barrett_navigating_2021,
	title = {Navigating the science of emotion},
	booktitle = {Emotion measurement},
	publisher = {Elsevier},
	author = {Barrett, Lisa Feldman and Westlin, Christiana},
	year = {2021},
	pages = {39--84},
}

@article{hevner_three_2007,
	title = {A three cycle view of design science research},
	volume = {19},
	number = {2},
	journal = {Scandinavian journal of information systems},
	author = {Hevner, Alan R},
	year = {2007},
	pages = {4},
}

@article{barrett_theory_2017,
	title = {The theory of constructed emotion: an active inference account of interoception and categorization},
	volume = {12},
	number = {1},
	journal = {Social cognitive and affective neuroscience},
	author = {Barrett, Lisa Feldman},
	year = {2017},
	note = {Publisher: Oxford University Press},
	keywords = {constructed emotions, critique classical view},
	pages = {1--23},
}

@misc{crawford_atlas_2021,
	title = {The {Atlas} of {AI}: {Power}, {Politics}, and the {Planetary} {Costs} of {Artificial} {Intelligence}},
	publisher = {Yale University Press},
	author = {Crawford, Kate},
	year = {2021},
}

@article{brysbaert_how_2019,
	title = {How many words do we read per minute? {A} review and meta-analysis of reading rate},
	volume = {109},
	issn = {0749-596X},
	shorttitle = {How many words do we read per minute?},
	url = {https://www.sciencedirect.com/science/article/pii/S0749596X19300786},
	doi = {10.1016/j.jml.2019.104047},
	abstract = {Based on the analysis of 190 studies (18,573 participants), we estimate that the average silent reading rate for adults in English is 238 words per minute (wpm) for non-fiction and 260 wpm for fiction. The difference can be predicted by taking into account the length of the words, with longer words in non-fiction than in fiction. The estimates are lower than the numbers often cited in scientific and popular writings. The reasons for the overestimates are reviewed. The average oral reading rate (based on 77 studies and 5965 participants) is 183 wpm. Reading rates are lower for children, old adults, and readers with English as second language. The reading rates are in line with maximum listening speed and do not require the assumption of reading-specific language processing. Within each group/task there are reliable individual differences, which are not yet fully understood. For silent reading of English non-fiction most adults fall in the range of 175–300 wpm; for fiction the range is 200–320 wpm. Reading rates in other languages can be predicted reasonably well by taking into account the number of words these languages require to convey the same message as in English.},
	urldate = {2025-01-18},
	journal = {Journal of Memory and Language},
	author = {Brysbaert, Marc},
	month = dec,
	year = {2019},
	keywords = {Language differences, Oral reading, Reading rate, Reading speed, Silent reading, Words per minute},
	pages = {104047},
}

@misc{owens_multi-llm_2024,
	title = {A {Multi}-{LLM} {Debiasing} {Framework}},
	url = {http://arxiv.org/abs/2409.13884},
	doi = {10.48550/arXiv.2409.13884},
	abstract = {Large Language Models (LLMs) are powerful tools with the potential to benefit society immensely, yet, they have demonstrated biases that perpetuate societal inequalities. Despite significant advancements in bias mitigation techniques using data augmentation, zero-shot prompting, and model fine-tuning, biases continuously persist, including subtle biases that may elude human detection. Recent research has shown a growing interest in multi-LLM approaches, which have been demonstrated to be effective in improving the quality of reasoning and factuality in LLMs. Building on this approach, we propose a novel multi-LLM debiasing framework aimed at reducing bias in LLMs. Our work is the first to introduce and evaluate two distinct approaches within this framework for debiasing LLMs: a centralized method, where the conversation is facilitated by a single central LLM, and a decentralized method, where all models communicate directly. Our findings reveal that our multi-LLM framework significantly reduces bias in LLMs, outperforming the baseline method across several social groups.},
	urldate = {2025-01-16},
	publisher = {arXiv},
	author = {Owens, Deonna M. and Rossi, Ryan A. and Kim, Sungchul and Yu, Tong and Dernoncourt, Franck and Chen, Xiang and Zhang, Ruiyi and Gu, Jiuxiang and Deilamsalehy, Hanieh and Lipka, Nedim},
	month = sep,
	year = {2024},
	note = {arXiv:2409.13884 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@inproceedings{lin_rouge_2004,
	address = {Barcelona, Spain},
	title = {{ROUGE}: {A} {Package} for {Automatic} {Evaluation} of {Summaries}},
	shorttitle = {{ROUGE}},
	url = {https://aclanthology.org/W04-1013/},
	urldate = {2025-01-16},
	booktitle = {Text {Summarization} {Branches} {Out}},
	publisher = {Association for Computational Linguistics},
	author = {Lin, Chin-Yew},
	month = jul,
	year = {2004},
	pages = {74--81},
}

@misc{li_generation_2025,
	title = {From {Generation} to {Judgment}: {Opportunities} and {Challenges} of {LLM}-as-a-judge},
	shorttitle = {From {Generation} to {Judgment}},
	url = {http://arxiv.org/abs/2411.16594},
	doi = {10.48550/arXiv.2411.16594},
	abstract = {Assessment and evaluation have long been critical challenges in artificial intelligence (AI) and natural language processing (NLP). However, traditional methods, whether matching-based or embedding-based, often fall short of judging subtle attributes and delivering satisfactory results. Recent advancements in Large Language Models (LLMs) inspire the "LLM-as-a-judge" paradigm, where LLMs are leveraged to perform scoring, ranking, or selection across various tasks and applications. This paper provides a comprehensive survey of LLM-based judgment and assessment, offering an in-depth overview to advance this emerging field. We begin by giving detailed definitions from both input and output perspectives. Then we introduce a comprehensive taxonomy to explore LLM-as-a-judge from three dimensions: what to judge, how to judge and where to judge. Finally, we compile benchmarks for evaluating LLM-as-a-judge and highlight key challenges and promising directions, aiming to provide valuable insights and inspire future research in this promising research area. Paper list and more resources about LLM-as-a-judge can be found at {\textbackslash}url\{https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge\} and {\textbackslash}url\{https://llm-as-a-judge.github.io\}.},
	urldate = {2025-01-16},
	publisher = {arXiv},
	author = {Li, Dawei and Jiang, Bohan and Huang, Liangjie and Beigi, Alimohammad and Zhao, Chengshuai and Tan, Zhen and Bhattacharjee, Amrita and Jiang, Yuxuan and Chen, Canyu and Wu, Tianhao and Shu, Kai and Cheng, Lu and Liu, Huan},
	month = jan,
	year = {2025},
	note = {arXiv:2411.16594 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{google_generate_2024,
	title = {Generate content with the {Gemini} {Enterprise} {API} {\textbar} {Generative} {AI} on {Vertex} {AI}},
	url = {https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference},
	abstract = {Maximum number of tokens that can be generated in the response. A token is approximately four characters. 100 tokens correspond to roughly 60-80 words.},
	language = {de-x-mtfrom-en},
	urldate = {2025-01-16},
	journal = {Google Cloud},
	author = {Google},
	year = {2024},
	keywords = {Token},
}

@misc{wang_novelqa_2024,
	title = {{NovelQA}: {Benchmarking} {Question} {Answering} on {Documents} {Exceeding} {200K} {Tokens}},
	shorttitle = {{NovelQA}},
	url = {http://arxiv.org/abs/2403.12766},
	doi = {10.48550/arXiv.2403.12766},
	abstract = {The rapid advancement of Large Language Models (LLMs) has introduced a new frontier in natural language processing, particularly in understanding and processing long-context information. However, the evaluation of these models’ longcontext abilities remains a challenge due to the limitations of current benchmarks. To address this gap, we introduce NovelQA, a benchmark specifically designed to test the capabilities of LLMs with extended texts. Constructed from English novels, NovelQA offers a unique blend of complexity, length, and narrative coherence, making it an ideal tool for assessing deep textual understanding in LLMs. This paper presents the design and construction of NovelQA, highlighting its manual annotation, and diverse question types. Our evaluation of Long-context LLMs on NovelQA reveals significant insights into the models’ performance, particularly emphasizing the challenges they face with multi-hop reasoning, detail-oriented questions, and extremely long input with an average length more than 200,000 tokens. The results underscore the necessity for further advancements in LLMs to improve their long-context comprehension and computational literary studies.},
	language = {en},
	urldate = {2025-01-12},
	publisher = {arXiv},
	author = {Wang, Cunxiang and Ning, Ruoxi and Pan, Boqi and Wu, Tonghui and Guo, Qipeng and Deng, Cheng and Bao, Guangsheng and Hu, Xiangkun and Zhang, Zheng and Wang, Qian and Zhang, Yue},
	month = jun,
	year = {2024},
	note = {arXiv:2403.12766 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{karpinska_one_2024,
	title = {One {Thousand} and {One} {Pairs}: {A} "novel" challenge for long-context language models},
	shorttitle = {One {Thousand} and {One} {Pairs}},
	url = {http://arxiv.org/abs/2406.16264},
	doi = {10.48550/arXiv.2406.16264},
	abstract = {Synthetic long-context LLM benchmarks (e.g., “needle-in-the-haystack”) test only surfacelevel retrieval capabilities, but how well can long-context LLMs retrieve, synthesize, and reason over information across book-length inputs? We address this question by creating NOCHA, a dataset of 1,001 minimally different pairs of true and false claims about 67 recentlypublished English fictional books, written by human readers of those books. In contrast to existing long-context benchmarks, our annotators confirm that the largest share of pairs in NOCHA require global reasoning over the entire book to verify. Our experiments show that while human readers easily perform this task, it is enormously challenging for all ten long-context LLMs that we evaluate: no open-weight model performs above random chance (despite their strong performance on synthetic benchmarks), while GPT-4O achieves the highest accuracy at 55.8\%. Further analysis reveals that (1) on average, models perform much better on pairs that require only sentence-level retrieval vs. global reasoning; (2) model-generated explanations for their decisions are often inaccurate even for correctly-labeled claims; and (3) models perform substantially worse on speculative fiction books that contain extensive world-building. The methodology proposed in NOCHA allows for the evolution of the benchmark dataset and the easy analysis of future models.},
	language = {en},
	urldate = {2025-01-12},
	publisher = {arXiv},
	author = {Karpinska, Marzena and Thai, Katherine and Lo, Kyle and Goyal, Tanya and Iyyer, Mohit},
	month = oct,
	year = {2024},
	note = {arXiv:2406.16264 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{levy_same_2024,
	title = {Same {Task}, {More} {Tokens}: the {Impact} of {Input} {Length} on the {Reasoning} {Performance} of {Large} {Language} {Models}},
	shorttitle = {Same {Task}, {More} {Tokens}},
	url = {http://arxiv.org/abs/2402.14848},
	doi = {10.48550/arXiv.2402.14848},
	abstract = {This paper explores the impact of extending input lengths on the capabilities of Large Language Models (LLMs). Despite LLMs advancements in recent times, their performance consistency across different input lengths is not well understood. We investigate this aspect by introducing a novel QA reasoning framework, specifically designed to assess the impact of input length. We isolate the effect of input length using multiple versions of the same sample, each being extended with padding of different lengths, types and locations. Our findings show a notable degradation in LLMs’ reasoning performance at much shorter input lengths than their technical maximum. We show that the degradation trend appears in every version of our dataset, although at different intensities. Additionally, our study reveals that traditional perplexity metrics do not correlate with performance of LLMs’ in long input reasoning tasks. We analyse our results and identify failure modes that can serve as useful guides for future research, potentially informing strategies to address the limitations observed in LLMs.},
	language = {en},
	urldate = {2025-01-12},
	publisher = {arXiv},
	author = {Levy, Mosh and Jacoby, Alon and Goldberg, Yoav},
	month = jul,
	year = {2024},
	note = {arXiv:2402.14848 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{adams_longhealth_2024,
	title = {{LongHealth}: {A} {Question} {Answering} {Benchmark} with {Long} {Clinical} {Documents}},
	shorttitle = {{LongHealth}},
	url = {http://arxiv.org/abs/2401.14490},
	doi = {10.48550/arXiv.2401.14490},
	abstract = {Background: Recent advancements in large language models (LLMs) offer potential benefits in healthcare, particularly in processing extensive patient records. However, existing benchmarks do not fully assess LLMs’ capability in handling real-world, lengthy clinical data.
Methods: We present the LongHealth benchmark, comprising 20 detailed fictional patient cases across various diseases, with each case containing 5,090 to 6,754 words. The benchmark challenges LLMs with 400 multiple-choice questions in three categories: information extraction, negation, and sorting, challenging LLMs to extract and interpret information from large clinical documents.
Results: We evaluated nine open-source LLMs with a minimum of 16,000 tokens and also included OpenAI’s proprietary and cost-efficient GPT-3.5 Turbo for comparison. The highest accuracy was observed for Mixtral-8x7B-Instruct-v0.1, particularly in tasks focused on information retrieval from single and multiple patient documents. However, all models struggled significantly in tasks requiring the identification of missing information, highlighting a critical area for improvement in clinical data interpretation.
Conclusion: While LLMs show considerable potential for processing long clinical documents, their current accuracy levels are insufficient for reliable clinical use, especially in scenarios requiring the identification of missing information. The LongHealth benchmark provides a more realistic assessment of LLMs in a healthcare setting and highlights the need for further model refinement for safe and effective clinical application.},
	language = {en},
	urldate = {2025-01-12},
	publisher = {arXiv},
	author = {Adams, Lisa and Busch, Felix and Han, Tianyu and Excoffier, Jean-Baptiste and Ortala, Matthieu and Löser, Alexander and Aerts, Hugo JWL and Kather, Jakob Nikolas and Truhn, Daniel and Bressem, Keno},
	month = jan,
	year = {2024},
	note = {arXiv:2401.14490 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{reddy_docfinqa_2024,
	title = {{DocFinQA}: {A} {Long}-{Context} {Financial} {Reasoning} {Dataset}},
	shorttitle = {{DocFinQA}},
	url = {http://arxiv.org/abs/2401.06915},
	doi = {10.48550/arXiv.2401.06915},
	abstract = {For large language models (LLMs) to be effective in the financial domain – where each decision can have a significant impact – it is necessary to investigate realistic tasks and data. Financial professionals often interact with documents that are hundreds of pages long, but most financial research datasets only deal with short excerpts from these documents. To address this, we introduce a long-document financial QA task. We augment 7,437 questions from the existing FinQA dataset with the fulldocument context, extending the average context length from under 700 words in FinQA to 123k words in DocFinQA. We conduct extensive experiments over retrieval-based QA pipelines and long-context language models. DocFinQA proves a significant challenge for even state-of-the-art systems. We also provide a case-study on the longest documents in DocFinQA and find that models particularly struggle on these documents. Addressing these challenges may have a wide reaching impact across applications where specificity and long-range contexts are critical, like gene sequences and legal document contract analysis. The data and code is publicly accessible at github.com/anonymous.},
	language = {en},
	urldate = {2025-01-12},
	publisher = {arXiv},
	author = {Reddy, Varshini and Koncel-Kedziorski, Rik and Lai, Viet Dac and Krumdick, Michael and Lovering, Charles and Tanner, Chris},
	month = feb,
	year = {2024},
	note = {arXiv:2401.06915 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{shaham_scrolls_2022,
	title = {{SCROLLS}: {Standardized} {CompaRison} {Over} {Long} {Language} {Sequences}},
	shorttitle = {{SCROLLS}},
	url = {http://arxiv.org/abs/2201.03533},
	doi = {10.48550/arXiv.2201.03533},
	abstract = {NLP benchmarks have largely focused on short texts, such as sentences and paragraphs, even though long texts comprise a considerable amount of natural language in the wild. We introduce SCROLLS, a suite of tasks that require reasoning over long texts. We examine existing long-text datasets, and handpick ones where the text is naturally long, while prioritizing tasks that involve synthesizing information across the input. SCROLLS contains summarization, question answering, and natural language inference tasks, covering multiple domains, including literature, science, business, and entertainment. Initial baselines, including Longformer Encoder-Decoder, indicate that there is ample room for improvement on SCROLLS. We make all datasets available in a unified text-to-text format and host a live leaderboard to facilitate research on model architecture and pretraining methods.},
	language = {en},
	urldate = {2025-01-12},
	publisher = {arXiv},
	author = {Shaham, Uri and Segal, Elad and Ivgi, Maor and Efrat, Avia and Yoran, Ori and Haviv, Adi and Gupta, Ankit and Xiong, Wenhan and Geva, Mor and Berant, Jonathan and Levy, Omer},
	month = oct,
	year = {2022},
	note = {arXiv:2201.03533 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{openai_openai_2024,
	title = {{OpenAI} o1 {System} {Card}},
	shorttitle = {System {Card} of o1},
	url = {https://openai.com/index/openai-o1-system-card/},
	abstract = {Contains the System Card for o1 - provided by OpenAi},
	language = {English},
	urldate = {2024-06-12},
	publisher = {OpenAi},
	author = {OpenAi},
	month = jun,
	year = {2024},
}

@article{beurer-kellner_large_2023,
	title = {Large {Language} {Models} are {Zero}-{Shot} {Multi}-{Tool} {Users}},
	abstract = {We introduce ACTIONS, a framework and programming environment to facilitate the implementation of tool-augmented language models (LMs). Concretely, we augment LMs with the ability to call actions (arbitrary Python functions), and experiment with different ways of tool discovery and invocation. We find that, while previous works heavily rely on few-shot prompting to teach tool use, a zero-shot, instruction-only approach is enough to achieve competitive performance. At the same time, ACTIONS zero-shot approach also offers a much simpler programming interface, not requiring any involved demonstrations. Building on this, we show how ACTIONS enables LLMs to automatically discover and combine multiple tools to solve complex tasks. Overall, we find that inline tool use as enabled by ACTIONS, outperforms existing tool augmentation approaches, both in arithmetic reasoning tasks and text-based question answering. Our implementation extends the open source LMQL programming language for LM interaction (Beurer-Kellner et al., 2023) and is available at ANONYMIZED (upon publication).},
	language = {en},
	author = {Beurer-Kellner, Luca and Fischer, Marc and Vechev, Martin},
	year = {2023},
}

@article{schmidt_sentence_2024,
	title = {Sentence {Completion} as a {User} {Experience} {Research} {Method}: {Recommendations} {From} an {Experimental} {Study}},
	volume = {36},
	copyright = {https://academic.oup.com/pages/standard-publication-reuse-rights},
	issn = {0953-5438, 1873-7951},
	shorttitle = {Sentence {Completion} as a {User} {Experience} {Research} {Method}},
	url = {https://academic.oup.com/iwc/article/36/1/48/7617597},
	doi = {10.1093/iwc/iwae002},
	abstract = {Abstract
            The aim of this study is to investigate the use of the sentence completion technique (SCT) as a user experience (UX) research method. We conducted an online experiment (N = 400) to test the effect of sentence stem variations on sentence completion outcomes. Using a between-subjects design, half of the participants were exposed to impersonal sentence stems that did not include pronouns while the other half were exposed to stems formulated using first-person pronouns (PR). Additional hypotheses around stem formats (use of redundant stems, imaginative stems, two blanks stems, generic stems, stems prompting others’ perception) were tested using a within-subject design. The results do not support hypothesized differences between the pronoun and no pronoun condition. Findings however show that varying the format of the stem influences response behavior, as measured by variety, quantity and novelty of ideas, as well as the length of response. This study contributes to consolidating the use of SCT as a user research method and proposes actionable recommendations on how to create optimal sentence completion surveys in Human-Computer Interaction (HCI).},
	language = {en},
	number = {1},
	urldate = {2025-01-11},
	journal = {Interacting with Computers},
	author = {Schmidt, Denise and Nebe, Karsten and Lallemand, Carine},
	month = jan,
	year = {2024},
	pages = {48--61},
}

@inproceedings{elvitigala_grand_2024,
	address = {Honolulu HI USA},
	title = {Grand {Challenges} in {SportsHCI}},
	isbn = {9798400703300},
	url = {https://dl.acm.org/doi/10.1145/3613904.3642050},
	doi = {10.1145/3613904.3642050},
	language = {en},
	urldate = {2025-01-10},
	booktitle = {Proceedings of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Elvitigala, Don Samitha and Karahanoğlu, Armağan and Matviienko, Andrii and Turmo Vidal, Laia and Postma, Dees and Jones, Michael D and Montoya, Maria F. and Harrison, Daniel and Elbæk, Lars and Daiber, Florian and Burr, Lisa Anneke and Patibanda, Rakesh and Buono, Paolo and Hämäläinen, Perttu and Van Delden, Robby and Bernhaupt, Regina and Ren, Xipei and Van Rheden, Vincent and Zambetta, Fabio and Van Den Hoven, Elise and Lallemand, Carine and Reidsma, Dennis and Mueller, Florian ‘Floyd’},
	month = may,
	year = {2024},
	pages = {1--20},
}

@misc{noauthor_methods_nodate,
	title = {Methods in human-computer interaction research and practice: challenges and innovations - {A} {Special} {Issue} of {Interacting} with {Computers}},
	shorttitle = {Methods in human-computer interaction research and practice},
	url = {https://academic.oup.com/iwc/pages/methods-in-human-computer-interaction-research-and-practice-challenges-and-innovations},
	abstract = {Editors

Regina Bernhaupt, TU Eindhoven
Nicolai Hansen, Aalborg University
Carine Lallemand, University of Luxembourg and TU Eindhoven
Marta Kristín Lárusdóttir},
	language = {en},
	urldate = {2025-01-10},
	journal = {Oxford Academic},
}

@article{russell_core_2003,
	title = {Core affect and the psychological construction of emotion},
	volume = {110},
	issn = {1939-1471},
	doi = {10.1037/0033-295X.110.1.145},
	abstract = {At the heart of emotion, mood, and any other emotionally charged event are states experienced as simply feeling good or bad, energized or enervated. These states--called core affect--influence reflexes, perception, cognition, and behavior and are influenced by many causes internal and external, but people have no direct access to these causal connections. Core affect can therefore be experienced as freefloating (mood) or can be attributed to some cause (and thereby begin an emotional episode). These basic processes spawn a broad framework that includes perception of the core-affect-altering properties of stimuli, motives, empathy, emotional meta-experience, and affect versus emotion regulation; it accounts for prototypical emotional episodes, such as fear and anger, as core affect attributed to something plus various nonemotional processes. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {1},
	journal = {Psychological Review},
	author = {Russell, James A.},
	year = {2003},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Emotional Control, Emotional Responses, Emotional States},
	pages = {145--172},
}

@misc{noauthor_core_nodate,
	title = {Core affect and the psychological construction of emotion.},
	url = {https://psycnet.apa.org/record/2002-08416-007},
	urldate = {2025-01-10},
}

@misc{yang_large_2024,
	title = {Large {Language} {Models} as {Optimizers}},
	url = {http://arxiv.org/abs/2309.03409},
	doi = {10.48550/arXiv.2309.03409},
	abstract = {Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to our main application in prompt optimization, where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8\% on GSM8K, and by up to 50\% on Big-Bench Hard tasks. Code at https://github.com/google-deepmind/opro.},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Yang, Chengrun and Wang, Xuezhi and Lu, Yifeng and Liu, Hanxiao and Le, Quoc V. and Zhou, Denny and Chen, Xinyun},
	month = apr,
	year = {2024},
	note = {arXiv:2309.03409},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, take a deep breath},
}

@article{wei_chain--thought_2022,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html},
	language = {en},
	urldate = {2024-06-14},
	journal = {Advances in Neural Information Processing Systems},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc V. and Zhou, Denny},
	month = dec,
	year = {2022},
	pages = {24824--24837},
}

@inproceedings{wang_harnessing_2012,
	title = {Harnessing {Twitter} "{Big} {Data}" for {Automatic} {Emotion} {Identification}},
	url = {https://ieeexplore.ieee.org/abstract/document/6406313},
	doi = {10.1109/SocialCom-PASSAT.2012.119},
	abstract = {User generated content on Twitter (produced at an enormous rate of 340 million tweets per day) provides a rich source for gleaning people's emotions, which is necessary for deeper understanding of people's behaviors and actions. Extant studies on emotion identification lack comprehensive coverage of "emotional situations" because they use relatively small training datasets. To overcome this bottleneck, we have automatically created a large emotion-labeled dataset (of about 2.5 million tweets) by harnessing emotion-related hash tags available in the tweets. We have applied two different machine learning algorithms for emotion identification, to study the effectiveness of various feature combinations as well as the effect of the size of the training data on the emotion identification task. Our experiments demonstrate that a combination of unigrams, big rams, sentiment/emotion-bearing words, and parts-of-speech information is most effective for gleaning emotions. The highest accuracy (65.57\%) is achieved with a training data containing about 2 million tweets.},
	urldate = {2025-01-02},
	booktitle = {2012 {International} {Conference} on {Privacy}, {Security}, {Risk} and {Trust} and 2012 {International} {Confernece} on {Social} {Computing}},
	author = {Wang, Wenbo and Chen, Lu and Thirunarayan, Krishnaprasad and Sheth, Amit P.},
	month = sep,
	year = {2012},
	keywords = {Accuracy, Blogs, Educational institutions, Emotion Analysis, Emotion Identification, Emotion Intelligence, Training, Training data, Twitter, USA Councils},
	pages = {587--592},
}

@misc{wang_glue_2019,
	title = {{GLUE}: {A} {Multi}-{Task} {Benchmark} and {Analysis} {Platform} for {Natural} {Language} {Understanding}},
	shorttitle = {{GLUE}},
	url = {http://arxiv.org/abs/1804.07461},
	doi = {10.48550/arXiv.1804.07461},
	abstract = {For natural language understanding (NLU) technology to be maximally useful, both practically and as a scientific object of study, it must be general: it must be able to process language in a way that is not exclusively tailored to any one specific task or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation benchmark (GLUE), a tool for evaluating and analyzing the performance of models across a diverse range of existing NLU tasks. GLUE is model-agnostic, but it incentivizes sharing knowledge across tasks because certain tasks have very limited training data. We further provide a hand-crafted diagnostic test suite that enables detailed linguistic analysis of NLU models. We evaluate baselines based on current methods for multi-task and transfer learning and find that they do not immediately give substantial improvements over the aggregate performance of training a separate model per task, indicating room for improvement in developing general and robust NLU systems.},
	urldate = {2024-06-06},
	publisher = {arXiv},
	author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
	month = feb,
	year = {2019},
	note = {arXiv:1804.07461 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{pang_opinion_2008,
	title = {Opinion {Mining} and {Sentiment} {Analysis}},
	volume = {2},
	issn = {1554-0669, 1554-0677},
	url = {https://www.nowpublishers.com/article/Details/INR-011},
	doi = {10.1561/1500000011},
	abstract = {Opinion Mining and Sentiment Analysis},
	language = {English},
	number = {1–2},
	urldate = {2024-12-18},
	journal = {Foundations and Trends® in Information Retrieval},
	author = {Pang, Bo and Lee, Lillian},
	month = jul,
	year = {2008},
	note = {Publisher: Now Publishers, Inc.},
	pages = {1--135},
}

@article{mohammad_once_2012,
	series = {1) {Computational} {Approaches} to {Subjectivity} and {Sentiment} {Analysis} 2) {Service} {Science} in {Information} {Systems} {Research} : {Special} {Issue} on {PACIS} 2010},
	title = {From once upon a time to happily ever after: {Tracking} emotions in mail and books},
	volume = {53},
	issn = {0167-9236},
	shorttitle = {From once upon a time to happily ever after},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923612001418},
	doi = {10.1016/j.dss.2012.05.030},
	abstract = {In this paper, we show how sentiment analysis can be used in tandem with effective visualizations to quantify and track emotions in mail and books. We study a number of specific datasets and show, among other things, how collections of texts can be organized for affect-based search and how books portray different entities through co-occurring emotion words. Analysis of the Enron Email Corpus reveals that there are marked differences across genders in how they use emotion words in work-place email. Finally, we show that fairy tales have more extreme emotion densities than novels.},
	number = {4},
	urldate = {2025-01-02},
	journal = {Decision Support Systems},
	author = {Mohammad, Saif M.},
	month = nov,
	year = {2012},
	keywords = {Email, Emotion analysis, Enron Corpus, Fairy tales, Google Books Corpus, Lexicon, Sentiment analysis},
	pages = {730--741},
}

@article{lindquist_constructing_2008,
	title = {Constructing {Emotion}: {The} {Experience} of {Fear} as a {Conceptual} {Act}},
	volume = {19},
	issn = {0956-7976},
	shorttitle = {Constructing {Emotion}},
	url = {https://doi.org/10.1111/j.1467-9280.2008.02174.x},
	doi = {10.1111/j.1467-9280.2008.02174.x},
	abstract = {This study examined the hypothesis that emotion is a psychological event constructed from the more basic elements of core affect and conceptual knowledge. Participants were primed with conceptual knowledge of fear, conceptual knowledge of anger, or a neutral prime and then proceeded through an affect-induction procedure designed to induce unpleasant, high-arousal affect or a neutral affective state. As predicted, only those individuals for whom conceptual knowledge of fear had been primed experienced unpleasant core affect as evidence that the world was threatening. This study provides the first experimental support for the hypothesis that people experience world-focused emotion when they conceptualize their core affective state using accessible knowledge about emotion.},
	language = {en},
	number = {9},
	urldate = {2024-12-18},
	journal = {Psychological Science},
	author = {Lindquist, Kristen A. and Barrett, Lisa Feldman},
	month = sep,
	year = {2008},
	note = {Publisher: SAGE Publications Inc},
	pages = {898--903},
}

@article{li_text-based_2014,
	title = {Text-based emotion classification using emotion cause extraction},
	volume = {41},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417413006945},
	doi = {10.1016/j.eswa.2013.08.073},
	abstract = {In recent years, increasing impact of social networks on people’s opinions and decision making has attracted lots of attention. Microblogging, one of the most popular social network applications that allows people to share ideas and discuss over various topics, is taken as a rich resource of opinion and emotion data. In this paper, we propose and implement a novel method for identifying emotions in microblog posts. Unlike traditional approaches which are mostly based on statistical methods, we try to infer and extract the reasons of emotions by importing knowledge and theories from other fields such as Sociology. Based on the theory that a triggering cause event is an integral part of emotion, the technique of emotion cause extraction is used as a crucial step to improve the quality of selected features. First, after thorough analysis on sample data we constructed an automatic rule-based system to detect and extract the cause event of each emotional post. We build an emotion corpus with Chinese microblog posts labeled by human annotators. Then a classifier is trained to classify emotions in microblog posts based on extracted cause events. The overall performance of our system is very promising. The experiment results show that our approach is effective in selecting informative features. Our system outperformed the baseline noticeably in most cases, suggesting its great potential. This exploration should provide a new way to look at the emotion classification task and lay the ground for future research on textual emotion processing.},
	number = {4, Part 2},
	urldate = {2025-01-05},
	journal = {Expert Systems with Applications},
	author = {Li, Weiyuan and Xu, Hua},
	month = mar,
	year = {2014},
	keywords = {Emotion cause extraction, Emotion classification, Microblogging, Weibo},
	pages = {1742--1749},
}

@misc{chiang_chatbot_2024,
	title = {Chatbot {Arena}: {An} {Open} {Platform} for {Evaluating} {LLMs} by {Human} {Preference}},
	shorttitle = {Chatbot {Arena}},
	url = {http://arxiv.org/abs/2403.04132},
	doi = {10.48550/arXiv.2403.04132},
	abstract = {Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies. Our demo is publicly available at {\textbackslash}url\{https://chat.lmsys.org\}.},
	urldate = {2025-01-03},
	publisher = {arXiv},
	author = {Chiang, Wei-Lin and Zheng, Lianmin and Sheng, Ying and Angelopoulos, Anastasios Nikolas and Li, Tianle and Li, Dacheng and Zhang, Hao and Zhu, Banghua and Jordan, Michael and Gonzalez, Joseph E. and Stoica, Ion},
	month = mar,
	year = {2024},
	note = {arXiv:2403.04132 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{breazeal_toward_2003,
	series = {Socially {Interactive} {Robots}},
	title = {Toward sociable robots},
	volume = {42},
	issn = {0921-8890},
	url = {https://www.sciencedirect.com/science/article/pii/S0921889002003731},
	doi = {10.1016/S0921-8890(02)00373-1},
	abstract = {This paper explores the topic of social robots—the class of robots that people anthropomorphize in order to interact with them. From the diverse and growing number of applications for such robots, a few distinct modes of interaction are beginning to emerge. We distinguish four such classes: socially evocative, social interface, socially receptive, and sociable. For the remainder of the paper, we explore a few key features of sociable robots that distinguish them from the others. We use the vocal turn-taking behavior of our robot, Kismet, as a case study to highlight these points.},
	number = {3},
	urldate = {2025-01-02},
	journal = {Robotics and Autonomous Systems},
	author = {Breazeal, Cynthia},
	month = mar,
	year = {2003},
	keywords = {Facial expression, Humanoid robots, Human–robot interaction, Sociable robots, Social interaction},
	pages = {167--175},
}

@misc{google_long_2025,
	title = {Long context {\textbar} {Generative} {AI} on {Vertex} {AI}},
	url = {https://cloud.google.com/vertex-ai/generative-ai/docs/long-context},
	language = {en},
	urldate = {2025-01-09},
	journal = {Google Cloud},
	author = {Google},
	month = jan,
	year = {2025},
}

@misc{google_our_2024,
	title = {Our next-generation model: {Gemini} 1.5},
	shorttitle = {Our next-generation model},
	url = {https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/},
	abstract = {Gemini 1.5 delivers dramatically enhanced performance, with a breakthrough in long-context understanding across modalities.},
	language = {en-us},
	urldate = {2025-01-09},
	journal = {Google},
	author = {Google},
	month = feb,
	year = {2024},
}

@misc{google_controlled_2024,
	title = {Controlled generation {\textbar} {Generative} {AI} on {Vertex} {AI}},
	url = {https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output},
	language = {en},
	urldate = {2024-11-21},
	journal = {Google Cloud},
	author = {Google},
	month = nov,
	year = {2024},
}

@misc{openai_structured_2024,
	title = {Structured {Output}},
	url = {https://platform.openai.com/docs/guides/structured-outputs/how-to-use?context=with_parse#how-to-use},
	abstract = {Explore developer resources, tutorials, API docs, and dynamic examples to get the most out of OpenAI's platform.},
	language = {en},
	urldate = {2025-01-04},
	author = {OpenAi},
	year = {2024},
}

@misc{openai_how_2024,
	title = {How reasoning works},
	url = {https://platform.openai.com/docs/guides/reasoning#how-reasoning-works},
	abstract = {Explore developer resources, tutorials, API docs, and dynamic examples to get the most out of OpenAI's platform.},
	language = {en},
	urldate = {2025-01-03},
	author = {OpenAi},
	year = {2024},
}

@misc{openai_openai_2024-1,
	title = {{OpenAI} - {Prompt} engineering},
	url = {https://platform.openai.com/docs/guides/prompt-engineering},
	abstract = {Explore developer resources, tutorials, API docs, and dynamic examples to get the most out of OpenAI's platform.},
	language = {en},
	urldate = {2025-01-09},
	author = {OpenAi},
	year = {2024},
}

@misc{derstandard_derstandardat_2025,
	title = {{derStandard}.at {\textbar} {Nachrichten}, {Kommentare} \& {Community}},
	url = {https://www.derstandard.at/},
	abstract = {DER STANDARD – Nachrichten in Echtzeit: Lesen Sie jetzt Nachrichten aktuell aus dem online News-Room der führenden Qualitätszeitung in Österreich.},
	language = {de-AT},
	urldate = {2025-01-08},
	journal = {DER STANDARD},
	author = {derStandard},
	year = {2025},
}

@misc{anthropic_use_2024,
	title = {Use {XML} tags to structure your prompts},
	url = {https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/use-xml-tags},
	language = {en},
	urldate = {2025-01-09},
	journal = {Anthropic},
	author = {Anthropic},
	year = {2024},
}

@misc{google_structure_2025,
	title = {Structure prompts {\textbar} {Generative} {AI} on {Vertex} {AI}},
	url = {https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/structure-prompts},
	language = {en},
	urldate = {2025-01-09},
	journal = {Google Cloud},
	author = {Google},
	month = jan,
	year = {2025},
}

@misc{microsoft_guidance-aiguidance_2025,
	title = {guidance-ai/guidance},
	copyright = {MIT},
	url = {https://github.com/guidance-ai/guidance},
	abstract = {A guidance language for controlling large language models.},
	urldate = {2025-01-04},
	publisher = {guidance-ai},
	author = {Microsoft},
	month = jan,
	year = {2025},
	note = {original-date: 2022-11-10T18:21:45Z},
}

@misc{lmql_lmql_nodate,
	title = {{LMQL} is a programming language for {LLM} interaction. {\textbar} {LMQL}},
	url = {https://lmql.ai/},
	urldate = {2024-05-16},
	author = {LMQL},
}

@misc{indydevdan_best_2024,
	title = {{BEST} {Prompt} {Format}: {Markdown}, {XML}, or {Raw}? {CONFIRMED} on {Llama} 3.1 \& {Promptfoo}},
	shorttitle = {{BEST} {Prompt} {Format}},
	url = {https://www.youtube.com/watch?v=W6Z0U11nnhA},
	abstract = {Which prompt format is BEST for your AI agents? Is it Markdown, XML, or Raw Prompts?

🚀 Ready to unlock the true potential of your AI agents? In this video, we're diving deep into the world of prompt formats to find out which one reigns supreme: Markdown, XML, or Raw Prompts. Whether you're a seasoned AI engineer or just starting out, understanding the best prompt format can drastically improve your AI workflows and performance.

🔥 In the past few weeks, we've seen incredible advancements in large language models like Llama-3.1 8B, Llama-3.1 405B, and Mistral Nemo. These models are pushing the boundaries of AI capabilities, and we're here to explore how different prompt formats can optimize their performance. Inspired by Anthropic's XML format insights, this video will help you master prompt engineering and choose the best format for your AI agents.

🛠️ We're not just talking theory; we're running real tests with the Prompt Testing Framework Promptfoo! Watch as we compare Markdown, XML, and Raw Prompts across multiple models, including the latest Llama-3.1 405B and Mistral Nemo. We'll show you how each format performs in various scenarios, from simple bullet summaries to complex YouTube chapter summaries and AI command selectors.

🌟 Hit the like and subscribe for more insights on prompt engineering, AI agents, and agentic workflows. Stay ahead of the curve and transform your AI projects with the best prompt formats.

💡 Key takeaways:
Prompt Format: Discover why XML tags might be the best format for maximizing prompt performance and accuracy.
Surprising Results: See how well raw prompts perform against structured formats (hint: it's not as bad as you think)
Markdown Prompt: Learn when to use Markdown for readability and ease of use.
Raw Prompts: See how raw, unformatted prompts stack up against structured formats.
Llama-3.1: Get insights into the latest models like Llama-3.1 8B and 405B, and how they handle different prompt formats.
Mistral Nemo: Explore the performance of Mistral Nemo in various prompt scenarios.
AI Agent: Understand how to build robust AI agents with the right prompt formats.
Agentic Workflow: Learn how to optimize your agentic workflows for better results.

Join us as we put these prompt formats to the test and find out which one is the best for your AI agents. Whether you're working on AI coding assistants, personal AI assistants, or complex agentic workflows, this video is packed with actionable insights to help you succeed.

🔗 Resources
📄 Why use XML tags https://docs.anthropic.com/en/docs/bu...
🧪 Start testing your prompts https://github.com/disler/elm-itv-ben...
🎥 Previous promptfoo testing video https://docs.anthropic.com/en/docs/bu...
🎓 GPT-4o mini SOTA Accuracy TRICK    • GPT-4o mini Prompt Chain: Legit TRICK...  
📢 Mistral AI large enough https://mistral.ai/news/mistral-large...

📖 Chapters
00:00 Wow Llama 3.1, Mistral Large 2, and Gpt-4o mini
01:25 What's the best prompt format?
02:48 Promptfoo Test 1 - Bullet Summary
05:00 Promptfoo Test 2 - YouTube chapter summaries
07:18 Promptfoo Test 3 - Personal AI Commands
10:03 Promptfoo Test 4 - Nuxt Vue Component Generation
11:11 Promptfoo Test 5 - Code Debugging
14:20 Promptfoo Test 6 - Update Config File
15:50 Promptfoo Test 7 - QA Chatbot
17:03 Promptfoo Test 8 - Script to Key Ideas
18:30 Why use XML? Why use Markdown?

\#prompt \#agentic \#testing},
	urldate = {2025-01-09},
	author = {{IndyDevDan}},
	month = jul,
	year = {2024},
}

@misc{white_prompt_2023,
	title = {A {Prompt} {Pattern} {Catalog} to {Enhance} {Prompt} {Engineering} with {ChatGPT}},
	url = {http://arxiv.org/abs/2302.11382},
	doi = {10.48550/arXiv.2302.11382},
	abstract = {Prompt engineering is an increasingly important skill set needed to converse effectively with large language models (LLMs), such as ChatGPT. Prompts are instructions given to an LLM to enforce rules, automate processes, and ensure speciﬁc qualities (and quantities) of generated output. Prompts are also a form of programming that can customize the outputs and interactions with an LLM.},
	language = {en},
	urldate = {2025-01-09},
	publisher = {arXiv},
	author = {White, Jules and Fu, Quchen and Hays, Sam and Sandborn, Michael and Olea, Carlos and Gilbert, Henry and Elnashar, Ashraf and Spencer-Smith, Jesse and Schmidt, Douglas C.},
	month = feb,
	year = {2023},
	note = {arXiv:2302.11382 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
}

@misc{he_does_2024,
	title = {Does {Prompt} {Formatting} {Have} {Any} {Impact} on {LLM} {Performance}?},
	url = {http://arxiv.org/abs/2411.10541},
	doi = {10.48550/arXiv.2411.10541},
	abstract = {In the realm of Large Language Models (LLMs), prompt optimization is crucial for model performance. Although previous research has explored aspects like rephrasing prompt contexts, using various prompting techniques (like in-context learning and chain-of-thought), and ordering few-shot examples, our understanding of LLM sensitivity to prompt templates remains limited. Therefore, this paper examines the impact of different prompt templates on LLM performance. We formatted the same contexts into various human-readable templates, including plain text, Markdown, JSON, and YAML, and evaluated their impact across tasks like natural language reasoning, code generation, and translation using OpenAI's GPT models. Experiments show that GPT-3.5-turbo's performance varies by up to 40{\textbackslash}\% in a code translation task depending on the prompt template, while larger models like GPT-4 are more robust to these variations. Our analysis highlights the need to reconsider the use of fixed prompt templates, as different formats can significantly affect model performance.},
	urldate = {2025-01-09},
	publisher = {arXiv},
	author = {He, Jia and Rungta, Mukund and Koleczek, David and Sekhon, Arshdeep and Wang, Franklin X. and Hasan, Sadid},
	month = nov,
	year = {2024},
	note = {arXiv:2411.10541 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{gendron_cultural_2014,
	title = {Cultural {Relativity} in {Perceiving} {Emotion} {From} {Vocalizations}},
	volume = {25},
	issn = {0956-7976, 1467-9280},
	url = {https://journals.sagepub.com/doi/10.1177/0956797613517239},
	doi = {10.1177/0956797613517239},
	abstract = {A central question in the study of human behavior is whether certain emotions, such as anger, fear, and sadness, are recognized in nonverbal cues across cultures. We predicted and found that in a concept-free experimental task, participants from an isolated cultural context (the Himba ethnic group from northwestern Namibia) did not freely label Western vocalizations with expected emotion terms. Responses indicate that Himba participants perceived more basic affective properties of valence (positivity or negativity) and to some extent arousal (high or low activation). In a second, concept-embedded task, we manipulated whether the target and foil on a given trial matched in both valence and arousal, neither valence nor arousal, valence only, or arousal only. Himba participants achieved above-chance accuracy only when foils differed from targets in valence only. Our results indicate that the voice can reliably convey affective meaning across cultures, but that perceptions of emotion from the voice are culturally variable.},
	language = {en},
	number = {4},
	urldate = {2025-01-08},
	journal = {Psychological Science},
	author = {Gendron, Maria and Roberson, Debi and Van Der Vyver, Jacoba Marieta and Barrett, Lisa Feldman},
	month = apr,
	year = {2014},
	pages = {911--920},
}

@article{poria_beneath_2023,
	title = {Beneath the {Tip} of the {Iceberg}: {Current} {Challenges} and {New} {Directions} in {Sentiment} {Analysis} {Research}},
	volume = {14},
	issn = {1949-3045},
	shorttitle = {Beneath the {Tip} of the {Iceberg}},
	url = {https://ieeexplore.ieee.org/abstract/document/9260964},
	doi = {10.1109/TAFFC.2020.3038167},
	abstract = {Sentiment analysis as a field has come a long way since it was first introduced as a task nearly 20 years ago. It has widespread commercial applications in various domains like marketing, risk management, market research, and politics, to name a few. Given its saturation in specific subtasks — such as sentiment polarity classification — and datasets, there is an underlying perception that this field has reached its maturity. In this article, we discuss this perception by pointing out the shortcomings and under-explored, yet key aspects of this field necessary to attain true sentiment understanding. We analyze the significant leaps responsible for its current relevance. Further, we attempt to chart a possible course for this field that covers many overlooked and unanswered questions.},
	number = {1},
	urldate = {2025-01-08},
	journal = {IEEE Transactions on Affective Computing},
	author = {Poria, Soujanya and Hazarika, Devamanyu and Majumder, Navonil and Mihalcea, Rada},
	month = jan,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Affective Computing},
	keywords = {Cognition, Context modeling, Market research, Natural language processing, Semantics, Sentiment analysis, Syntactics, Task analysis, aspect based sentiment analysis, bias in sentiment analysis systems, emotion recognition, sarcasm analysis, sentiment analysis, sentiment-aware dialogue generation},
	pages = {108--132},
}

@article{barrett_language_2007,
	title = {Language as context for the perception of emotion},
	volume = {11},
	issn = {1364-6613, 1879-307X},
	url = {https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(07)00153-2},
	doi = {10.1016/j.tics.2007.06.003},
	language = {English},
	number = {8},
	urldate = {2025-01-08},
	journal = {Trends in Cognitive Sciences},
	author = {Barrett, Lisa Feldman and Lindquist, Kristen A. and Gendron, Maria},
	month = aug,
	year = {2007},
	pmid = {17625952},
	note = {Publisher: Elsevier},
	pages = {327--332},
}

@article{yadollahi_current_2017,
	title = {Current {State} of {Text} {Sentiment} {Analysis} from {Opinion} to {Emotion} {Mining}},
	volume = {50},
	issn = {0360-0300},
	url = {https://dl.acm.org/doi/10.1145/3057270},
	doi = {10.1145/3057270},
	abstract = {Sentiment analysis from text consists of extracting information about opinions, sentiments, and even emotions conveyed by writers towards topics of interest. It is often equated to opinion mining, but it should also encompass emotion mining. Opinion mining involves the use of natural language processing and machine learning to determine the attitude of a writer towards a subject. Emotion mining is also using similar technologies but is concerned with detecting and classifying writers emotions toward events or topics. Textual emotion-mining methods have various applications, including gaining information about customer satisfaction, helping in selecting teaching materials in e-learning, recommending products based on users emotions, and even predicting mental-health disorders. In surveys on sentiment analysis, which are often old or incomplete, the strong link between opinion mining and emotion mining is understated. This motivates the need for a different and new perspective on the literature on sentiment analysis, with a focus on emotion mining. We present the state-of-the-art methods and propose the following contributions: (1) a taxonomy of sentiment analysis; (2) a survey on polarity classification methods and resources, especially those related to emotion mining; (3) a complete survey on emotion theories and emotion-mining research; and (4) some useful resources, including lexicons and datasets.},
	number = {2},
	urldate = {2025-01-05},
	journal = {ACM Comput. Surv.},
	author = {Yadollahi, Ali and Shahraki, Ameneh Gholipour and Zaiane, Osmar R.},
	month = may,
	year = {2017},
	pages = {25:1--25:33},
}

@inproceedings{mohammad_semeval-2018_2018,
	address = {New Orleans, Louisiana},
	title = {{SemEval}-2018 {Task} 1: {Affect} in {Tweets}},
	shorttitle = {{SemEval}-2018 {Task} 1},
	url = {https://aclanthology.org/S18-1001/},
	doi = {10.18653/v1/S18-1001},
	abstract = {We present the SemEval-2018 Task 1: Affect in Tweets, which includes an array of subtasks on inferring the affectual state of a person from their tweet. For each task, we created labeled data from English, Arabic, and Spanish tweets. The individual tasks are: 1. emotion intensity regression, 2. emotion intensity ordinal classification, 3. valence (sentiment) regression, 4. valence ordinal classification, and 5. emotion classification. Seventy-five teams (about 200 team members) participated in the shared task. We summarize the methods, resources, and tools used by the participating teams, with a focus on the techniques and resources that are particularly useful. We also analyze systems for consistent bias towards a particular race or gender. The data is made freely available to further improve our understanding of how people convey emotions through language.},
	urldate = {2025-01-05},
	booktitle = {Proceedings of the 12th {International} {Workshop} on {Semantic} {Evaluation}},
	publisher = {Association for Computational Linguistics},
	author = {Mohammad, Saif and Bravo-Marquez, Felipe and Salameh, Mohammad and Kiritchenko, Svetlana},
	editor = {Apidianaki, Marianna and Mohammad, Saif M. and May, Jonathan and Shutova, Ekaterina and Bethard, Steven and Carpuat, Marine},
	month = jun,
	year = {2018},
	pages = {1--17},
}

@article{wu_chinese_2019,
	title = {Chinese {Micro}-{Blog} {Sentiment} {Analysis} {Based} on {Multiple} {Sentiment} {Dictionaries} and {Semantic} {Rule} {Sets}},
	volume = {7},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8936054/?arnumber=8936054},
	doi = {10.1109/ACCESS.2019.2960655},
	abstract = {Sentiment analysis of Chinese micro-blog based on sentiment dictionary has become a challenging research subject in the field of artificial intelligence. However, due to insufficient sentiment words, Chinese micro-blog sentiment analysis is difficult to process high accuracy. Aimed at this issue, we propose a method for constructing multiple sentiment dictionaries, which mainly constructs original sentiment dictionary, emoji dictionary, and other related dictionaries. Among them, we have innovatively constructed a Chinese micro-blog new word sentiment dictionary. Multiple sentiment dictionaries increase the coverage of sentiment words. At the same time, we further analyze semantic rule sets between Chinese micro-blog texts and take the inter-sentence analysis rules and sentence pattern analysis rules into the sentiment analysis of Chinese micro-blog, which further improves the accuracy of Chinese micro-blog sentiment analysis. Finally, based on the method of multiple sentiment dictionaries and semantic rule sets, we propose an algorithm for Chinese micro-blog sentiment calculation from complex sentences to clauses and then from clauses to words, and finally combined with the emoji. This algorithm can accurately classify Chinese micro-blog into positive Chinese micro-blog, negative Chinese micro-blog, and neutral Chinese micro-blog. The experimental results show that this method has greatly improved the sentiment analysis of Chinese micro-blog.},
	urldate = {2025-01-05},
	journal = {IEEE Access},
	author = {Wu, Jiesheng and Lu, Kui and Su, Shuzhi and Wang, Shibing},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {Chinese micro-blog, Classification algorithms, Dictionaries, Feature extraction, Machine learning, Semantics, Sentiment analysis, Social networking (online), semantic rule sets, sentiment calculation, sentiment dictionary},
	pages = {183924--183939},
}

@misc{noauthor_chinese_nodate,
	title = {Chinese {Micro}-{Blog} {Sentiment} {Analysis} {Based} on {Multiple} {Sentiment} {Dictionaries} and {Semantic} {Rule} {Sets} {\textbar} {IEEE} {Journals} \& {Magazine} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/abstract/document/8936054},
	urldate = {2025-01-05},
}

@article{cambria_new_2013,
	title = {New {Avenues} in {Opinion} {Mining} and {Sentiment} {Analysis}},
	volume = {28},
	issn = {1941-1294},
	url = {https://ieeexplore.ieee.org/abstract/document/6468032?casa_token=nkWqOl1u7_gAAAAA:qNHPzk7DLqjwfmxrS9zJMO3BnQ9ofTDmtjXPgvjjlQnYUIkZ9Nlp2b5M74TAAlbiwIsxhuq-Dg},
	doi = {10.1109/MIS.2013.30},
	abstract = {The Web holds valuable, vast, and unstructured information about public opinion. Here, the history, current use, and future of opinion mining and sentiment analysis are discussed, along with relevant techniques and tools.},
	number = {2},
	urldate = {2025-01-05},
	journal = {IEEE Intelligent Systems},
	author = {Cambria, Erik and Schuller, Björn and Xia, Yunqing and Havasi, Catherine},
	month = mar,
	year = {2013},
	note = {Conference Name: IEEE Intelligent Systems},
	keywords = {AI, Context awareness, Data mining, Information analysis, Intelligent systems, Knowledge discovery, Market research, NLP, Natural language processing, Pragmatics, Semantics, intelligent systems, opinion mining, sentiment analysis},
	pages = {15--21},
}

@article{wiebe_annotating_2005,
	title = {Annotating {Expressions} of {Opinions} and {Emotions} in {Language}},
	volume = {39},
	issn = {1572-0218},
	url = {https://doi.org/10.1007/s10579-005-7880-9},
	doi = {10.1007/s10579-005-7880-9},
	abstract = {This paper describes a corpus annotation project to study issues in the manual annotation of opinions, emotions, sentiments, speculations, evaluations and other private states in language. The resulting corpus annotation scheme is described, as well as examples of its use. In addition, the manual annotation process and the results of an inter-annotator agreement study on a 10,000-sentence corpus of articles drawn from the world press are presented.},
	language = {en},
	number = {2},
	urldate = {2025-01-05},
	journal = {Language Resources and Evaluation},
	author = {Wiebe, Janyce and Wilson, Theresa and Cardie, Claire},
	month = may,
	year = {2005},
	keywords = {affect, attitudes, corpus annotation, emotion, natural language processing, opinions, sentiment, subjectivity},
	pages = {165--210},
}

@misc{noauthor_learning_nodate,
	title = {Learning to identify emotions in text {\textbar} {Proceedings} of the 2008 {ACM} symposium on {Applied} computing},
	url = {https://dl.acm.org/doi/abs/10.1145/1363686.1364052?casa_token=WLeiGqGsTT4AAAAA:NtU3XXD6wkOKqQBCPAXPPgUXfRZtCnp04BJcNfbwPCVHKNpiJHpFg7x0alVfH1AGMtAi3ScJJbLW8Q},
	urldate = {2025-01-05},
}

@article{rout_model_2018,
	title = {A model for sentiment and emotion analysis of unstructured social media text},
	volume = {18},
	issn = {1572-9362},
	url = {https://doi.org/10.1007/s10660-017-9257-8},
	doi = {10.1007/s10660-017-9257-8},
	abstract = {Sentiment analysis has applications in diverse contexts such as in the gathering and analysis of opinions of individuals about various products, issues, social, and political events. Understanding public opinion can help improve decision making. Opinion mining is a way of retrieving information via search engines, blogs, microblogs and social networks. Individual opinions are unique to each person, and Twitter tweets are an invaluable source of this type of data. However, the huge volume and unstructured nature of text/opinion data pose a challenge to analyzing the data efficiently. Accordingly, proficient algorithms/computational strategies are required for mining and condensing tweets as well as finding sentiment bearing words. Most existing computational methods/models/algorithms in the literature for identifying sentiments from such unstructured data rely on machine learning techniques with the bag-of-word approach as their basis. In this work, we use both unsupervised and supervised approaches on various datasets. Unsupervised approach is being used for the automatic identification of sentiment for tweets acquired from Twitter public domain. Different machine learning algorithms such as Multinomial Naive Bayes (MNB), Maximum Entropy and Support Vector Machines are applied for sentiment identification of tweets as well as to examine the effectiveness of various feature combinations. In our experiment on tweets, we achieve an accuracy of 80.68\% using the proposed unsupervised approach, in comparison to the lexicon based approach (the latter gives an accuracy of 75.20\%). In our experiments, the supervised approach where we combine unigram, bigram and Part-of-Speech as feature is efficient in finding emotion and sentiment of unstructured data. For short message services, using the unigram feature with MNB classifier allows us to achieve an accuracy of 67\%.},
	language = {en},
	number = {1},
	urldate = {2025-01-05},
	journal = {Electronic Commerce Research},
	author = {Rout, Jitendra Kumar and Choo, Kim-Kwang Raymond and Dash, Amiya Kumar and Bakshi, Sambit and Jena, Sanjay Kumar and Williams, Karen L.},
	month = mar,
	year = {2018},
	keywords = {Bag-of-words, Laplace smoothing, Lexicon, Machine learning, Parts-of-Speech (POS), Sentiment analysis},
	pages = {181--199},
}

@misc{gaind_emotion_2019,
	title = {Emotion {Detection} and {Analysis} on {Social} {Media}},
	url = {http://arxiv.org/abs/1901.08458},
	doi = {10.48550/arXiv.1901.08458},
	abstract = {In this paper, we address the problem of detection, classification and quantification of emotions of text in any form. We consider English text collected from social media like Twitter, which can provide information having utility in a variety of ways, especially opinion mining. Social media like Twitter and Facebook is full of emotions, feelings and opinions of people all over the world. However, analyzing and classifying text on the basis of emotions is a big challenge and can be considered as an advanced form of Sentiment Analysis. This paper proposes a method to classify text into six different Emotion-Categories: Happiness, Sadness, Fear, Anger, Surprise and Disgust. In our model, we use two different approaches and combine them to effectively extract these emotions from text. The first approach is based on Natural Language Processing, and uses several textual features like emoticons, degree words and negations, Parts Of Speech and other grammatical analysis. The second approach is based on Machine Learning classification algorithms. We have also successfully devised a method to automate the creation of the training-set itself, so as to eliminate the need of manual annotation of large datasets. Moreover, we have managed to create a large bag of emotional words, along with their emotion-intensities. On testing, it is shown that our model provides significant accuracy in classifying tweets taken from Twitter.},
	urldate = {2025-01-05},
	publisher = {arXiv},
	author = {Gaind, Bharat and Syal, Varun and Padgalwar, Sneha},
	month = jun,
	year = {2019},
	note = {arXiv:1901.08458 [cs]},
	keywords = {Computer Science - Information Retrieval, Computer Science - Machine Learning, Computer Science - Social and Information Networks},
}

@misc{noauthor_pdf_nodate,
	title = {({PDF}) {Multi}-{Class} {Twitter} {Emotion} {Classification}: {A} {New} {Approach}},
	shorttitle = {({PDF}) {Multi}-{Class} {Twitter} {Emotion} {Classification}},
	url = {https://www.researchgate.net/publication/269670995_Multi-Class_Twitter_Emotion_Classification_A_New_Approach},
	abstract = {PDF {\textbar} Micro blogging today has become a very popular communication tool among Internet users. Millions of users share opinions on different aspects of... {\textbar} Find, read and cite all the research you need on ResearchGate},
	language = {en},
	urldate = {2025-01-05},
	journal = {ResearchGate},
	doi = {10.5120/ijais12-450651},
}

@misc{barbieri_tweeteval_2020,
	title = {{TweetEval}: {Unified} {Benchmark} and {Comparative} {Evaluation} for {Tweet} {Classification}},
	shorttitle = {{TweetEval}},
	url = {http://arxiv.org/abs/2010.12421},
	doi = {10.48550/arXiv.2010.12421},
	abstract = {The experimental landscape in natural language processing for social media is too fragmented. Each year, new shared tasks and datasets are proposed, ranging from classics like sentiment analysis to irony detection or emoji prediction. Therefore, it is unclear what the current state of the art is, as there is no standardized evaluation protocol, neither a strong set of baselines trained on such domain-specific data. In this paper, we propose a new evaluation framework (TweetEval) consisting of seven heterogeneous Twitter-specific classification tasks. We also provide a strong set of baselines as starting point, and compare different language modeling pre-training strategies. Our initial experiments show the effectiveness of starting off with existing pre-trained generic language models, and continue training them on Twitter corpora.},
	urldate = {2025-01-04},
	publisher = {arXiv},
	author = {Barbieri, Francesco and Camacho-Collados, Jose and Neves, Leonardo and Espinosa-Anke, Luis},
	month = oct,
	year = {2020},
	note = {arXiv:2010.12421 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Social and Information Networks},
}

@misc{noauthor_chatbot_nodate,
	title = {Chatbot {Arena} (formerly {LMSYS}): {Free} {AI} {Chat} to {Compare} \& {Test} {Best} {AI} {Chatbots}},
	shorttitle = {Chatbot {Arena} (formerly {LMSYS})},
	url = {https://gradio.app/},
	language = {en-GB},
	urldate = {2025-01-03},
}

@misc{thoppilan_lamda_2022,
	title = {{LaMDA}: {Language} {Models} for {Dialog} {Applications}},
	shorttitle = {{LaMDA}},
	url = {http://arxiv.org/abs/2201.08239},
	doi = {10.48550/arXiv.2201.08239},
	abstract = {We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.},
	urldate = {2025-01-03},
	publisher = {arXiv},
	author = {Thoppilan, Romal and Freitas, Daniel De and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and Li, YaGuang and Lee, Hongrae and Zheng, Huaixiu Steven and Ghafouri, Amin and Menegali, Marcelo and Huang, Yanping and Krikun, Maxim and Lepikhin, Dmitry and Qin, James and Chen, Dehao and Xu, Yuanzhong and Chen, Zhifeng and Roberts, Adam and Bosma, Maarten and Zhao, Vincent and Zhou, Yanqi and Chang, Chung-Ching and Krivokon, Igor and Rusch, Will and Pickett, Marc and Srinivasan, Pranesh and Man, Laichee and Meier-Hellstern, Kathleen and Morris, Meredith Ringel and Doshi, Tulsee and Santos, Renelito Delos and Duke, Toju and Soraker, Johnny and Zevenbergen, Ben and Prabhakaran, Vinodkumar and Diaz, Mark and Hutchinson, Ben and Olson, Kristen and Molina, Alejandra and Hoffman-John, Erin and Lee, Josh and Aroyo, Lora and Rajakumar, Ravi and Butryna, Alena and Lamm, Matthew and Kuzmina, Viktoriya and Fenton, Joe and Cohen, Aaron and Bernstein, Rachel and Kurzweil, Ray and Aguera-Arcas, Blaise and Cui, Claire and Croak, Marian and Chi, Ed and Le, Quoc},
	month = feb,
	year = {2022},
	note = {arXiv:2201.08239 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{yang_accuracy_2024,
	title = {Accuracy and {Political} {Bias} of {News} {Source} {Credibility} {Ratings} by {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2304.00228},
	doi = {10.48550/arXiv.2304.00228},
	abstract = {Search engines increasingly leverage large language models (LLMs) to generate direct answers, and AI chatbots now access the Internet for fresh data. As information curators for billions of users, LLMs must assess the accuracy and reliability of different sources. This paper audits eight widely used LLMs from three major providers -- OpenAI, Google, and Meta -- to evaluate their ability to discern credible and high-quality information sources from low-credibility ones. We find that while LLMs can rate most tested news outlets, larger models more frequently refuse to provide ratings due to insufficient information, whereas smaller models are more prone to hallucination in their ratings. For sources where ratings are provided, LLMs exhibit a high level of agreement among themselves (average Spearman's \${\textbackslash}rho = 0.81\$), but their ratings align only moderately with human expert evaluations (average \${\textbackslash}rho = 0.59\$). Analyzing news sources with different political leanings in the US, we observe a liberal bias in credibility ratings yielded by all LLMs in default configurations. Additionally, assigning partisan identities to LLMs consistently results in strong politically congruent bias in the ratings. These findings have important implications for the use of LLMs in curating news and political information.},
	urldate = {2025-01-03},
	publisher = {arXiv},
	author = {Yang, Kai-Cheng and Menczer, Filippo},
	month = aug,
	year = {2024},
	note = {arXiv:2304.00228 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Information Retrieval},
}

@misc{liang_holistic_2023,
	title = {Holistic {Evaluation} of {Language} {Models}},
	url = {http://arxiv.org/abs/2211.09110},
	doi = {10.48550/arXiv.2211.09110},
	abstract = {Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5\% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze specific aspects (e.g. reasoning, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, 21 of which were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just 17.9\% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0\%: now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency, we release all raw model prompts and completions publicly for further analysis, as well as a general modular toolkit. We intend for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.},
	urldate = {2025-01-03},
	publisher = {arXiv},
	author = {Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and Newman, Benjamin and Yuan, Binhang and Yan, Bobby and Zhang, Ce and Cosgrove, Christian and Manning, Christopher D. and Ré, Christopher and Acosta-Navas, Diana and Hudson, Drew A. and Zelikman, Eric and Durmus, Esin and Ladhak, Faisal and Rong, Frieda and Ren, Hongyu and Yao, Huaxiu and Wang, Jue and Santhanam, Keshav and Orr, Laurel and Zheng, Lucia and Yuksekgonul, Mert and Suzgun, Mirac and Kim, Nathan and Guha, Neel and Chatterji, Niladri and Khattab, Omar and Henderson, Peter and Huang, Qian and Chi, Ryan and Xie, Sang Michael and Santurkar, Shibani and Ganguli, Surya and Hashimoto, Tatsunori and Icard, Thomas and Zhang, Tianyi and Chaudhary, Vishrav and Wang, William and Li, Xuechen and Mai, Yifan and Zhang, Yuhui and Koreeda, Yuta},
	month = oct,
	year = {2023},
	note = {arXiv:2211.09110 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{lopez-lira_can_2024,
	title = {Can {ChatGPT} {Forecast} {Stock} {Price} {Movements}? {Return} {Predictability} and {Large} {Language} {Models}},
	shorttitle = {Can {ChatGPT} {Forecast} {Stock} {Price} {Movements}?},
	url = {http://arxiv.org/abs/2304.07619},
	doi = {10.48550/arXiv.2304.07619},
	abstract = {We document the capability of large language models (LLMs) like ChatGPT to predict stock price movements using news headlines, even without direct financial training. ChatGPT scores significantly predict out-of-sample daily stock returns, subsuming traditional methods, and predictability is stronger among smaller stocks and following negative news. To explain these findings, we develop a theoretical model incorporating information capacity constraints, underreaction, limits-to-arbitrage, and LLMs. The model generates several key predictions, which we empirically test: (i) it establishes a critical threshold in AI capabilities necessary for profitable predictions, (ii) it demonstrates that only advanced LLMs can effectively interpret complex information, and (iii) it predicts that widespread LLM adoption can enhance market efficiency. Our results suggest that sophisticated return forecasting is an emerging capability of AI systems and that these technologies can alter information diffusion and decision-making processes in financial markets. Finally, we introduce an interpretability framework to evaluate LLMs' reasoning, contributing to AI transparency and economic decision-making.},
	urldate = {2025-01-03},
	publisher = {arXiv},
	author = {Lopez-Lira, Alejandro and Tang, Yuehua},
	month = sep,
	year = {2024},
	note = {arXiv:2304.07619 [q-fin]},
	keywords = {Computer Science - Computation and Language, Quantitative Finance - Statistical Finance},
}

@misc{qin_is_2023,
	title = {Is {ChatGPT} a {General}-{Purpose} {Natural} {Language} {Processing} {Task} {Solver}?},
	url = {http://arxiv.org/abs/2302.06476},
	doi = {10.48550/arXiv.2302.06476},
	abstract = {Spurred by advancements in scale, large language models (LLMs) have demonstrated the ability to perform a variety of natural language processing (NLP) tasks zero-shot -- i.e., without adaptation on downstream data. Recently, the debut of ChatGPT has drawn a great deal of attention from the natural language processing (NLP) community due to the fact that it can generate high-quality responses to human input and self-correct previous mistakes based on subsequent conversations. However, it is not yet known whether ChatGPT can serve as a generalist model that can perform many NLP tasks zero-shot. In this work, we empirically analyze the zero-shot learning ability of ChatGPT by evaluating it on 20 popular NLP datasets covering 7 representative task categories. With extensive empirical studies, we demonstrate both the effectiveness and limitations of the current version of ChatGPT. We find that ChatGPT performs well on many tasks favoring reasoning capabilities (e.g., arithmetic reasoning) while it still faces challenges when solving specific tasks such as sequence tagging. We additionally provide in-depth analysis through qualitative case studies.},
	urldate = {2025-01-03},
	publisher = {arXiv},
	author = {Qin, Chengwei and Zhang, Aston and Zhang, Zhuosheng and Chen, Jiaao and Yasunaga, Michihiro and Yang, Diyi},
	month = nov,
	year = {2023},
	note = {arXiv:2302.06476 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{gao_making_2021,
	title = {Making {Pre}-trained {Language} {Models} {Better} {Few}-shot {Learners}},
	url = {http://arxiv.org/abs/2012.15723},
	doi = {10.48550/arXiv.2012.15723},
	abstract = {The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF--better few-shot fine-tuning of language models--a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30\% absolute improvement, and 11\% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning.},
	urldate = {2025-01-03},
	publisher = {arXiv},
	author = {Gao, Tianyu and Fisch, Adam and Chen, Danqi},
	month = jun,
	year = {2021},
	note = {arXiv:2012.15723 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{hendrycks_measuring_2021,
	title = {Measuring {Massive} {Multitask} {Language} {Understanding}},
	url = {http://arxiv.org/abs/2009.03300},
	doi = {10.48550/arXiv.2009.03300},
	abstract = {We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.},
	urldate = {2025-01-03},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
	month = jan,
	year = {2021},
	note = {arXiv:2009.03300 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@inproceedings{wang_superglue_2019,
	title = {{SuperGLUE}: {A} {Stickier} {Benchmark} for {General}-{Purpose} {Language} {Understanding} {Systems}},
	volume = {32},
	shorttitle = {{SuperGLUE}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html},
	abstract = {In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at https://super.gluebenchmark.com.},
	urldate = {2025-01-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
	year = {2019},
}

@misc{anthropic_introducing_nodate,
	title = {Introducing {Contextual} {Retrieval}},
	url = {https://www.anthropic.com/news/contextual-retrieval},
	abstract = {Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.},
	language = {en},
	urldate = {2025-01-03},
	author = {Anthropic},
}

@inproceedings{lewis_retrieval-augmented_2020,
	title = {Retrieval-{Augmented} {Generation} for {Knowledge}-{Intensive} {NLP} {Tasks}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html},
	abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
	urldate = {2025-01-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
	year = {2020},
	pages = {9459--9474},
}

@misc{sebastian_raschka_rasbt_for_2023,
	type = {Tweet},
	title = {For scale: {The} first {Harry} {Potter} book has 76,944 words, i.e {\textasciitilde}100,000 tokens after tokenization.},
	url = {https://x.com/rasbt/status/1656724322164015105},
	language = {en},
	urldate = {2025-01-03},
	journal = {Twitter},
	author = {{Sebastian Raschka [@rasbt]}},
	month = may,
	year = {2023},
}

@misc{zhong_evaluation_2024,
	title = {Evaluation of {OpenAI} o1: {Opportunities} and {Challenges} of {AGI}},
	shorttitle = {Evaluation of {OpenAI} o1},
	url = {http://arxiv.org/abs/2409.18486},
	doi = {10.48550/arXiv.2409.18486},
	abstract = {This comprehensive study evaluates the performance of OpenAI's o1-preview large language model across a diverse array of complex reasoning tasks, spanning multiple domains, including computer science, mathematics, natural sciences, medicine, linguistics, and social sciences. Through rigorous testing, o1-preview demonstrated remarkable capabilities, often achieving human-level or superior performance in areas ranging from coding challenges to scientific reasoning and from language processing to creative problem-solving. Key findings include: -83.3\% success rate in solving complex competitive programming problems, surpassing many human experts. -Superior ability in generating coherent and accurate radiology reports, outperforming other evaluated models. -100\% accuracy in high school-level mathematical reasoning tasks, providing detailed step-by-step solutions. -Advanced natural language inference capabilities across general and specialized domains like medicine. -Impressive performance in chip design tasks, outperforming specialized models in areas such as EDA script generation and bug analysis. -Remarkable proficiency in anthropology and geology, demonstrating deep understanding and reasoning in these specialized fields. -Strong capabilities in quantitative investing. O1 has comprehensive financial knowledge and statistical modeling skills. -Effective performance in social media analysis, including sentiment analysis and emotion recognition. The model excelled particularly in tasks requiring intricate reasoning and knowledge integration across various fields. While some limitations were observed, including occasional errors on simpler problems and challenges with certain highly specialized concepts, the overall results indicate significant progress towards artificial general intelligence.},
	urldate = {2025-01-03},
	publisher = {arXiv},
	author = {Zhong, Tianyang and Liu, Zhengliang and Pan, Yi and Zhang, Yutong and Zhou, Yifan and Liang, Shizhe and Wu, Zihao and Lyu, Yanjun and Shu, Peng and Yu, Xiaowei and Cao, Chao and Jiang, Hanqi and Chen, Hanxu and Li, Yiwei and Chen, Junhao and Hu, Huawen and Liu, Yihen and Zhao, Huaqin and Xu, Shaochen and Dai, Haixing and Zhao, Lin and Zhang, Ruidong and Zhao, Wei and Yang, Zhenyuan and Chen, Jingyuan and Wang, Peilong and Ruan, Wei and Wang, Hui and Zhao, Huan and Zhang, Jing and Ren, Yiming and Qin, Shihuan and Chen, Tong and Li, Jiaxi and Zidan, Arif Hassan and Jahin, Afrar and Chen, Minheng and Xia, Sichen and Holmes, Jason and Zhuang, Yan and Wang, Jiaqi and Xu, Bochen and Xia, Weiran and Yu, Jichao and Tang, Kaibo and Yang, Yaxuan and Sun, Bolun and Yang, Tao and Lu, Guoyu and Wang, Xianqiao and Chai, Lilong and Li, He and Lu, Jin and Sun, Lichao and Zhang, Xin and Ge, Bao and Hu, Xintao and Zhang, Lian and Zhou, Hua and Zhang, Lu and Zhang, Shu and Liu, Ninghao and Jiang, Bei and Kong, Linglong and Xiang, Zhen and Ren, Yudan and Liu, Jun and Jiang, Xi and Bao, Yu and Zhang, Wei and Li, Xiang and Li, Gang and Liu, Wei and Shen, Dinggang and Sikora, Andrea and Zhai, Xiaoming and Zhu, Dajiang and Liu, Tianming},
	month = sep,
	year = {2024},
	note = {arXiv:2409.18486 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{shavit_practices_nodate,
	title = {Practices for {Governing} {Agentic} {AI} {Systems}},
	abstract = {Agentic AI systems—AI systems that can pursue complex goals with limited direct supervision—are likely to be broadly useful if we can integrate them responsibly into our society. While such systems have substantial potential to help people more eﬃciently and eﬀectively achieve their own goals, they also create risks of harm. In this white paper, we suggest a deﬁnition of agentic AI systems and the parties in the agentic AI system life-cycle, and highlight the importance of agreeing on a set of baseline responsibilities and safety best practices for each of these parties. As our primary contribution, we oﬀer an initial set of practices for keeping agents’ operations safe and accountable, which we hope can serve as building blocks in the development of agreed baseline best practices. We enumerate the questions and uncertainties around operationalizing each of these practices that must be addressed before such practices can be codiﬁed. We then highlight categories of indirect impacts from the wide-scale adoption of agentic AI systems, which are likely to necessitate additional governance frameworks.},
	language = {en},
	author = {Shavit, Yonadav and O’Keefe, Cullen and Eloundou, Tyna and McMillan, Paul and Agarwal, Sandhini and Brundage, Miles and Adler, Steven and Campbell, Rosie and Lee, Teddy and Mishkin, Pamela and Hickey, Alan and Slama, Katarina and Ahmad, Lama and Beutel, Alex and Passos, Alexandre and Robinson, David G},
}

@misc{noauthor_learning_nodate-1,
	title = {Learning to {Reason} with {LLMs}},
	url = {https://openai.com/index/learning-to-reason-with-llms/},
	abstract = {We are introducing OpenAI o1, a new large language model trained with reinforcement learning to perform complex reasoning. o1 thinks before it answers—it can produce a long internal chain of thought before responding to the user.},
	language = {en-US},
	urldate = {2025-01-03},
}

@misc{sprague_cot_2024,
	title = {To {CoT} or not to {CoT}? {Chain}-of-thought helps mainly on math and symbolic reasoning},
	shorttitle = {To {CoT} or not to {CoT}?},
	url = {http://arxiv.org/abs/2409.12183},
	doi = {10.48550/arXiv.2409.12183},
	abstract = {Chain-of-thought (CoT) via prompting is the de facto method for eliciting reasoning capabilities from large language models (LLMs). But for what kinds of tasks is this extra ``thinking'' really helpful? To analyze this, we conducted a quantitative meta-analysis covering over 100 papers using CoT and ran our own evaluations of 20 datasets across 14 models. Our results show that CoT gives strong performance benefits primarily on tasks involving math or logic, with much smaller gains on other types of tasks. On MMLU, directly generating the answer without CoT leads to almost identical accuracy as CoT unless the question or model's response contains an equals sign, indicating symbolic operations and reasoning. Following this finding, we analyze the behavior of CoT on these problems by separating planning and execution and comparing against tool-augmented LLMs. Much of CoT's gain comes from improving symbolic execution, but it underperforms relative to using a symbolic solver. Our results indicate that CoT can be applied selectively, maintaining performance while saving inference costs. Furthermore, they suggest a need to move beyond prompt-based CoT to new paradigms that better leverage intermediate computation across the whole range of LLM applications.},
	urldate = {2024-10-15},
	publisher = {arXiv},
	author = {Sprague, Zayne and Yin, Fangcong and Rodriguez, Juan Diego and Jiang, Dongwei and Wadhwa, Manya and Singhal, Prasann and Zhao, Xinyu and Ye, Xi and Mahowald, Kyle and Durrett, Greg},
	month = sep,
	year = {2024},
	note = {arXiv:2409.12183 
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{sojitra_timeline_2024,
	address = {New York, NY, USA},
	series = {{SIGIR} '24},
	title = {Timeline {Summarization} in the {Era} of {LLMs}},
	isbn = {9798400704314},
	url = {https://dl.acm.org/doi/10.1145/3626772.3657899},
	doi = {10.1145/3626772.3657899},
	abstract = {Timeline summarization is the task of automatically generating concise overviews of documents that capture the key events and their progression on timelines. While this capability is useful for quickly comprehending event sequences without reading lengthy descriptions, timeline summarization remains a relatively underexplored area in recent years when compared to traditional document summarization task and their evolution. The advent of large language models (LLMs) has led some to presume summarization as a solved problem. However, timeline summarization poses unique challenges for LLMs. Our investigation is centered on evaluating the performance of LLMs, against state-of-the-art models in this field. We employed three different approaches: chunking, knowledge graph-based summarization, and TimeRanker. Each of these methods was systematically tested on three benchmark datasets for timeline summarization to assess their effectiveness in capturing and condensing key events and their evolution within timelines. Our findings reveal that while LLMs show promise, timeline summarization remains a complex task that is not yet fully resolved.},
	urldate = {2025-01-03},
	booktitle = {Proceedings of the 47th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Sojitra, Daivik and Jain, Raghav and Saha, Sriparna and Jatowt, Adam and Gupta, Manish},
	month = jul,
	year = {2024},
	pages = {2657--2661},
}

@article{alswaidan_survey_2020,
	title = {A survey of state-of-the-art approaches for emotion recognition in text},
	volume = {62},
	issn = {0219-3116},
	url = {https://doi.org/10.1007/s10115-020-01449-0},
	doi = {10.1007/s10115-020-01449-0},
	abstract = {Emotion recognition in text is an important natural language processing (NLP) task whose solution can benefit several applications in different fields, including data mining, e-learning, information filtering systems, human–computer interaction, and psychology. Explicit emotion recognition in text is the most addressed problem in the literature. The solution to this problem is mainly based on identifying keywords. Implicit emotion recognition is the most challenging problem to solve because such emotion is typically hidden within the text, and thus, its solution requires an understanding of the context. There are four main approaches for implicit emotion recognition in text: rule-based approaches, classical learning-based approaches, deep learning approaches, and hybrid approaches. In this paper, we critically survey the state-of-the-art research for explicit and implicit emotion recognition in text. We present the different approaches found in the literature, detail their main features, discuss their advantages and limitations, and compare them within tables. This study shows that hybrid approaches and learning-based approaches that utilize traditional text representation with distributed word representation outperform the other approaches on benchmark corpora. This paper also identifies the sets of features that lead to the best-performing approaches; highlights the impacts of simple NLP tasks, such as part-of-speech tagging and parsing, on the performances of these approaches; and indicates some open problems.},
	language = {en},
	number = {8},
	urldate = {2025-01-03},
	journal = {Knowledge and Information Systems},
	author = {Alswaidan, Nourah and Menai, Mohamed El Bachir},
	month = aug,
	year = {2020},
	keywords = {Emotion recognition in text, Explicit emotion, Human emotion, Implicit emotion},
	pages = {2937--2987},
}

@inproceedings{aman_identifying_2007,
	address = {Berlin, Heidelberg},
	title = {Identifying {Expressions} of {Emotion} in {Text}},
	isbn = {978-3-540-74628-7},
	doi = {10.1007/978-3-540-74628-7_27},
	abstract = {Finding emotions in text is an area of research with wide-ranging applications. We describe an emotion annotation task of identifying emotion category, emotion intensity and the words/phrases that indicate emotion in text. We introduce the annotation scheme and present results of an annotation agreement study on a corpus of blog posts. The average inter-annotator agreement on labeling a sentence as emotion or non-emotion was 0.76. The agreement on emotion categories was in the range 0.6 to 0.79; for emotion indicators, it was 0.66. Preliminary results of emotion classification experiments show the accuracy of 73.89\%, significantly above the baseline.},
	language = {en},
	booktitle = {Text, {Speech} and {Dialogue}},
	publisher = {Springer},
	author = {Aman, Saima and Szpakowicz, Stan},
	editor = {Matoušek, Václav and Mautner, Pavel},
	year = {2007},
	keywords = {Basic Emotion, Emotion Category, Emotion Intensity, Sentiment Analysis, Support Vector Machine},
	pages = {196--205},
}

@article{mohammad_crowdsourcing_2013,
	title = {Crowdsourcing a {Word}–{Emotion} {Association} {Lexicon}},
	volume = {29},
	copyright = {© 2012 National Research Council Canada},
	issn = {1467-8640},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8640.2012.00460.x},
	doi = {10.1111/j.1467-8640.2012.00460.x},
	abstract = {Even though considerable attention has been given to the polarity of words (positive and negative) and the creation of large polarity lexicons, research in emotion analysis has had to rely on limited and small emotion lexicons. In this paper, we show how the combined strength and wisdom of the crowds can be used to generate a large, high-quality, word–emotion and word–polarity association lexicon quickly and inexpensively. We enumerate the challenges in emotion annotation in a crowdsourcing scenario and propose solutions to address them. Most notably, in addition to questions about emotions associated with terms, we show how the inclusion of a word choice question can discourage malicious data entry, help to identify instances where the annotator may not be familiar with the target term (allowing us to reject such annotations), and help to obtain annotations at sense level (rather than at word level). We conducted experiments on how to formulate the emotion-annotation questions, and show that asking if a term is associated with an emotion leads to markedly higher interannotator agreement than that obtained by asking if a term evokes an emotion.},
	language = {en},
	number = {3},
	urldate = {2025-01-02},
	journal = {Computational Intelligence},
	author = {Mohammad, Saif M. and Turney, Peter D.},
	year = {2013},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-8640.2012.00460.x},
	keywords = {Mechanical Turk, affect, crowdsourcing, emotion lexicon, emotions, polarity, polarity lexicon, semantic orientation, sentiment analysis, word–emotion associations},
	pages = {436--465},
}

@inproceedings{strapparava_semeval-2007_2007,
	address = {Prague, Czech Republic},
	title = {{SemEval}-2007 {Task} 14: {Affective} {Text}},
	shorttitle = {{SemEval}-2007 {Task} 14},
	url = {https://aclanthology.org/S07-1013/},
	urldate = {2025-01-02},
	booktitle = {Proceedings of the {Fourth} {International} {Workshop} on {Semantic} {Evaluations} ({SemEval}-2007)},
	publisher = {Association for Computational Linguistics},
	author = {Strapparava, Carlo and Mihalcea, Rada},
	editor = {Agirre, Eneko and Màrquez, Lluís and Wicentowski, Richard},
	month = jun,
	year = {2007},
	pages = {70--74},
}

@inproceedings{saravia_carer_2018,
	address = {Brussels, Belgium},
	title = {{CARER}: {Contextualized} {Affect} {Representations} for {Emotion} {Recognition}},
	shorttitle = {{CARER}},
	url = {https://aclanthology.org/D18-1404/},
	doi = {10.18653/v1/D18-1404},
	abstract = {Emotions are expressed in nuanced ways, which varies by collective or individual experiences, knowledge, and beliefs. Therefore, to understand emotion, as conveyed through text, a robust mechanism capable of capturing and modeling different linguistic nuances and phenomena is needed. We propose a semi-supervised, graph-based algorithm to produce rich structural descriptors which serve as the building blocks for constructing contextualized affect representations from text. The pattern-based representations are further enriched with word embeddings and evaluated through several emotion recognition tasks. Our experimental results demonstrate that the proposed method outperforms state-of-the-art techniques on emotion recognition tasks.},
	urldate = {2025-01-02},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Saravia, Elvis and Liu, Hsien-Chi Toby and Huang, Yen-Hao and Wu, Junlin and Chen, Yi-Shin},
	editor = {Riloff, Ellen and Chiang, David and Hockenmaier, Julia and Tsujii, Jun'ichi},
	month = oct,
	year = {2018},
	pages = {3687--3697},
}

@inproceedings{socher_recursive_2013,
	address = {Seattle, Washington, USA},
	title = {Recursive {Deep} {Models} for {Semantic} {Compositionality} {Over} a {Sentiment} {Treebank}},
	url = {https://aclanthology.org/D13-1170/},
	urldate = {2025-01-02},
	booktitle = {Proceedings of the 2013 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D. and Ng, Andrew and Potts, Christopher},
	editor = {Yarowsky, David and Baldwin, Timothy and Korhonen, Anna and Livescu, Karen and Bethard, Steven},
	month = oct,
	year = {2013},
	pages = {1631--1642},
}

@inproceedings{abdul-mageed_emonet_2017,
	address = {Vancouver, Canada},
	title = {{EmoNet}: {Fine}-{Grained} {Emotion} {Detection} with {Gated} {Recurrent} {Neural} {Networks}},
	shorttitle = {{EmoNet}},
	url = {https://aclanthology.org/P17-1067/},
	doi = {10.18653/v1/P17-1067},
	abstract = {Accurate detection of emotion from natural language has applications ranging from building emotional chatbots to better understanding individuals and their lives. However, progress on emotion detection has been hampered by the absence of large labeled datasets. In this work, we build a very large dataset for fine-grained emotions and develop deep learning models on it. We achieve a new state-of-the-art on 24 fine-grained types of emotions (with an average accuracy of 87.58\%). We also extend the task beyond emotion types to model Robert Plutick`s 8 primary emotion dimensions, acquiring a superior accuracy of 95.68\%.},
	urldate = {2025-01-02},
	booktitle = {Proceedings of the 55th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Abdul-Mageed, Muhammad and Ungar, Lyle},
	editor = {Barzilay, Regina and Kan, Min-Yen},
	month = jul,
	year = {2017},
	pages = {718--728},
}

@article{tian_recognizing_2001,
	title = {Recognizing action units for facial expression analysis},
	volume = {23},
	issn = {1939-3539},
	url = {https://ieeexplore.ieee.org/abstract/document/908962?casa_token=GjOlNHVbKBIAAAAA:Q_Au2F3eFHqrnJo_t3D39dR3WzsQryUhc3eXuP4DHJvH5ZGO1BN86lSe0rD-g9HD8NUGMAfC},
	doi = {10.1109/34.908962},
	abstract = {Most automatic expression analysis systems attempt to recognize a small set of prototypic expressions, such as happiness, anger, surprise, and fear. Such prototypic expressions, however, occur rather infrequently. Human emotions and intentions are more often communicated by changes in one or a few discrete facial features. In this paper, we develop an automatic face analysis (AFA) system to analyze facial expressions based on both permanent facial features (brows, eyes, mouth) and transient facial features (deepening of facial furrows) in a nearly frontal-view face image sequence. The AFA system recognizes fine-grained changes in facial expression into action units (AU) of the Facial Action Coding System (FACS), instead of a few prototypic expressions. Multistate face and facial component models are proposed for tracking and modeling the various facial features, including lips, eyes, brows, cheeks, and furrows. During tracking, detailed parametric descriptions of the facial features are extracted. With these parameters as the inputs, a group of action units (neutral expression, six upper face AU and 10 lower face AU) are recognized whether they occur alone or in combinations. The system has achieved average recognition rates of 96.4 percent (95.4 percent if neutral expressions are excluded) for upper face AU and 96.7 percent (95.6 percent with neutral expressions excluded) for lower face AU. The generalizability of the system has been tested by using independent image databases collected and FACS-coded for ground-truth by different research teams.},
	number = {2},
	urldate = {2025-01-02},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Tian, Y.-I. and Kanade, T. and Cohn, J.F.},
	month = feb,
	year = {2001},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Eyes, Face recognition, Facial features, Gold, Humans, Image analysis, Image sequence analysis, Mouth, Prototypes, Transient analysis},
	pages = {97--115},
}

@article{pantic_automatic_2000,
	title = {Automatic analysis of facial expressions: the state of the art},
	volume = {22},
	issn = {1939-3539},
	shorttitle = {Automatic analysis of facial expressions},
	url = {https://ieeexplore.ieee.org/document/895976/?arnumber=895976},
	doi = {10.1109/34.895976},
	abstract = {Humans detect and interpret faces and facial expressions in a scene with little or no effort. Still, development of an automated system that accomplishes this task is rather difficult. There are several related problems: detection of an image segment as a face, extraction of the facial expression information, and classification of the expression (e.g., in emotion categories). A system that performs these operations accurately and in real time would form a big step in achieving a human-like interaction between man and machine. The paper surveys the past work in solving these problems. The capability of the human visual system with respect to these problems is discussed, too. It is meant to serve as an ultimate goal and a guide for determining recommendations for development of an automatic facial expression analyzer.},
	number = {12},
	urldate = {2025-01-02},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Pantic, M. and Rothkrantz, L.J.M.},
	month = dec,
	year = {2000},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Communication channels, Data mining, Emotion recognition, Face detection, Human computer interaction, Image sequence analysis, Man machine systems, Pattern recognition, Robustness, Visual system},
	pages = {1424--1445},
}

@inproceedings{alm_emotions_2005,
	address = {Vancouver, British Columbia, Canada},
	title = {Emotions from {Text}: {Machine} {Learning} for {Text}-based {Emotion} {Prediction}},
	shorttitle = {Emotions from {Text}},
	url = {https://aclanthology.org/H05-1073/},
	urldate = {2025-01-02},
	booktitle = {Proceedings of {Human} {Language} {Technology} {Conference} and {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Alm, Cecilia Ovesdotter and Roth, Dan and Sproat, Richard},
	editor = {Mooney, Raymond and Brew, Chris and Chien, Lee-Feng and Kirchhoff, Katrin},
	month = oct,
	year = {2005},
	pages = {579--586},
}

@book{liu_sentiment_2022,
	title = {Sentiment {Analysis} and {Opinion} {Mining}},
	isbn = {978-3-031-02145-9},
	abstract = {Sentiment analysis and opinion mining is the field of study that analyzes people's opinions, sentiments, evaluations, attitudes, and emotions from written language. It is one of the most active research areas in natural language processing and is also widely studied in data mining, Web mining, and text mining. In fact, this research has spread outside of computer science to the management sciences and social sciences due to its importance to business and society as a whole. The growing importance of sentiment analysis coincides with the growth of social media such as reviews, forum discussions, blogs, micro-blogs, Twitter, and social networks. For the first time in human history, we now have a huge volume of opinionated data recorded in digital form for analysis. Sentiment analysis systems are being applied in almost every business and social domain because opinions are central to almost all human activities and are key influencers of our behaviors. Our beliefs and perceptions of reality, and the choices we make, are largely conditioned on how others see and evaluate the world. For this reason, when we need to make a decision we often seek out the opinions of others. This is true not only for individuals but also for organizations. This book is a comprehensive introductory and survey text. It covers all important topics and the latest developments in the field with over 400 references. It is suitable for students, researchers and practitioners who are interested in social media analysis in general and sentiment analysis in particular. Lecturers can readily use it in class for courses on natural language processing, social media analysis, text mining, and data mining. Lecture slides are also available online. Table of Contents: Preface / Sentiment Analysis: A Fascinating Problem / The Problem of Sentiment Analysis / Document Sentiment Classification / Sentence Subjectivity and Sentiment Classification / Aspect-Based Sentiment Analysis / Sentiment Lexicon Generation / Opinion Summarization / Analysis of Comparative Opinions / Opinion Search and Retrieval / Opinion Spam Detection / Quality of Reviews / Concluding Remarks / Bibliography / Author Biography},
	language = {en},
	publisher = {Springer Nature},
	author = {Liu, Bing},
	month = may,
	year = {2022},
	note = {Google-Books-ID: xYhyEAAAQBAJ},
	keywords = {Computers / Artificial Intelligence / General, Computers / Artificial Intelligence / Natural Language Processing, Computers / Information Technology, Computers / Speech \& Audio Processing, Language Arts \& Disciplines / Linguistics / General},
}

@article{poria_review_2017,
	title = {A review of affective computing: {From} unimodal analysis to multimodal fusion},
	volume = {37},
	issn = {1566-2535},
	shorttitle = {A review of affective computing},
	url = {https://www.sciencedirect.com/science/article/pii/S1566253517300738},
	doi = {10.1016/j.inffus.2017.02.003},
	abstract = {Affective computing is an emerging interdisciplinary research field bringing together researchers and practitioners from various fields, ranging from artificial intelligence, natural language processing, to cognitive and social sciences. With the proliferation of videos posted online (e.g., on YouTube, Facebook, Twitter) for product reviews, movie reviews, political views, and more, affective computing research has increasingly evolved from conventional unimodal analysis to more complex forms of multimodal analysis. This is the primary motivation behind our first of its kind, comprehensive literature review of the diverse field of affective computing. Furthermore, existing literature surveys lack a detailed discussion of state of the art in multimodal affect analysis frameworks, which this review aims to address. Multimodality is defined by the presence of more than one modality or channel, e.g., visual, audio, text, gestures, and eye gage. In this paper, we focus mainly on the use of audio, visual and text information for multimodal affect analysis, since around 90\% of the relevant literature appears to cover these three modalities. Following an overview of different techniques for unimodal affect analysis, we outline existing methods for fusing information from different modalities. As part of this review, we carry out an extensive study of different categories of state-of-the-art fusion techniques, followed by a critical analysis of potential performance improvements with multimodal analysis compared to unimodal analysis. A comprehensive overview of these two complementary fields aims to form the building blocks for readers, to better understand this challenging and exciting research field.},
	urldate = {2025-01-02},
	journal = {Information Fusion},
	author = {Poria, Soujanya and Cambria, Erik and Bajpai, Rajiv and Hussain, Amir},
	month = sep,
	year = {2017},
	keywords = {Affective computing, Audio, visual and text information fusion, Multimodal affect analysis, Multimodal fusion, Sentiment analysis},
	pages = {98--125},
}

@book{delac_recent_2008,
	title = {Recent {Advances} in {Face} {Recognition}},
	isbn = {978-953-7619-34-3},
	abstract = {The main idea and the driver of further research in the area of face recognition are security applications and human-computer interaction. Face recognition represents an intuitive and non-intrusive method of recognizing people and this is why it became one of three identification methods used in e-passports and a biometric of choice for many other security applications. This goal of this book is to provide the reader with the most up to date research performed in automatic face recognition. The chapters presented use innovative approaches to deal with a wide variety of unsolved issues.},
	language = {en},
	publisher = {BoD – Books on Demand},
	author = {Delac, Kresimir and Grgic, Mislav and Grgic, Sonja},
	month = dec,
	year = {2008},
	note = {Google-Books-ID: 6DehDwAAQBAJ},
	keywords = {Computers / General, Computers / Security / General},
}

@article{sariyanidi_automatic_2015,
	title = {Automatic {Analysis} of {Facial} {Affect}: {A} {Survey} of {Registration}, {Representation}, and {Recognition}},
	volume = {37},
	issn = {1939-3539},
	shorttitle = {Automatic {Analysis} of {Facial} {Affect}},
	url = {https://ieeexplore.ieee.org/document/6940284/?arnumber=6940284},
	doi = {10.1109/TPAMI.2014.2366127},
	abstract = {Automatic affect analysis has attracted great interest in various contexts including the recognition of action units and basic or non-basic emotions. In spite of major efforts, there are several open questions on what the important cues to interpret facial expressions are and how to encode them. In this paper, we review the progress across a range of affect recognition applications to shed light on these fundamental questions. We analyse the state-of-the-art solutions by decomposing their pipelines into fundamental components, namely face registration, representation, dimensionality reduction and recognition. We discuss the role of these components and highlight the models and new trends that are followed in their design. Moreover, we provide a comprehensive analysis of facial representations by uncovering their advantages and limitations; we elaborate on the type of information they encode and discuss how they deal with the key challenges of illumination variations, registration errors, head-pose variations, occlusions, and identity bias. This survey allows us to identify open issues and to define future directions for designing real-world affect recognition systems.},
	number = {6},
	urldate = {2025-01-02},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Sariyanidi, Evangelos and Gunes, Hatice and Cavallaro, Andrea},
	month = jun,
	year = {2015},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Affect Sensing and Analysis, Affect sensing and analysis, Emotion recognition, Face, Face recognition, Facial Expressions, Facial Representations, Histograms, Lighting, Registration, Shape, Survey, Training, facial expressions, facial representations, registration, survey},
	pages = {1113--1133},
}

@inproceedings{zeng_survey_2007,
	address = {New York, NY, USA},
	series = {{ICMI} '07},
	title = {A survey of affect recognition methods: audio, visual and spontaneous expressions},
	isbn = {978-1-59593-817-6},
	shorttitle = {A survey of affect recognition methods},
	url = {https://dl.acm.org/doi/10.1145/1322192.1322216},
	doi = {10.1145/1322192.1322216},
	abstract = {Automated analysis of human affective behavior has attracted increasing attention from researchers in psychology, computer science, linguistics, neuroscience, and related disciplines. Promising approaches have been reported, including automatic methods for facial and vocal affect recognition. However, the existing methods typically handle only deliberately displayed and exaggerated expressions of prototypical emotions-despite the fact that deliberate behavior differs in visual and audio expressions from spontaneously occurring behavior. Recently efforts to develop algorithms that can process naturally occurring human affective behavior have emerged. This paper surveys these efforts. We first discuss human emotion perception from a psychological perspective. Next, we examine the available approaches to solving the problem of machine understanding of human affective behavior occurring in real-world settings. We finally outline some scientific and engineering challenges for advancing human affect sensing technology.},
	urldate = {2025-01-02},
	booktitle = {Proceedings of the 9th international conference on {Multimodal} interfaces},
	publisher = {Association for Computing Machinery},
	author = {Zeng, Zhihong and Pantic, Maja and Roisman, Glenn I. and Huang, Thomas S.},
	month = nov,
	year = {2007},
	pages = {126--133},
}

@misc{noauthor_tokenizer_nodate,
	title = {The {Tokenizer} {Playground} - a {Hugging} {Face} {Space} by {Xenova}},
	url = {https://huggingface.co/spaces/Xenova/the-tokenizer-playground},
	abstract = {Experiment with and compare different tokenizers},
	urldate = {2025-01-02},
}

@misc{brown_large_2024,
	title = {Large {Language} {Monkeys}: {Scaling} {Inference} {Compute} with {Repeated} {Sampling}},
	shorttitle = {Large {Language} {Monkeys}},
	url = {http://arxiv.org/abs/2407.21787},
	doi = {10.48550/arXiv.2407.21787},
	abstract = {Scaling the amount of compute used to train language models has dramatically improved their capabilities. However, when it comes to inference, we often limit the amount of compute to only one attempt per problem. Here, we explore inference compute as another axis for scaling by increasing the number of generated samples. Across multiple tasks and models, we observe that coverage - the fraction of problems solved by any attempt - scales with the number of samples over four orders of magnitude. In domains like coding and formal proofs, where all answers can be automatically verified, these increases in coverage directly translate into improved performance. When we apply repeated sampling to SWE-bench Lite, the fraction of issues solved with DeepSeek-V2-Coder-Instruct increases from 15.9\% with one sample to 56\% with 250 samples, outperforming the single-attempt state-of-the-art of 43\% which uses more capable frontier models. Moreover, using current API pricing, amplifying the cheaper DeepSeek model with five samples is more cost-effective and solves more issues than paying a premium for one sample from GPT-4o or Claude 3.5 Sonnet. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. Finally, we find that identifying correct samples out of many generations remains an important direction for future research in domains without automatic verifiers. When solving math word problems from GSM8K and MATH, coverage with Llama-3 models grows to over 95\% with 10,000 samples. However, common methods to pick correct solutions from a sample collection, such as majority voting or reward models, plateau beyond several hundred samples and fail to fully scale with the sample budget.},
	urldate = {2024-12-19},
	publisher = {arXiv},
	author = {Brown, Bradley and Juravsky, Jordan and Ehrlich, Ryan and Clark, Ronald and Le, Quoc V. and Ré, Christopher and Mirhoseini, Azalia},
	month = sep,
	year = {2024},
	note = {arXiv:2407.21787 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{barrett_conceptual_2014,
	title = {The {Conceptual} {Act} {Theory}: {A} {Précis}},
	volume = {6},
	issn = {1754-0739},
	shorttitle = {The {Conceptual} {Act} {Theory}},
	url = {https://doi.org/10.1177/1754073914534479},
	doi = {10.1177/1754073914534479},
	abstract = {According to the conceptual act theory, emotions emerge when physical sensations in the self and physical actions in others are meaningfully linked to situations during a process that can be called both cognitive and perceptual (creating emotional experiences, and emotion perceptions, respectively). There are key four hypotheses: (a) an emotion (like anger) is a conceptual category, populated with instances that are tailored to the environment; (b) each instance of emotion is constructed within the brain’s functional architecture of domain-general core systems; (c) the workings of each system must be holistically understood within the momentary state of the brain, the body, and the surrounding context; (d) being emergent states, emotional episodes have functional features that physical states, alone, do not have. Similarities and differences to other theoretical approaches to emotion are discussed.},
	language = {en},
	number = {4},
	urldate = {2024-12-18},
	journal = {Emotion Review},
	author = {Barrett, Lisa Feldman},
	month = oct,
	year = {2014},
	note = {Publisher: SAGE Publications},
	pages = {292--297},
}

@article{barrett_emotions_2012,
	title = {Emotions are real},
	volume = {12},
	issn = {1931-1516},
	doi = {10.1037/a0027555},
	abstract = {It is obvious that emotions are real, but the question is what kind of “real” are they? In this article, I outline a theoretical approach where emotions are a part of social reality. I propose that physical changes (in the face, voice, and body, or neural circuits for behavioral adaptations like freezing, fleeing, or fighting) transform into an emotion when those changes take on psychological functions that they cannot perform by their physical nature alone. This requires socially shared conceptual knowledge that perceivers use to create meaning from these physical changes (as well as the circuitry that supports this meaning making). My claim is that emotions are, at the same time, socially constructed and biologically evident. Only when we understand all the elements that construct emotional episodes, in social, psychological, and biological terms, will we understand the nature of emotion. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {3},
	journal = {Emotion},
	author = {Barrett, Lisa Feldman},
	year = {2012},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Concepts, Emotional Regulation, Emotional States, Emotions, Social Processes},
	pages = {413--429},
}

@article{jack_facial_2012,
	title = {Facial expressions of emotion are not culturally universal},
	volume = {109},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.1200155109},
	doi = {10.1073/pnas.1200155109},
	abstract = {Since Darwin’s seminal works, the universality of facial expressions of emotion has remained one of the longest standing debates in the biological and social sciences. Briefly stated, the universality hypothesis claims that all humans communicate six basic internal emotional states (happy, surprise, fear, disgust, anger, and sad) using the same facial movements by virtue of their biological and evolutionary origins [Susskind JM, et al. (2008) Nat Neurosci 11:843–850]. Here, we refute this assumed universality. Using a unique computer graphics platform that combines generative grammars [Chomsky N (1965) MIT Press, Cambridge, MA] with visual perception, we accessed the mind’s eye of 30 Western and Eastern culture individuals and reconstructed their mental representations of the six basic facial expressions of emotion. Cross-cultural comparisons of the mental representations challenge universality on two separate counts. First, whereas Westerners represent each of the six basic emotions with a distinct set of facial movements common to the group, Easterners do not. Second, Easterners represent emotional intensity with distinctive dynamic eye activity. By refuting the long-standing universality hypothesis, our data highlight the powerful influence of culture on shaping basic behaviors once considered biologically hardwired. Consequently, our data open a unique nature–nurture debate across broad fields from evolutionary psychology and social neuroscience to social networking via digital avatars.},
	number = {19},
	urldate = {2024-12-18},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Jack, Rachael E. and Garrod, Oliver G. B. and Yu, Hui and Caldara, Roberto and Schyns, Philippe G.},
	month = may,
	year = {2012},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {7241--7244},
}

@article{gendron_perceptions_2014,
	title = {Perceptions of emotion from facial expressions are not culturally universal: {Evidence} from a remote culture},
	volume = {14},
	issn = {1931-1516},
	shorttitle = {Perceptions of emotion from facial expressions are not culturally universal},
	doi = {10.1037/a0036052},
	abstract = {It is widely believed that certain emotions are universally recognized in facial expressions. Recent evidence indicates that Western perceptions (e.g., scowls as anger) depend on cues to U.S. emotion concepts embedded in experiments. Because such cues are standard features in methods used in cross-cultural experiments, we hypothesized that evidence of universality depends on this conceptual context. In our study, participants from the United States and the Himba ethnic group from the Keunene region of northwestern Namibia sorted images of posed facial expressions into piles by emotion type. Without cues to emotion concepts, Himba participants did not show the presumed “universal” pattern, whereas U.S. participants produced a pattern with presumed universal features. With cues to emotion concepts, participants in both cultures produced sorts that were closer to the presumed “universal” pattern, although substantial cultural variation persisted. Our findings indicate that perceptions of emotion are not universal, but depend on cultural and conceptual contexts. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {2},
	journal = {Emotion},
	author = {Gendron, Maria and Roberson, Debi and van der Vyver, Jacoba Marietta and Barrett, Lisa Feldman},
	year = {2014},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Cross Cultural Differences, Emotion Recognition, Emotions, Face Perception, Facial Expressions},
	pages = {251--262},
}

@article{calvo_affect_2010,
	title = {Affect {Detection}: {An} {Interdisciplinary} {Review} of {Models}, {Methods}, and {Their} {Applications}},
	volume = {1},
	issn = {1949-3045},
	shorttitle = {Affect {Detection}},
	url = {https://ieeexplore.ieee.org/abstract/document/5520655},
	doi = {10.1109/T-AFFC.2010.1},
	abstract = {This survey describes recent progress in the field of Affective Computing (AC), with a focus on affect detection. Although many AC researchers have traditionally attempted to remain agnostic to the different emotion theories proposed by psychologists, the affective technologies being developed are rife with theoretical assumptions that impact their effectiveness. Hence, an informed and integrated examination of emotion theories from multiple areas will need to become part of computing practice if truly effective real-world systems are to be achieved. This survey discusses theoretical perspectives that view emotions as expressions, embodiments, outcomes of cognitive appraisal, social constructs, products of neural circuitry, and psychological interpretations of basic feelings. It provides meta-analyses on existing reviews of affect detection systems that focus on traditional affect detection modalities like physiology, face, and voice, and also reviews emerging research on more novel channels such as text, body language, and complex multimodal systems. This survey explicitly explores the multidisciplinary foundation that underlies all AC applications by describing how AC researchers have incorporated psychological theories of emotion and how these theories affect research questions, methods, results, and their interpretations. In this way, models and methods can be compared, and emerging insights from various disciplines can be more expertly integrated.},
	number = {1},
	urldate = {2024-11-06},
	journal = {IEEE Transactions on Affective Computing},
	author = {Calvo, Rafael A. and D'Mello, Sidney},
	month = jan,
	year = {2010},
	note = {Conference Name: IEEE Transactions on Affective Computing},
	keywords = {Affective computing, Chemicals, Degradation, Electrons, High K dielectric materials, High-K gate dielectrics, Hot carriers, MOSFET circuits, Stress, Temperature distribution, Voltage, affect sensing and analysis, emotion detection, emotion theory., multimodal recognition},
	pages = {18--37},
}

@article{aviezer_body_2012,
	title = {Body {Cues}, {Not} {Facial} {Expressions}, {Discriminate} {Between} {Intense} {Positive} and {Negative} {Emotions}},
	volume = {338},
	url = {https://www.science.org/doi/full/10.1126/science.1224313},
	doi = {10.1126/science.1224313},
	abstract = {The distinction between positive and negative emotions is fundamental in emotion models. Intriguingly, neurobiological work suggests shared mechanisms across positive and negative emotions. We tested whether similar overlap occurs in real-life facial expressions. During peak intensities of emotion, positive and negative situations were successfully discriminated from isolated bodies but not faces. Nevertheless, viewers perceived illusory positivity or negativity in the nondiagnostic faces when seen with bodies. To reveal the underlying mechanisms, we created compounds of intense negative faces combined with positive bodies, and vice versa. Perceived affect and mimicry of the faces shifted systematically as a function of their contextual body emotion. These findings challenge standard models of emotion expression and highlight the role of the body in expressing and perceiving emotions.},
	number = {6111},
	urldate = {2024-12-18},
	journal = {Science},
	author = {Aviezer, Hillel and Trope, Yaacov and Todorov, Alexander},
	month = nov,
	year = {2012},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {1225--1229},
}

@article{barrett_are_2006,
	title = {Are {Emotions} {Natural} {Kinds}?},
	volume = {1},
	issn = {1745-6916},
	url = {https://doi.org/10.1111/j.1745-6916.2006.00003.x},
	doi = {10.1111/j.1745-6916.2006.00003.x},
	abstract = {Laypeople and scientists alike believe that they know anger, or sadness, or fear, when they see it. These emotions and a few others are presumed to have specific causal mechanisms in the brain and properties that are observable (on the face, in the voice, in the body, or in experience)—that is, they are assumed to be natural kinds. If a given emotion is a natural kind and can be identified objectively, then it is possible to make discoveries about that emotion. Indeed, the scientific study of emotion is founded on this assumption. In this article, I review the accumulating empirical evidence that is inconsistent with the view that there are kinds of emotion with boundaries that are carved in nature. I then consider what moving beyond a natural-kind view might mean for the scientific understanding of emotion.},
	language = {en},
	number = {1},
	urldate = {2024-12-18},
	journal = {Perspectives on Psychological Science},
	author = {Barrett, Lisa Feldman},
	month = mar,
	year = {2006},
	note = {Publisher: SAGE Publications Inc},
	pages = {28--58},
}

@article{hassin_inherently_2013,
	title = {Inherently {Ambiguous}: {Facial} {Expressions} of {Emotions}, in {Context}},
	volume = {5},
	issn = {1754-0739},
	shorttitle = {Inherently {Ambiguous}},
	url = {https://doi.org/10.1177/1754073912451331},
	doi = {10.1177/1754073912451331},
	abstract = {With a few yet increasing number of exceptions, the cognitive sciences enthusiastically endorsed the idea that there are basic facial expressions of emotions that are created by specific configurations of facial muscles. We review evidence that suggests an inherent role for context in emotion perception. Context does not merely change emotion perception at the edges; it leads to radical categorical changes. The reviewed findings suggest that configurations of facial muscles are inherently ambiguous, and they call for a different approach towards the understanding of facial expressions of emotions. Prices of sticking with the modal view, and advantages of an expanded view, are succinctly reviewed.},
	language = {en},
	number = {1},
	urldate = {2024-12-18},
	journal = {Emotion Review},
	author = {Hassin, Ran R. and Aviezer, Hillel and Bentin, Shlomo},
	month = jan,
	year = {2013},
	note = {Publisher: SAGE Publications},
	pages = {60--65},
}

@article{russell_circumplex_1980,
	title = {A circumplex model of affect},
	volume = {39},
	issn = {1939-1315},
	doi = {10.1037/h0077714},
	abstract = {Factor-analytic evidence has led most psychologists to describe affect as a set of dimensions, such as displeasure, distress, depression, excitement, and so on, with each dimension varying independently of the others. However, there is other evidence that rather than being independent, these affective dimensions are interrelated in a highly systematic fashion. The evidence suggests that these interrelationships can be represented by a spatial model in which affective concepts fall in a circle in the following order: pleasure (0), excitement (45), arousal (90), distress (135), displeasure (180), depression (225), sleepiness (270), and relaxation (315). This model was offered both as a way psychologists can represent the structure of affective experience, as assessed through self-report, and as a representation of the cognitive structure that laymen utilize in conceptualizing affect. Supportive evidence was obtained by scaling 28 emotion-denoting adjectives in 4 different ways: R. T. Ross's (1938) technique for a circular ordering of variables, a multidimensional scaling procedure based on perceived similarity among the terms, a unidimensional scaling on hypothesized pleasure–displeasure and degree-of-arousal dimensions, and a principal-components analysis of 343 Ss' self-reports of their current affective states. (70 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {6},
	journal = {Journal of Personality and Social Psychology},
	author = {Russell, James A.},
	year = {1980},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Emotional States, Emotions, Models},
	pages = {1161--1178},
}

@misc{noauthor_are_nodate,
	title = {Are {Emotions} {Natural} {Kinds}? - {Lisa} {Feldman} {Barrett}, 2006},
	url = {https://journals.sagepub.com/doi/full/10.1111/j.1745-6916.2006.00003.x},
	urldate = {2024-12-18},
}

@article{elfenbein_universality_2002,
	title = {On the universality and cultural specificity of emotion recognition: {A} meta-analysis},
	volume = {128},
	issn = {1939-1455},
	shorttitle = {On the universality and cultural specificity of emotion recognition},
	doi = {10.1037/0033-2909.128.2.203},
	abstract = {A meta-analysis examined emotion recognition within and across cultures. Emotions were universally recognized at better-than-chance levels. Accuracy was higher when emotions were both expressed and recognized by members of the same national, ethnic, or regional group, suggesting an in-group advantage. This advantage was smaller for cultural groups with greater exposure to one another, measured in terms of living in the same nation, physical proximity, and telephone communication. Majority group members were poorer at judging minority group members than the reverse. Cross-cultural accuracy was lower in studies that used a balanced research design, and higher in studies that used imitation rather than posed or spontaneous emotional expressions. Attributes of study design appeared not to moderate the size of the in-group advantage. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {2},
	journal = {Psychological Bulletin},
	author = {Elfenbein, Hillary Anger and Ambady, Nalini},
	year = {2002},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Cross Cultural Differences, Emotion Recognition, Emotional States, Nonverbal Communication},
	pages = {203--235},
}

@article{matsumoto_cultural_1990,
	title = {Cultural similarities and differences in display rules},
	volume = {14},
	issn = {1573-6644},
	url = {https://doi.org/10.1007/BF00995569},
	doi = {10.1007/BF00995569},
	abstract = {Two decades of cross-cultural research on the emotions have produced a wealth of information concerning cultural similarities and differences in the communication of emotion. Still, gaps in our knowledge remain. This article presents a theoretical framework that predicts cultural differences in display rules according to cultural differences in individualism-collectivism (I-C) and power distance (PD; Hofstede, 1980, 1983), and the social distinctions ingroups-outgroups and status. The model was tested using an American-Japanese comparison, where subjects in both cultures rated the appropriateness of the six universal facial expressions of emotion in eight different social situations. The findings were generally supportive of the theoretical model, and argue for the joint consideration of display rules and actual emotional behaviors in cross-cultural research.},
	language = {en},
	number = {3},
	urldate = {2024-12-18},
	journal = {Motivation and Emotion},
	author = {Matsumoto, David},
	month = sep,
	year = {1990},
	keywords = {Cultural Difference, Facial Expression, Social Psychology, Social Situation, Theoretical Model},
	pages = {195--214},
}

@article{ekman_repertoire_1969,
	title = {The {Repertoire} of {Nonverbal} {Behavior}: {Categories}, {Origins}, {Usage}, and {Coding}},
	volume = {1},
	copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
	issn = {1613-3692},
	shorttitle = {The {Repertoire} of {Nonverbal} {Behavior}},
	url = {https://www.degruyter.com/document/doi/10.1515/semi.1969.1.1.49/html},
	doi = {10.1515/semi.1969.1.1.49},
	abstract = {Article The Repertoire of Nonverbal Behavior: Categories, Origins, Usage, and Coding was published on January 1, 1969 in the journal Semiotica (volume 1, issue 1).},
	language = {en},
	number = {1},
	urldate = {2024-12-18},
	journal = {Semiotica},
	author = {Ekman, Paul and Friesen, Wallace V.},
	month = jan,
	year = {1969},
	note = {Publisher: De Gruyter Mouton},
	pages = {49--98},
}

@misc{noauthor_affect_nodate,
	title = {Affect {Detection}: {An} {Interdisciplinary} {Review} of {Models}, {Methods}, and {Their} {Applications} {\textbar} {IEEE} {Journals} \& {Magazine} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/abstract/document/5520655},
	urldate = {2024-12-18},
}

@article{picard_affective_nodate,
	title = {{AFFECTIVE} {COMPUTING} {FOR} {HCI}},
	language = {en},
	author = {Picard, Rosalind W},
}

@article{hofmaier_exploration_nodate,
	title = {Exploration of {Content}-{Based} {Cross}-{Domain} {Podcast} {Recommender} {Systems}},
	language = {de},
	journal = {Recommender Systems},
	author = {Hofmaier, Matthias},
}

@article{steindl_evaluating_nodate,
	title = {Evaluating the {Fairness} of {News} {Recommender} {Algorithms} {Within} {Detected} {User} {Communities}},
	language = {en},
	journal = {Data Science},
	author = {Steindl, Bernhard},
}

@article{huebner_exploring_nodate,
	title = {Exploring {Group} {Fairness} in {News} {Media} {Recommendations}: {Algorithms}, {Metrics}, and {Grouping}},
	language = {en},
	journal = {Data Science},
	author = {Huebner, Blake},
}

@article{howcroft_twenty_2020,
	title = {Twenty {Years} of {Confusion} in {Human} {Evaluation}: 13th {International} {Conference} on {Natural} {Language} {Generation} 2020},
	shorttitle = {Twenty {Years} of {Confusion} in {Human} {Evaluation}},
	url = {http://www.scopus.com/inward/record.url?scp=85101841968&partnerID=8YFLogxK},
	abstract = {Human assessment remains the most trusted form of evaluation in NLG, but highly diverse approaches and a proliferation of different quality criteria used by researchers make it difficult to compare results and draw conclusions across papers, with adverse implications for meta-evaluation and reproducibility. In this paper, we present (i) our dataset of 165 NLG papers with human evaluations, (ii) the annotation scheme we developed to label the papers for different aspects of evaluations, (iii) quantitative analyses of the annotations, and (iv) a set of recommendations for improving standards in evaluation reporting. We use the annotations as a basis for examining information included in evaluation reports, and levels of consistency in approaches, experimental design and terminology, focusing in particular on the 200+ different terms that have been used for evaluated aspects of quality. We conclude that due to a pervasive lack of clarity in reports and extreme diversity in approaches, human evaluation in NLG presents as extremely confused in 2020, and that the field is in urgent need of standard methods and terminology.},
	urldate = {2024-12-08},
	journal = {Proceedings of the 13th International Conference on Natural Language Generation},
	author = {Howcroft, David M. and Belz, Anya and Clinciu, Miruna and Gkatzia, Dimitra and Hasan, Sadid A. and Mahamood, Saad and Mille, Simon and van Miltenburg, Emiel and Santhanam, Sashank and Rieser, Verena},
	editor = {Davis, Brian and Graham, Yvette and Kelleher, John and Sripada, Yaji},
	month = dec,
	year = {2020},
	note = {Publisher: Association for Computational Linguistics},
	pages = {169--182},
}

@misc{khashabi_genie_2022,
	title = {{GENIE}: {Toward} {Reproducible} and {Standardized} {Human} {Evaluation} for {Text} {Generation}},
	shorttitle = {{GENIE}},
	url = {http://arxiv.org/abs/2101.06561},
	doi = {10.48550/arXiv.2101.06561},
	abstract = {While often assumed a gold standard, effective human evaluation of text generation remains an important, open area for research. We revisit this problem with a focus on producing consistent evaluations that are reproducible -- over time and across different populations. We study this goal in different stages of the human evaluation pipeline. In particular, we consider design choices for the annotation interface used to elicit human judgments and their impact on reproducibility. Furthermore, we develop an automated mechanism for maintaining annotator quality via a probabilistic model that detects and excludes noisy annotators. Putting these lessons together, we introduce GENIE: a system for running standardized human evaluations across different generation tasks. We instantiate GENIE with datasets representing four core challenges in text generation: machine translation, summarization, commonsense reasoning, and machine comprehension. For each task, GENIE offers a leaderboard that automatically crowdsources annotations for submissions, evaluating them along axes such as correctness, conciseness, and fluency. We have made the GENIE leaderboards publicly available, and have already ranked 50 submissions from 10 different research groups. We hope GENIE encourages further progress toward effective, standardized evaluations for text generation.},
	urldate = {2024-12-08},
	publisher = {arXiv},
	author = {Khashabi, Daniel and Stanovsky, Gabriel and Bragg, Jonathan and Lourie, Nicholas and Kasai, Jungo and Choi, Yejin and Smith, Noah A. and Weld, Daniel S.},
	month = nov,
	year = {2022},
	note = {arXiv:2101.06561 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{celikyilmaz_evaluation_2021,
	title = {Evaluation of {Text} {Generation}: {A} {Survey}},
	shorttitle = {Evaluation of {Text} {Generation}},
	url = {http://arxiv.org/abs/2006.14799},
	doi = {10.48550/arXiv.2006.14799},
	abstract = {The paper surveys evaluation methods of natural language generation (NLG) systems that have been developed in the last few years. We group NLG evaluation methods into three categories: (1) human-centric evaluation metrics, (2) automatic metrics that require no training, and (3) machine-learned metrics. For each category, we discuss the progress that has been made and the challenges still being faced, with a focus on the evaluation of recently proposed NLG tasks and neural NLG models. We then present two examples for task-specific NLG evaluations for automatic text summarization and long text generation, and conclude the paper by proposing future research directions.},
	urldate = {2024-12-08},
	publisher = {arXiv},
	author = {Celikyilmaz, Asli and Clark, Elizabeth and Gao, Jianfeng},
	month = may,
	year = {2021},
	note = {arXiv:2006.14799 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{gao_llm-based_2024,
	title = {{LLM}-based {NLG} {Evaluation}: {Current} {Status} and {Challenges}},
	shorttitle = {{LLM}-based {NLG} {Evaluation}},
	url = {http://arxiv.org/abs/2402.01383},
	doi = {10.48550/arXiv.2402.01383},
	abstract = {Evaluating natural language generation (NLG) is a vital but challenging problem in artificial intelligence. Traditional evaluation metrics mainly capturing content (e.g. n-gram) overlap between system outputs and references are far from satisfactory, and large language models (LLMs) such as ChatGPT have demonstrated great potential in NLG evaluation in recent years. Various automatic evaluation methods based on LLMs have been proposed, including metrics derived from LLMs, prompting LLMs, and fine-tuning LLMs with labeled evaluation data. In this survey, we first give a taxonomy of LLM-based NLG evaluation methods, and discuss their pros and cons, respectively. We also discuss human-LLM collaboration for NLG evaluation. Lastly, we discuss several open problems in this area and point out future research directions.},
	urldate = {2024-12-08},
	publisher = {arXiv},
	author = {Gao, Mingqi and Hu, Xinyu and Ruan, Jie and Pu, Xiao and Wan, Xiaojun},
	month = feb,
	year = {2024},
	note = {arXiv:2402.01383 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{zheng_judging_2023,
	title = {Judging {LLM}-as-a-{Judge} with {MT}-{Bench} and {Chatbot} {Arena}},
	url = {http://arxiv.org/abs/2306.05685},
	doi = {10.48550/arXiv.2306.05685},
	abstract = {Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm\_judge.},
	urldate = {2024-12-06},
	publisher = {arXiv},
	author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P. and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion},
	month = dec,
	year = {2023},
	note = {arXiv:2306.05685 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{berberette_redefining_2024,
	title = {Redefining "{Hallucination}" in {LLMs}: {Towards} a psychology-informed framework for mitigating misinformation},
	shorttitle = {Redefining "{Hallucination}" in {LLMs}},
	url = {http://arxiv.org/abs/2402.01769},
	doi = {10.48550/arXiv.2402.01769},
	abstract = {In recent years, large language models (LLMs) have become incredibly popular, with ChatGPT for example being used by over a billion users. While these models exhibit remarkable language understanding and logical prowess, a notable challenge surfaces in the form of "hallucinations." This phenomenon results in LLMs outputting misinformation in a confident manner, which can lead to devastating consequences with such a large user base. However, we question the appropriateness of the term "hallucination" in LLMs, proposing a psychological taxonomy based on cognitive biases and other psychological phenomena. Our approach offers a more fine-grained understanding of this phenomenon, allowing for targeted solutions. By leveraging insights from how humans internally resolve similar challenges, we aim to develop strategies to mitigate LLM hallucinations. This interdisciplinary approach seeks to move beyond conventional terminology, providing a nuanced understanding and actionable pathways for improvement in LLM reliability.},
	urldate = {2024-12-06},
	publisher = {arXiv},
	author = {Berberette, Elijah and Hutchins, Jack and Sadovnik, Amir},
	month = feb,
	year = {2024},
	note = {arXiv:2402.01769 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{noauthor_hallucinations_1986,
	title = {Hallucinations: theoretical and clinical overview},
	volume = {143},
	issn = {0002-953X, 1535-7228},
	shorttitle = {Hallucinations},
	url = {https://psychiatryonline.org/doi/10.1176/ajp.143.9.1088},
	doi = {10.1176/ajp.143.9.1088},
	language = {en},
	number = {9},
	urldate = {2024-12-06},
	journal = {American Journal of Psychiatry},
	month = sep,
	year = {1986},
	pages = {1088--1097},
}

@article{navigli_biases_2023,
	title = {Biases in {Large} {Language} {Models}: {Origins}, {Inventory}, and {Discussion}},
	volume = {15},
	issn = {1936-1955, 1936-1963},
	shorttitle = {Biases in {Large} {Language} {Models}},
	url = {https://dl.acm.org/doi/10.1145/3597307},
	doi = {10.1145/3597307},
	abstract = {In this article, we introduce and discuss the pervasive issue of bias in the large language models that are currently at the core of mainstream approaches to Natural Language Processing (NLP). We first introduce data selection bias, that is, the bias caused by the choice of texts that make up a training corpus. Then, we survey the different types of social bias evidenced in the text generated by language models trained on such corpora, ranging from gender to age, from sexual orientation to ethnicity, and from religion to culture. We conclude with directions focused on measuring, reducing, and tackling the aforementioned types of bias.},
	language = {en},
	number = {2},
	urldate = {2024-12-06},
	journal = {Journal of Data and Information Quality},
	author = {Navigli, Roberto and Conia, Simone and Ross, Björn},
	month = jun,
	year = {2023},
	pages = {1--21},
}

@misc{noauthor_configure_nodate,
	title = {Configure safety filters {\textbar} {Generative} {AI} on {Vertex} {AI}},
	url = {https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters},
	language = {en},
	urldate = {2024-12-06},
	journal = {Google Cloud},
}

@misc{hartvigsen_toxigen_2022,
	title = {{ToxiGen}: {A} {Large}-{Scale} {Machine}-{Generated} {Dataset} for {Adversarial} and {Implicit} {Hate} {Speech} {Detection}},
	shorttitle = {{ToxiGen}},
	url = {http://arxiv.org/abs/2203.09509},
	doi = {10.48550/arXiv.2203.09509},
	abstract = {Toxic language detection systems often falsely flag text that contains minority group mentions as toxic, as those groups are often the targets of online hate. Such over-reliance on spurious correlations also causes systems to struggle with detecting implicitly toxic language. To help mitigate these issues, we create ToxiGen, a new large-scale and machine-generated dataset of 274k toxic and benign statements about 13 minority groups. We develop a demonstration-based prompting framework and an adversarial classifier-in-the-loop decoding method to generate subtly toxic and benign text with a massive pretrained language model. Controlling machine generation in this way allows ToxiGen to cover implicitly toxic text at a larger scale, and about more demographic groups, than previous resources of human-written text. We conduct a human evaluation on a challenging subset of ToxiGen and find that annotators struggle to distinguish machine-generated text from human-written language. We also find that 94.5\% of toxic examples are labeled as hate speech by human annotators. Using three publicly-available datasets, we show that finetuning a toxicity classifier on our data improves its performance on human-written data substantially. We also demonstrate that ToxiGen can be used to fight machine-generated toxicity as finetuning improves the classifier significantly on our evaluation subset. Our code and data can be found at https://github.com/microsoft/ToxiGen.},
	urldate = {2024-12-06},
	publisher = {arXiv},
	author = {Hartvigsen, Thomas and Gabriel, Saadia and Palangi, Hamid and Sap, Maarten and Ray, Dipankar and Kamar, Ece},
	month = jul,
	year = {2022},
	note = {arXiv:2203.09509 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{xiong_effective_2023,
	title = {Effective {Long}-{Context} {Scaling} of {Foundation} {Models}},
	url = {http://arxiv.org/abs/2309.16039},
	doi = {10.48550/arXiv.2309.16039},
	abstract = {We present a series of long-context LLMs that support effective context windows of up to 32,768 tokens. Our model series are built through continual pretraining from Llama 2 with longer training sequences and on a dataset where long texts are upsampled. We perform extensive evaluation on language modeling, synthetic context probing tasks, and a wide range of research benchmarks. On research benchmarks, our models achieve consistent improvements on most regular tasks and significant improvements on long-context tasks over Llama 2. Notably, with a cost-effective instruction tuning procedure that does not require human-annotated long instruction data, the 70B variant can already surpass gpt-3.5-turbo-16k's overall performance on a suite of long-context tasks. Alongside these results, we provide an in-depth analysis on the individual components of our method. We delve into Llama's position encodings and discuss its limitation in modeling long dependencies. We also examine the impact of various design choices in the pretraining process, including the data mix and the training curriculum of sequence lengths -- our ablation experiments suggest that having abundant long texts in the pretrain dataset is not the key to achieving strong performance, and we empirically verify that long context continual pretraining is more efficient and similarly effective compared to pretraining from scratch with long sequences.},
	urldate = {2024-12-06},
	publisher = {arXiv},
	author = {Xiong, Wenhan and Liu, Jingyu and Molybog, Igor and Zhang, Hejia and Bhargava, Prajjwal and Hou, Rui and Martin, Louis and Rungta, Rashi and Sankararaman, Karthik Abinav and Oguz, Barlas and Khabsa, Madian and Fang, Han and Mehdad, Yashar and Narang, Sharan and Malik, Kshitiz and Fan, Angela and Bhosale, Shruti and Edunov, Sergey and Lewis, Mike and Wang, Sinong and Ma, Hao},
	month = nov,
	year = {2023},
	note = {arXiv:2309.16039 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{tang_evaluating_2023,
	title = {Evaluating large language models on medical evidence summarization},
	volume = {6},
	copyright = {2023 The Author(s)},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-023-00896-7},
	doi = {10.1038/s41746-023-00896-7},
	abstract = {Recent advances in large language models (LLMs) have demonstrated remarkable successes in zero- and few-shot performance on various downstream tasks, paving the way for applications in high-stakes domains. In this study, we systematically examine the capabilities and limitations of LLMs, specifically GPT-3.5 and ChatGPT, in performing zero-shot medical evidence summarization across six clinical domains. We conduct both automatic and human evaluations, covering several dimensions of summary quality. Our study demonstrates that automatic metrics often do not strongly correlate with the quality of summaries. Furthermore, informed by our human evaluations, we define a terminology of error types for medical evidence summarization. Our findings reveal that LLMs could be susceptible to generating factually inconsistent summaries and making overly convincing or uncertain statements, leading to potential harm due to misinformation. Moreover, we find that models struggle to identify the salient information and are more error-prone when summarizing over longer textual contexts.},
	language = {en},
	number = {1},
	urldate = {2024-12-06},
	journal = {npj Digital Medicine},
	author = {Tang, Liyan and Sun, Zhaoyi and Idnay, Betina and Nestor, Jordan G. and Soroush, Ali and Elias, Pierre A. and Xu, Ziyang and Ding, Ying and Durrett, Greg and Rousseau, Justin F. and Weng, Chunhua and Peng, Yifan},
	month = aug,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Literature mining, Medical ethics, Translational research},
	pages = {1--8},
}

@article{peng_study_2023,
	title = {A {Study} of {Generative} {Large} {Language} {Model} for {Medical} {Research} and {Healthcare}},
	volume = {6},
	issn = {2398-6352},
	url = {http://arxiv.org/abs/2305.13523},
	doi = {10.1038/s41746-023-00958-w},
	abstract = {There is enormous enthusiasm and concerns in using large language models (LLMs) in healthcare, yet current assumptions are all based on general-purpose LLMs such as ChatGPT. This study develops a clinical generative LLM, GatorTronGPT, using 277 billion words of mixed clinical and English text with a GPT-3 architecture of 20 billion parameters. GatorTronGPT improves biomedical natural language processing for medical research. Synthetic NLP models trained using GatorTronGPT generated text outperform NLP models trained using real-world clinical text. Physicians Turing test using 1 (worst) to 9 (best) scale shows that there is no significant difference in linguistic readability (p = 0.22; 6.57 of GatorTronGPT compared with 6.93 of human) and clinical relevance (p = 0.91; 7.0 of GatorTronGPT compared with 6.97 of human) and that physicians cannot differentiate them (p {\textless} 0.001). This study provides insights on the opportunities and challenges of LLMs for medical research and healthcare.},
	number = {1},
	urldate = {2024-12-06},
	journal = {npj Digital Medicine},
	author = {Peng, Cheng and Yang, Xi and Chen, Aokun and Smith, Kaleb E. and PourNejatian, Nima and Costa, Anthony B. and Martin, Cheryl and Flores, Mona G. and Zhang, Ying and Magoc, Tanja and Lipori, Gloria and Mitchell, Duane A. and Ospina, Naykky S. and Ahmed, Mustafa M. and Hogan, William R. and Shenkman, Elizabeth A. and Guo, Yi and Bian, Jiang and Wu, Yonghui},
	month = nov,
	year = {2023},
	note = {arXiv:2305.13523 [cs]},
	keywords = {Computer Science - Computation and Language},
	pages = {210},
}

@article{kung_performance_2023,
	title = {Performance of {ChatGPT} on {USMLE}: {Potential} for {AI}-assisted medical education using large language models},
	volume = {2},
	issn = {2767-3170},
	shorttitle = {Performance of {ChatGPT} on {USMLE}},
	url = {https://journals.plos.org/digitalhealth/article?id=10.1371/journal.pdig.0000198},
	doi = {10.1371/journal.pdig.0000198},
	abstract = {We evaluated the performance of a large language model called ChatGPT on the United States Medical Licensing Exam (USMLE), which consists of three exams: Step 1, Step 2CK, and Step 3. ChatGPT performed at or near the passing threshold for all three exams without any specialized training or reinforcement. Additionally, ChatGPT demonstrated a high level of concordance and insight in its explanations. These results suggest that large language models may have the potential to assist with medical education, and potentially, clinical decision-making.},
	language = {en},
	number = {2},
	urldate = {2024-12-06},
	journal = {PLOS Digital Health},
	author = {Kung, Tiffany H. and Cheatham, Morgan and Medenilla, Arielle and Sillos, Czarina and Leon, Lorie De and Elepaño, Camille and Madriaga, Maria and Aggabao, Rimel and Diaz-Candido, Giezel and Maningo, James and Tseng, Victor},
	month = feb,
	year = {2023},
	note = {Publisher: Public Library of Science},
	keywords = {Artificial intelligence, Human learning, Language, Language acquisition, Medical education, Physicians, Programming languages, Reasoning},
	pages = {e0000198},
}

@misc{singhal_large_2022,
	title = {Large {Language} {Models} {Encode} {Clinical} {Knowledge}},
	url = {http://arxiv.org/abs/2212.13138},
	doi = {10.48550/arXiv.2212.13138},
	abstract = {Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, but the quality bar for medical and clinical applications is high. Today, attempts to assess models' clinical knowledge typically rely on automated evaluations on limited benchmarks. There is no standard to evaluate model predictions and reasoning across a breadth of tasks. To address this, we present MultiMedQA, a benchmark combining six existing open question answering datasets spanning professional medical exams, research, and consumer queries; and HealthSearchQA, a new free-response dataset of medical questions searched online. We propose a framework for human evaluation of model answers along multiple axes including factuality, precision, possible harm, and bias. In addition, we evaluate PaLM (a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM, on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA, MedMCQA, PubMedQA, MMLU clinical topics), including 67.6\% accuracy on MedQA (US Medical License Exam questions), surpassing prior state-of-the-art by over 17\%. However, human evaluation reveals key gaps in Flan-PaLM responses. To resolve this we introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show that comprehension, recall of knowledge, and medical reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal important limitations of today's models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLM models for clinical applications.},
	urldate = {2024-12-06},
	publisher = {arXiv},
	author = {Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S. Sara and Wei, Jason and Chung, Hyung Won and Scales, Nathan and Tanwani, Ajay and Cole-Lewis, Heather and Pfohl, Stephen and Payne, Perry and Seneviratne, Martin and Gamble, Paul and Kelly, Chris and Scharli, Nathaneal and Chowdhery, Aakanksha and Mansfield, Philip and Arcas, Blaise Aguera y and Webster, Dale and Corrado, Greg S. and Matias, Yossi and Chou, Katherine and Gottweis, Juraj and Tomasev, Nenad and Liu, Yun and Rajkomar, Alvin and Barral, Joelle and Semturs, Christopher and Karthikesalingam, Alan and Natarajan, Vivek},
	month = dec,
	year = {2022},
	note = {arXiv:2212.13138 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{zhang_bertscore_2020,
	title = {{BERTScore}: {Evaluating} {Text} {Generation} with {BERT}},
	shorttitle = {{BERTScore}},
	url = {http://arxiv.org/abs/1904.09675},
	doi = {10.48550/arXiv.1904.09675},
	abstract = {We propose BERTScore, an automatic evaluation metric for text generation. Analogously to common metrics, BERTScore computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTScore correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task to show that BERTScore is more robust to challenging examples when compared to existing metrics.},
	urldate = {2024-12-05},
	publisher = {arXiv},
	author = {Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q. and Artzi, Yoav},
	month = feb,
	year = {2020},
	note = {arXiv:1904.09675 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{scialom_questeval_2021,
	title = {{QuestEval}: {Summarization} {Asks} for {Fact}-based {Evaluation}},
	shorttitle = {{QuestEval}},
	url = {http://arxiv.org/abs/2103.12693},
	doi = {10.48550/arXiv.2103.12693},
	abstract = {Summarization evaluation remains an open research problem: current metrics such as ROUGE are known to be limited and to correlate poorly with human judgments. To alleviate this issue, recent work has proposed evaluation metrics which rely on question answering models to assess whether a summary contains all the relevant information in its source document. Though promising, the proposed approaches have so far failed to correlate better than ROUGE with human judgments. In this paper, we extend previous approaches and propose a unified framework, named QuestEval. In contrast to established metrics such as ROUGE or BERTScore, QuestEval does not require any ground-truth reference. Nonetheless, QuestEval substantially improves the correlation with human judgments over four evaluation dimensions (consistency, coherence, fluency, and relevance), as shown in the extensive experiments we report.},
	urldate = {2024-12-05},
	publisher = {arXiv},
	author = {Scialom, Thomas and Dray, Paul-Alexis and Gallinari, Patrick and Lamprier, Sylvain and Piwowarski, Benjamin and Staiano, Jacopo and Wang, Alex},
	month = apr,
	year = {2021},
	note = {arXiv:2103.12693 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{noauthor_intro_2017,
	title = {An intro to {ROUGE}, and how to use it to evaluate summaries},
	url = {https://www.freecodecamp.org/news/what-is-rouge-and-how-it-works-for-evaluation-of-summaries-e059fb8ac840/},
	abstract = {By Kavita Ganesan ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It is essentially a set of metrics for evaluating automatic summarization of texts as well as machine translations. It works by comparing an automatically produced ...},
	language = {en},
	urldate = {2024-12-04},
	journal = {freeCodeCamp.org},
	month = jan,
	year = {2017},
}

@misc{huang_inner_2022,
	title = {Inner {Monologue}: {Embodied} {Reasoning} through {Planning} with {Language} {Models}},
	shorttitle = {Inner {Monologue}},
	url = {http://arxiv.org/abs/2207.05608},
	doi = {10.48550/arXiv.2207.05608},
	abstract = {Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent's own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high-level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.},
	urldate = {2024-12-04},
	publisher = {arXiv},
	author = {Huang, Wenlong and Xia, Fei and Xiao, Ted and Chan, Harris and Liang, Jacky and Florence, Pete and Zeng, Andy and Tompson, Jonathan and Mordatch, Igor and Chebotar, Yevgen and Sermanet, Pierre and Brown, Noah and Jackson, Tomas and Luu, Linda and Levine, Sergey and Hausman, Karol and Ichter, Brian},
	month = jul,
	year = {2022},
	note = {arXiv:2207.05608 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{kocmi_large_2023,
	title = {Large {Language} {Models} {Are} {State}-of-the-{Art} {Evaluators} of {Translation} {Quality}},
	url = {http://arxiv.org/abs/2302.14520},
	doi = {10.48550/arXiv.2302.14520},
	abstract = {We describe GEMBA, a GPT-based metric for assessment of translation quality, which works both with a reference translation and without. In our evaluation, we focus on zero-shot prompting, comparing four prompt variants in two modes, based on the availability of the reference. We investigate nine versions of GPT models, including ChatGPT and GPT-4. We show that our method for translation quality assessment only works with GPT{\textasciitilde}3.5 and larger models. Comparing to results from WMT22's Metrics shared task, our method achieves state-of-the-art accuracy in both modes when compared to MQM-based human labels. Our results are valid on the system level for all three WMT22 Metrics shared task language pairs, namely English into German, English into Russian, and Chinese into English. This provides a first glimpse into the usefulness of pre-trained, generative large language models for quality assessment of translations. We publicly release all our code and prompt templates used for the experiments described in this work, as well as all corresponding scoring results, to allow for external validation and reproducibility.},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Kocmi, Tom and Federmann, Christian},
	month = may,
	year = {2023},
	note = {arXiv:2302.14520},
	keywords = {Computer Science - Computation and Language},
}

@misc{wang_is_2023,
	title = {Is {ChatGPT} a {Good} {NLG} {Evaluator}? {A} {Preliminary} {Study}},
	shorttitle = {Is {ChatGPT} a {Good} {NLG} {Evaluator}?},
	url = {http://arxiv.org/abs/2303.04048},
	abstract = {Recently, the emergence of ChatGPT has attracted wide attention from the computational linguistics community. Many prior studies have shown that ChatGPT achieves remarkable performance on various NLP tasks in terms of automatic evaluation metrics. However, the ability of ChatGPT to serve as an evaluation metric is still underexplored. Considering assessing the quality of natural language generation (NLG) models is an arduous task and NLG metrics notoriously show their poor correlation with human judgments, we wonder whether ChatGPT is a good NLG evaluation metric. In this report, we provide a preliminary meta-evaluation on ChatGPT to show its reliability as an NLG metric. In detail, we regard ChatGPT as a human evaluator and give task-specific (e.g., summarization) and aspect-specific (e.g., relevance) instruction to prompt ChatGPT to evaluate the generated results of NLG models. We conduct experiments on five NLG meta-evaluation datasets (including summarization, story generation and data-to-text tasks). Experimental results show that compared with previous automatic metrics, ChatGPT achieves state-of-the-art or competitive correlation with human judgments in most cases. In addition, we find that the effectiveness of the ChatGPT evaluator might be influenced by the creation method of the meta-evaluation datasets. For the meta-evaluation datasets which are created greatly depending on the reference and thus are biased, the ChatGPT evaluator might lose its effectiveness. We hope our preliminary study could prompt the emergence of a general-purposed reliable NLG metric.},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Wang, Jiaan and Liang, Yunlong and Meng, Fandong and Sun, Zengkui and Shi, Haoxiang and Li, Zhixu and Xu, Jinan and Qu, Jianfeng and Zhou, Jie},
	month = oct,
	year = {2023},
	note = {arXiv:2303.04048},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{noauthor_classifying_nodate,
	title = {Classifying and tagging {PII} fields residing in {BigQuery} and automating access control to them.},
	url = {https://cloud.google.com/blog/products/identity-security/how-to-use-google-cloud-to-find-and-protect-pii},
	abstract = {Google Cloud’s open source automatic tagging solution can automate column-level access controls in order to minimize the risk of unrestricted access to sensitive data or PII in BigQuery},
	language = {en},
	urldate = {2024-11-28},
	journal = {Google Cloud Blog},
}

@misc{li_leveraging_2024,
	title = {Leveraging {Large} {Language} {Models} for {NLG} {Evaluation}: {Advances} and {Challenges}},
	shorttitle = {Leveraging {Large} {Language} {Models} for {NLG} {Evaluation}},
	url = {http://arxiv.org/abs/2401.07103},
	doi = {10.48550/arXiv.2401.07103},
	abstract = {In the rapidly evolving domain of Natural Language Generation (NLG) evaluation, introducing Large Language Models (LLMs) has opened new avenues for assessing generated content quality, e.g., coherence, creativity, and context relevance. This paper aims to provide a thorough overview of leveraging LLMs for NLG evaluation, a burgeoning area that lacks a systematic analysis. We propose a coherent taxonomy for organizing existing LLM-based evaluation metrics, offering a structured framework to understand and compare these methods. Our detailed exploration includes critically assessing various LLM-based methodologies, as well as comparing their strengths and limitations in evaluating NLG outputs. By discussing unresolved challenges, including bias, robustness, domain-specificity, and unified evaluation, this paper seeks to offer insights to researchers and advocate for fairer and more advanced NLG evaluation techniques.},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Li, Zhen and Xu, Xiaohan and Shen, Tao and Xu, Can and Gu, Jia-Chen and Lai, Yuxuan and Tao, Chongyang and Ma, Shuai},
	month = jun,
	year = {2024},
	note = {arXiv:2401.07103},
	keywords = {Computer Science - Computation and Language},
}

@misc{elvis_omarsar0_overview_2024,
	type = {Tweet},
	title = {Overview of {LLMs} for {Evaluation} {LLMs} as tools for evaluation is growing rapidly. {This} new paper thoroughly surveys the methodologies and explores their strengths and limitations. {There} are a lot of creative ways in which {LLMs} are used for evaluation. {The} taxonomy below https://t.co/{R1D8j01NR6}},
	url = {https://x.com/omarsar0/status/1748016227090305167},
	language = {en},
	urldate = {2024-11-28},
	journal = {Twitter},
	author = {{elvis [@omarsar0]}},
	month = jan,
	year = {2024},
}

@misc{aparna_dhinakaran_aparnadhinak_openai_2024,
	type = {Tweet},
	title = {@{OpenAI} @{ArizePhoenix} @arizeai (6/9) {The} distributions of these scores across runs tell an even more interesting story 🤔 {We} ran multiple tests with different random seeds for each \% of corruption. {The} scores for each \% level are collected and visualized as a distribution. {The} two things that stood out to us https://t.co/{LHGPURoBLy}},
	url = {https://x.com/aparnadhinak/status/1748381257208152221},
	language = {en},
	urldate = {2024-11-28},
	journal = {Twitter},
	author = {{Aparna Dhinakaran [@aparnadhinak]}},
	month = jan,
	year = {2024},
}

@misc{valentin_cost-effective_2024,
	title = {Cost-{Effective} {Hallucination} {Detection} for {LLMs}},
	url = {http://arxiv.org/abs/2407.21424},
	doi = {10.48550/arXiv.2407.21424},
	abstract = {Large language models (LLMs) can be prone to hallucinations - generating unreliable outputs that are unfaithful to their inputs, external facts or internally inconsistent. In this work, we address several challenges for post-hoc hallucination detection in production settings. Our pipeline for hallucination detection entails: first, producing a confidence score representing the likelihood that a generated answer is a hallucination; second, calibrating the score conditional on attributes of the inputs and candidate response; finally, performing detection by thresholding the calibrated score. We benchmark a variety of state-of-the-art scoring methods on different datasets, encompassing question answering, fact checking, and summarization tasks. We employ diverse LLMs to ensure a comprehensive assessment of performance. We show that calibrating individual scoring methods is critical for ensuring risk-aware downstream decision making. Based on findings that no individual score performs best in all situations, we propose a multi-scoring framework, which combines different scores and achieves top performance across all datasets. We further introduce cost-effective multi-scoring, which can match or even outperform more expensive detection methods, while significantly reducing computational overhead.},
	urldate = {2024-11-27},
	publisher = {arXiv},
	author = {Valentin, Simon and Fu, Jinmiao and Detommaso, Gianluca and Xu, Shaoyuan and Zappella, Giovanni and Wang, Bryan},
	month = aug,
	year = {2024},
	note = {arXiv:2407.21424},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{sriramanan_llm-check_2024,
	title = {{LLM}-{Check}: {Investigating} {Detection} of {Hallucinations} in {Large} {Language} {Models}},
	shorttitle = {{LLM}-{Check}},
	url = {https://openreview.net/forum?id=LYx4w3CAgy},
	abstract = {While Large Language Models (LLMs) have become immensely popular due to their outstanding performance on a broad range of tasks, these models are prone to producing hallucinations— outputs that are fallacious or fabricated yet often appear plausible or tenable at a glance. In this paper, we conduct a comprehensive investigation into the nature of hallucinations within LLMs and furthermore explore effective techniques for detecting such inaccuracies in various real-world settings. Prior approaches to detect hallucinations in LLM outputs, such as consistency checks or retrieval-based methods, typically assume access to multiple model responses or large databases. These techniques, however, tend to be computationally expensive in practice, thereby limiting their applicability to real-time analysis. In contrast, in this work, we seek to identify hallucinations within a single response in both white-box and black-box settings by analyzing the internal hidden states, attention maps, and output prediction probabilities of an auxiliary LLM. In addition, we also study hallucination detection in scenarios where ground-truth references are also available, such as in the setting of Retrieval-Augmented Generation (RAG). We demonstrate that the proposed detection methods are extremely compute-efficient, with speedups of up to 45x and 450x over other baselines, while achieving significant improvements in detection performance over diverse datasets.},
	language = {en},
	urldate = {2024-11-27},
	author = {Sriramanan, Gaurang and Bharti, Siddhant and Sadasivan, Vinu Sankar and Saha, Shoumik and Kattakinda, Priyatham and Feizi, Soheil},
	month = nov,
	year = {2024},
}

@misc{noauthor_derstandardat_nodate,
	title = {{derStandard}.at {\textbar} {Nachrichten}, {Kommentare} \& {Community}},
	url = {https://www.derstandard.at/},
	abstract = {DER STANDARD – Nachrichten in Echtzeit: Lesen Sie jetzt Nachrichten aktuell aus dem online News-Room der führenden Qualitätszeitung in Österreich.},
	language = {de-AT},
	urldate = {2024-11-20},
	journal = {DER STANDARD},
}

@misc{noauthor_derstandardat_nodate-1,
	title = {{derStandard}.at},
	url = {https://www.derstandard.at/consent/tcf/},
	urldate = {2024-11-20},
}

@article{snyder_literature_2019,
	title = {Literature review as a research methodology: {An} overview and guidelines},
	volume = {104},
	issn = {0148-2963},
	shorttitle = {Literature review as a research methodology},
	url = {https://www.sciencedirect.com/science/article/pii/S0148296319304564},
	doi = {10.1016/j.jbusres.2019.07.039},
	abstract = {Knowledge production within the field of business research is accelerating at a tremendous speed while at the same time remaining fragmented and interdisciplinary. This makes it hard to keep up with state-of-the-art and to be at the forefront of research, as well as to assess the collective evidence in a particular area of business research. This is why the literature review as a research method is more relevant than ever. Traditional literature reviews often lack thoroughness and rigor and are conducted ad hoc, rather than following a specific methodology. Therefore, questions can be raised about the quality and trustworthiness of these types of reviews. This paper discusses literature review as a methodology for conducting research and offers an overview of different types of reviews, as well as some guidelines to how to both conduct and evaluate a literature review paper. It also discusses common pitfalls and how to get literature reviews published.},
	urldate = {2024-11-20},
	journal = {Journal of Business Research},
	author = {Snyder, Hannah},
	month = nov,
	year = {2019},
	keywords = {Integrative review, Literature review, Research methodology, Synthesis, Systematic review},
	pages = {333--339},
}

@article{ji_survey_2023,
	title = {Survey of {Hallucination} in {Natural} {Language} {Generation}},
	volume = {55},
	issn = {0360-0300},
	url = {https://dl.acm.org/doi/10.1145/3571730},
	doi = {10.1145/3571730},
	abstract = {Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation, and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before.In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions, and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.},
	number = {12},
	urldate = {2024-11-20},
	journal = {ACM Comput. Surv.},
	author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
	month = mar,
	year = {2023},
	pages = {248:1--248:38},
}

@misc{noauthor_you_2018,
	title = {You aren’t at the mercy of your emotions ({TED} {Talk})},
	url = {https://lisafeldmanbarrett.com/2018/01/13/ted-talk-you-arent-at-the-mercy-of-your-emotions-your-brain-creates-them/},
	abstract = {Can you look at someone’s face and know what they’re feeling? Does everyone experience happiness, sadness and anxiety the same way? What are emotions anyway, and how much control can we have over them?},
	language = {en-US},
	urldate = {2024-11-18},
	journal = {Lisa Feldman Barrett},
	month = jan,
	year = {2018},
}

@misc{srivastava_beyond_2023,
	title = {Beyond the {Imitation} {Game}: {Quantifying} and extrapolating the capabilities of language models},
	shorttitle = {Beyond the {Imitation} {Game}},
	url = {http://arxiv.org/abs/2206.04615},
	abstract = {Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit "breakthrough" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.},
	urldate = {2024-11-18},
	publisher = {arXiv},
	author = {Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R. and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adrià and Kluska, Agnieszka and Lewkowycz, Aitor and Agarwal, Akshat and Power, Alethea and Ray, Alex and Warstadt, Alex and Kocurek, Alexander W. and Safaya, Ali and Tazarv, Ali and Xiang, Alice and Parrish, Alicia and Nie, Allen and Hussain, Aman and Askell, Amanda and Dsouza, Amanda and Slone, Ambrose and Rahane, Ameet and Iyer, Anantharaman S. and Andreassen, Anders and Madotto, Andrea and Santilli, Andrea and Stuhlmüller, Andreas and Dai, Andrew and La, Andrew and Lampinen, Andrew and Zou, Andy and Jiang, Angela and Chen, Angelica and Vuong, Anh and Gupta, Animesh and Gottardi, Anna and Norelli, Antonio and Venkatesh, Anu and Gholamidavoodi, Arash and Tabassum, Arfa and Menezes, Arul and Kirubarajan, Arun and Mullokandov, Asher and Sabharwal, Ashish and Herrick, Austin and Efrat, Avia and Erdem, Aykut and Karakaş, Ayla and Roberts, B. Ryan and Loe, Bao Sheng and Zoph, Barret and Bojanowski, Bartłomiej and Özyurt, Batuhan and Hedayatnia, Behnam and Neyshabur, Behnam and Inden, Benjamin and Stein, Benno and Ekmekci, Berk and Lin, Bill Yuchen and Howald, Blake and Orinion, Bryan and Diao, Cameron and Dour, Cameron and Stinson, Catherine and Argueta, Cedrick and Ramírez, César Ferri and Singh, Chandan and Rathkopf, Charles and Meng, Chenlin and Baral, Chitta and Wu, Chiyu and Callison-Burch, Chris and Waites, Chris and Voigt, Christian and Manning, Christopher D. and Potts, Christopher and Ramirez, Cindy and Rivera, Clara E. and Siro, Clemencia and Raffel, Colin and Ashcraft, Courtney and Garbacea, Cristina and Sileo, Damien and Garrette, Dan and Hendrycks, Dan and Kilman, Dan and Roth, Dan and Freeman, Daniel and Khashabi, Daniel and Levy, Daniel and González, Daniel Moseguí and Perszyk, Danielle and Hernandez, Danny and Chen, Danqi and Ippolito, Daphne and Gilboa, Dar and Dohan, David and Drakard, David and Jurgens, David and Datta, Debajyoti and Ganguli, Deep and Emelin, Denis and Kleyko, Denis and Yuret, Deniz and Chen, Derek and Tam, Derek and Hupkes, Dieuwke and Misra, Diganta and Buzan, Dilyar and Mollo, Dimitri Coelho and Yang, Diyi and Lee, Dong-Ho and Schrader, Dylan and Shutova, Ekaterina and Cubuk, Ekin Dogus and Segal, Elad and Hagerman, Eleanor and Barnes, Elizabeth and Donoway, Elizabeth and Pavlick, Ellie and Rodola, Emanuele and Lam, Emma and Chu, Eric and Tang, Eric and Erdem, Erkut and Chang, Ernie and Chi, Ethan A. and Dyer, Ethan and Jerzak, Ethan and Kim, Ethan and Manyasi, Eunice Engefu and Zheltonozhskii, Evgenii and Xia, Fanyue and Siar, Fatemeh and Martínez-Plumed, Fernando and Happé, Francesca and Chollet, Francois and Rong, Frieda and Mishra, Gaurav and Winata, Genta Indra and Melo, Gerard de and Kruszewski, Germán and Parascandolo, Giambattista and Mariani, Giorgio and Wang, Gloria and Jaimovitch-López, Gonzalo and Betz, Gregor and Gur-Ari, Guy and Galijasevic, Hana and Kim, Hannah and Rashkin, Hannah and Hajishirzi, Hannaneh and Mehta, Harsh and Bogar, Hayden and Shevlin, Henry and Schütze, Hinrich and Yakura, Hiromu and Zhang, Hongming and Wong, Hugh Mee and Ng, Ian and Noble, Isaac and Jumelet, Jaap and Geissinger, Jack and Kernion, Jackson and Hilton, Jacob and Lee, Jaehoon and Fisac, Jaime Fernández and Simon, James B. and Koppel, James and Zheng, James and Zou, James and Kocoń, Jan and Thompson, Jana and Wingfield, Janelle and Kaplan, Jared and Radom, Jarema and Sohl-Dickstein, Jascha and Phang, Jason and Wei, Jason and Yosinski, Jason and Novikova, Jekaterina and Bosscher, Jelle and Marsh, Jennifer and Kim, Jeremy and Taal, Jeroen and Engel, Jesse and Alabi, Jesujoba and Xu, Jiacheng and Song, Jiaming and Tang, Jillian and Waweru, Joan and Burden, John and Miller, John and Balis, John U. and Batchelder, Jonathan and Berant, Jonathan and Frohberg, Jörg and Rozen, Jos and Hernandez-Orallo, Jose and Boudeman, Joseph and Guerr, Joseph and Jones, Joseph and Tenenbaum, Joshua B. and Rule, Joshua S. and Chua, Joyce and Kanclerz, Kamil and Livescu, Karen and Krauth, Karl and Gopalakrishnan, Karthik and Ignatyeva, Katerina and Markert, Katja and Dhole, Kaustubh D. and Gimpel, Kevin and Omondi, Kevin and Mathewson, Kory and Chiafullo, Kristen and Shkaruta, Ksenia and Shridhar, Kumar and McDonell, Kyle and Richardson, Kyle and Reynolds, Laria and Gao, Leo and Zhang, Li and Dugan, Liam and Qin, Lianhui and Contreras-Ochando, Lidia and Morency, Louis-Philippe and Moschella, Luca and Lam, Lucas and Noble, Lucy and Schmidt, Ludwig and He, Luheng and Colón, Luis Oliveros and Metz, Luke and Şenel, Lütfi Kerem and Bosma, Maarten and Sap, Maarten and Hoeve, Maartje ter and Farooqi, Maheen and Faruqui, Manaal and Mazeika, Mantas and Baturan, Marco and Marelli, Marco and Maru, Marco and Quintana, Maria Jose Ramírez and Tolkiehn, Marie and Giulianelli, Mario and Lewis, Martha and Potthast, Martin and Leavitt, Matthew L. and Hagen, Matthias and Schubert, Mátyás and Baitemirova, Medina Orduna and Arnaud, Melody and McElrath, Melvin and Yee, Michael A. and Cohen, Michael and Gu, Michael and Ivanitskiy, Michael and Starritt, Michael and Strube, Michael and Swędrowski, Michał and Bevilacqua, Michele and Yasunaga, Michihiro and Kale, Mihir and Cain, Mike and Xu, Mimee and Suzgun, Mirac and Walker, Mitch and Tiwari, Mo and Bansal, Mohit and Aminnaseri, Moin and Geva, Mor and Gheini, Mozhdeh and T, Mukund Varma and Peng, Nanyun and Chi, Nathan A. and Lee, Nayeon and Krakover, Neta Gur-Ari and Cameron, Nicholas and Roberts, Nicholas and Doiron, Nick and Martinez, Nicole and Nangia, Nikita and Deckers, Niklas and Muennighoff, Niklas and Keskar, Nitish Shirish and Iyer, Niveditha S. and Constant, Noah and Fiedel, Noah and Wen, Nuan and Zhang, Oliver and Agha, Omar and Elbaghdadi, Omar and Levy, Omer and Evans, Owain and Casares, Pablo Antonio Moreno and Doshi, Parth and Fung, Pascale and Liang, Paul Pu and Vicol, Paul and Alipoormolabashi, Pegah and Liao, Peiyuan and Liang, Percy and Chang, Peter and Eckersley, Peter and Htut, Phu Mon and Hwang, Pinyu and Miłkowski, Piotr and Patil, Piyush and Pezeshkpour, Pouya and Oli, Priti and Mei, Qiaozhu and Lyu, Qing and Chen, Qinlang and Banjade, Rabin and Rudolph, Rachel Etta and Gabriel, Raefer and Habacker, Rahel and Risco, Ramon and Millière, Raphaël and Garg, Rhythm and Barnes, Richard and Saurous, Rif A. and Arakawa, Riku and Raymaekers, Robbe and Frank, Robert and Sikand, Rohan and Novak, Roman and Sitelew, Roman and LeBras, Ronan and Liu, Rosanne and Jacobs, Rowan and Zhang, Rui and Salakhutdinov, Ruslan and Chi, Ryan and Lee, Ryan and Stovall, Ryan and Teehan, Ryan and Yang, Rylan and Singh, Sahib and Mohammad, Saif M. and Anand, Sajant and Dillavou, Sam and Shleifer, Sam and Wiseman, Sam and Gruetter, Samuel and Bowman, Samuel R. and Schoenholz, Samuel S. and Han, Sanghyun and Kwatra, Sanjeev and Rous, Sarah A. and Ghazarian, Sarik and Ghosh, Sayan and Casey, Sean and Bischoff, Sebastian and Gehrmann, Sebastian and Schuster, Sebastian and Sadeghi, Sepideh and Hamdan, Shadi and Zhou, Sharon and Srivastava, Shashank and Shi, Sherry and Singh, Shikhar and Asaadi, Shima and Gu, Shixiang Shane and Pachchigar, Shubh and Toshniwal, Shubham and Upadhyay, Shyam and Shyamolima and Debnath and Shakeri, Siamak and Thormeyer, Simon and Melzi, Simone and Reddy, Siva and Makini, Sneha Priscilla and Lee, Soo-Hwan and Torene, Spencer and Hatwar, Sriharsha and Dehaene, Stanislas and Divic, Stefan and Ermon, Stefano and Biderman, Stella and Lin, Stephanie and Prasad, Stephen and Piantadosi, Steven T. and Shieber, Stuart M. and Misherghi, Summer and Kiritchenko, Svetlana and Mishra, Swaroop and Linzen, Tal and Schuster, Tal and Li, Tao and Yu, Tao and Ali, Tariq and Hashimoto, Tatsu and Wu, Te-Lin and Desbordes, Théo and Rothschild, Theodore and Phan, Thomas and Wang, Tianle and Nkinyili, Tiberius and Schick, Timo and Kornev, Timofei and Tunduny, Titus and Gerstenberg, Tobias and Chang, Trenton and Neeraj, Trishala and Khot, Tushar and Shultz, Tyler and Shaham, Uri and Misra, Vedant and Demberg, Vera and Nyamai, Victoria and Raunak, Vikas and Ramasesh, Vinay and Prabhu, Vinay Uday and Padmakumar, Vishakh and Srikumar, Vivek and Fedus, William and Saunders, William and Zhang, William and Vossen, Wout and Ren, Xiang and Tong, Xiaoyu and Zhao, Xinran and Wu, Xinyi and Shen, Xudong and Yaghoobzadeh, Yadollah and Lakretz, Yair and Song, Yangqiu and Bahri, Yasaman and Choi, Yejin and Yang, Yichi and Hao, Yiding and Chen, Yifu and Belinkov, Yonatan and Hou, Yu and Hou, Yufang and Bai, Yuntao and Seid, Zachary and Zhao, Zhuoye and Wang, Zijian and Wang, Zijie J. and Wang, Zirui and Wu, Ziyi},
	month = jun,
	year = {2023},
	note = {arXiv:2206.04615},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{patel_identification_2024,
	title = {Identification and {Description} of {Emotions} by {Current} {Large} {Language} {Models}},
	copyright = {© 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2023.07.17.549421v2},
	doi = {10.1101/2023.07.17.549421},
	abstract = {The assertion that artificial intelligence (AI) cannot grasp the complexities of human emotions has been a long-standing debate. However, recent advancements in large language models (LLMs) challenge this notion by demonstrating an increased capacity for understanding and generating human-like text. In this study, we evaluated the empathy levels and the identification and description of emotions by three current language models: Bard, GPT 3.5, and GPT 4. We used the Toronto Alexithymia Scale (TAS-20) and the 60-question Empathy Quotient (EQ-60) questions to prompt these models and score the responses. The models’ performance was contrasted with human benchmarks of neurotypical controls and clinical populations. We found that the less sophisticated models (Bard and GPT 3.5) performed inferiorly on TAS-20, aligning close to alexithymia, a condition with significant difficulties in recognizing, expressing, and describing one’s or others’ experienced emotions. However, GPT 4 achieved performance close to the human level. These results demonstrated that LLMs are comparable in their ability to identify and describe emotions and may be able to surpass humans in their capacity for emotional intelligence. Our novel insights provide alignment research benchmarks and a methodology for aligning AI with human values, leading toward an empathetic AI that mitigates risk.},
	language = {en},
	urldate = {2024-11-16},
	publisher = {bioRxiv},
	author = {Patel, Suketu C. and Fan, Jin},
	month = jul,
	year = {2024},
	note = {Pages: 2023.07.17.549421
Section: Confirmatory Results},
}

@article{russell_core_1999,
	title = {Core affect, prototypical emotional episodes, and other things called emotion: dissecting the elephant.},
	volume = {76},
	shorttitle = {Core affect, prototypical emotional episodes, and other things called emotion},
	url = {https://psycnet.apa.org/journals/psp/76/5/805/},
	number = {5},
	urldate = {2024-04-04},
	journal = {Journal of personality and social psychology},
	author = {Russell, James A. and Barrett, Lisa Feldman},
	year = {1999},
	note = {Publisher: American Psychological Association},
	keywords = {Emotional Content, Emotions},
	pages = {805},
}

@article{ke_exploring_nodate,
	title = {Exploring the {Frontiers} of {LLMs} in {Psychological} {Applications}: {A} {Comprehensive} {Review}},
	abstract = {This paper explores the frontiers of large language models (LLMs) in psychology applications. Psychology has undergone several theoretical changes, and the current use of Artificial Intelligence (AI) and Machine Learning, particularly LLMs, promises to open up new research directions. We provide a detailed exploration of how LLMs like ChatGPT transform psychological research. It discusses the impact of LLMs across various branches of psychology, including cognitive and behavioral, clinical and counseling, educational and developmental, and social and cultural psychology, highlighting their potential to simulate aspects of human cognition and behavior. The paper delves into the capabilities of these models to emulate human-like text generation, offering innovative tools for literature review, hypothesis generation, experimental design, experimental subjects, data analysis, academic writing, and peer review in psychology. While LLMs are essential in advancing research methodologies in psychology, the paper also cautions about their technical and ethical challenges. There are issues like data privacy, the ethical implications of using LLMs in psychological research, and the need for a deeper understanding of these models' limitations. Researchers should responsibly use LLMs in psychological studies, adhering to ethical standards and considering the potential consequences of deploying these technologies in sensitive areas. Overall, the article provides a comprehensive overview of the current state of LLMs in psychology, exploring potential benefits and challenges. It serves as a call to action for researchers to leverage LLMs' advantages responsibly while addressing associated risks.},
	language = {en},
	author = {Ke, Luoma and Tong, Song and Cheng, Peng and Peng, Kaiping},
}

@inproceedings{manakul_selfcheckgpt_2023,
	address = {Singapore},
	title = {{SelfCheckGPT}: {Zero}-{Resource} {Black}-{Box} {Hallucination} {Detection} for {Generative} {Large} {Language} {Models}},
	shorttitle = {{SelfCheckGPT}},
	url = {https://aclanthology.org/2023.emnlp-main.557},
	doi = {10.18653/v1/2023.emnlp-main.557},
	abstract = {Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose “SelfCheckGPT”, a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that our approach has considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods.},
	urldate = {2024-11-12},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Manakul, Potsawee and Liusie, Adian and Gales, Mark},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {9004--9017},
}

@inproceedings{fu_gptscore_2024,
	address = {Mexico City, Mexico},
	title = {{GPTScore}: {Evaluate} as {You} {Desire}},
	shorttitle = {{GPTScore}},
	url = {https://aclanthology.org/2024.naacl-long.365},
	doi = {10.18653/v1/2024.naacl-long.365},
	abstract = {Generative Artificial Intelligence (AI) has enabled the development of sophisticated models that are capable of producing high-caliber text, images, and other outputs through the utilization of large pre-trained models.Nevertheless, assessing the quality of the generation is an even more arduous task than the generation itself, and this issue has not been given adequate consideration recently.This paper proposes a novel evaluation framework, GPTScore, which utilizes the emergent abilities (e.g., in-context learning, zero-shot instruction) of generative pre-trained models to score generated texts. There are 19 pre-trained models explored in this paper, ranging in size from 80M (e.g., Flan-T5-small) to 175B (e.g., GPT3).Experimental results on four text generation tasks, 22 evaluation aspects, and corresponding 37 datasets demonstrate that this approach can effectively allow us to achieve what one desires to evaluate for texts simply by natural language instructions.This nature helps us overcome several long-standing challenges in text evaluation–how to achieve customized, multi-faceted evaluation without model training. We make our code publicly available.},
	urldate = {2024-11-12},
	booktitle = {Proceedings of the 2024 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Fu, Jinlan and Ng, See-Kiong and Jiang, Zhengbao and Liu, Pengfei},
	editor = {Duh, Kevin and Gomez, Helena and Bethard, Steven},
	month = jun,
	year = {2024},
	pages = {6556--6576},
}

@misc{cui_theoretical_2024,
	title = {A {Theoretical} {Understanding} of {Chain}-of-{Thought}: {Coherent} {Reasoning} and {Error}-{Aware} {Demonstration}},
	shorttitle = {A {Theoretical} {Understanding} of {Chain}-of-{Thought}},
	url = {http://arxiv.org/abs/2410.16540},
	doi = {10.48550/arXiv.2410.16540},
	abstract = {Few-shot Chain-of-Thought (CoT) prompting has demonstrated strong performance in improving the reasoning capabilities of large language models (LLMs). While theoretical investigations have been conducted to understand CoT, the underlying transformer used in these studies isolates the CoT reasoning process into separated in-context learning steps (Stepwise ICL). In this work, we theoretically show that, compared to Stepwise ICL, the transformer gains better error correction ability and more accurate predictions if the reasoning from earlier steps (Coherent CoT) is integrated. Given that this coherent reasoning changes the behavior of the transformer, we further investigate the sensitivity of the transformer with Coherent CoT when the demonstration examples are corrupted at the inference stage. Our theoretical results indicate that the transformer is more sensitive to errors in intermediate reasoning steps than the final outcome. Building upon this observation, we propose an improvement on CoT by incorporating both correct and incorrect reasoning paths in the demonstration. Our experiments validate the effectiveness of the proposed approach.},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Cui, Yingqian and He, Pengfei and Tang, Xianfeng and He, Qi and Luo, Chen and Tang, Jiliang and Xing, Yue},
	month = oct,
	year = {2024},
	note = {arXiv:2410.16540},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@book{picard_affective_2000,
	title = {Affective computing},
	publisher = {MIT press},
	author = {Picard, Rosalind W},
	year = {2000},
}

@misc{picard_affective_nodate-1,
	title = {Affective {Computing} -  {Picard} - {Google} {Books}},
	url = {https://books.google.at/books?hl=en&lr=&id=GaVncRTcb1gC&oi=fnd&pg=PR9&dq=Affective+computing.+MIT+press.+(A+foundational+text+in+affective+computing,+emphasizing+the+importance+of+context&ots=F6k4wnuccd&sig=Q-9yw3iCw2aAhkBR_iEPVc9j-Ks&redir_esc=y#v=onepage&q&f=false},
	urldate = {2024-11-06},
	author = {Picard, Rosalind W.},
}

@misc{noauthor_chi_nodate,
	title = {{CHI} '24 {Home}},
	url = {https://programs.sigchi.org/chi/2024},
	abstract = {Conference homepage: check conference details and the list of sponsors for the conference supported by SIGCHI and ACM.},
	language = {en},
	urldate = {2024-11-06},
}

@misc{noauthor_conference_nodate,
	title = {Conference {Programs}},
	url = {https://programs.sigchi.org/},
	abstract = {SIGCHI PWA - is a place where you get a conference schedule for the events supported by Computer-Human Interaction organization and ACM. See the list of conferences and join our worldwide community!},
	language = {en},
	urldate = {2024-11-06},
}

@book{griffiths_what_2008,
	title = {What {Emotions} {Really} {Are}: {The} {Problem} of {Psychological} {Categories}},
	isbn = {978-0-226-30876-0},
	shorttitle = {What {Emotions} {Really} {Are}},
	abstract = {In this provocative contribution to the philosophy of science and mind, Paul E. Griffiths criticizes contemporary philosophy and psychology of emotion for failing to take in an evolutionary perspective and address current work in neurobiology and cognitive science. Reviewing the three current models of emotion, Griffiths points out their deficiencies and constructs a basis for future models that pay equal attention to biological fact and conceptual rigor.  "Griffiths has written a work of depth and clarity in an area of murky ambiguity, producing a much-needed standard at the border of science, philosophy, and psychology. . . . As he presents his case, offering a forthright critique of past and present theories, Griffiths touches on such issues as evolution, social construction, natural kinds (categories corresponding with real distinctions in nature), cognition, and moods. While addressing specialists, the book will reward general readers who apply themselves to its remarkably accessible style."—Library Journal  "What Emotions Really Are makes a strong claim to be one of the best books to have emerged on the subject of human emotion."—Ray Dolan, Nature},
	language = {en},
	publisher = {University of Chicago Press},
	author = {Griffiths, Paul E.},
	month = apr,
	year = {2008},
	note = {Google-Books-ID: t3BgX4KRVksC},
	keywords = {Psychology / Emotions, Psychology / General},
}

@book{fernandez-dols_science_2017,
	title = {The {Science} of {Facial} {Expression}},
	isbn = {978-0-19-061351-8},
	abstract = {The importance of facial expressions has led to a steadily growing body of empirical findings and theoretical analyses. Every decade has seen work that extends or challenges previous thinking on facial expression. The Science of Facial Expression provides an updated review of the current psychology of facial expression . This book summarizes current conclusions and conceptual frameworks from leading figures who have shaped the field in their various subfields, and will therefore be of interest to practitioners, students, and researchers of emotion in cognitive psychology, neuroscience, biology, anthropology, linguistics, affective computing, and homeland security. Organized in eleven thematic sections, The Science of Facial Expression offers a broad perspective of the "geography" of the science of facial expression. It reviews the scientific history of emotion perception and the evolutionary origins and functions of facial expression. It includes an updated compilation on the great debate around Basic Emotion Theory versus Behavioral Ecology and Psychological constructionism. The developmental psychology and social psychology of facial expressions is explored in the role of facial expressions in child development, social interactions, and culture. The book also covers appraisal theory, concepts, neural and behavioral processes, and lesser-known facial behaviors such as yawing, vocal crying, and vomiting. In addition, the book reflects that research on the "expression of emotion" is moving towards a significance of context in the production and interpretation of facial expression The authors expose various fundamental questions and controversies yet to be resolved, but in doing so, open many sources of inspiration to pursue in the scientific study of facial expression.},
	language = {en},
	publisher = {Oxford University Press},
	author = {Fernández-Dols, José-Miguel and Russell, James A.},
	month = apr,
	year = {2017},
	note = {Google-Books-ID: eaKpDgAAQBAJ},
	keywords = {Psychology / Cognitive Psychology \& Cognition, Psychology / General},
}

@book{fridlund_human_2014,
	title = {Human {Facial} {Expression}: {An} {Evolutionary} {View}},
	isbn = {978-1-4832-8851-2},
	shorttitle = {Human {Facial} {Expression}},
	abstract = {Approx.369 pagesApprox.369 pages},
	language = {en},
	publisher = {Academic Press},
	author = {Fridlund, Alan J.},
	month = mar,
	year = {2014},
	note = {Google-Books-ID: TtrWAgAAQBAJ},
	keywords = {Psychology / Cognitive Psychology \& Cognition, Psychology / Neuropsychology},
}

@misc{noauthor_chi_nodate-1,
	title = {{CHI} '24 {Search}},
	url = {https://programs.sigchi.org/chi/2024/search/content?searchKey=emotion},
	abstract = {See the results of contents, sessions, authors and notes matching the search query},
	language = {en},
	urldate = {2024-11-06},
}

@misc{liu_understanding_2024,
	title = {Understanding {LLMs}: {A} {Comprehensive} {Overview} from {Training} to {Inference}},
	shorttitle = {Understanding {LLMs}},
	url = {http://arxiv.org/abs/2401.02038},
	doi = {10.48550/arXiv.2401.02038},
	abstract = {The introduction of ChatGPT has led to a significant increase in the utilization of Large Language Models (LLMs) for addressing downstream tasks. There's an increasing focus on cost-efficient training and deployment within this context. Low-cost training and deployment of LLMs represent the future development trend. This paper reviews the evolution of large language model training techniques and inference deployment technologies aligned with this emerging trend. The discussion on training includes various aspects, including data preprocessing, training architecture, pre-training tasks, parallel training, and relevant content related to model fine-tuning. On the inference side, the paper covers topics such as model compression, parallel computation, memory scheduling, and structural optimization. It also explores LLMs' utilization and provides insights into their future development.},
	urldate = {2024-10-31},
	publisher = {arXiv},
	author = {Liu, Yiheng and He, Hao and Han, Tianle and Zhang, Xu and Liu, Mengyuan and Tian, Jiaming and Zhang, Yutong and Wang, Jiaqi and Gao, Xiaohui and Zhong, Tianyang and Pan, Yi and Xu, Shaochen and Wu, Zihao and Liu, Zhengliang and Zhang, Xin and Zhang, Shu and Hu, Xintao and Zhang, Tuo and Qiang, Ning and Liu, Tianming and Ge, Bao},
	month = jan,
	year = {2024},
	note = {arXiv:2401.02038},
	keywords = {Computer Science - Computation and Language},
}

@misc{suzgun_meta-prompting_2024,
	title = {Meta-{Prompting}: {Enhancing} {Language} {Models} with {Task}-{Agnostic} {Scaffolding}},
	shorttitle = {Meta-{Prompting}},
	url = {http://arxiv.org/abs/2401.12954},
	abstract = {We introduce meta-prompting, an effective scaffolding technique designed to enhance the functionality of language models (LMs). This approach transforms a single LM into a multi-faceted conductor, adept at managing and integrating multiple independent LM queries. By employing high-level instructions, meta-prompting guides the LM to break down complex tasks into smaller, more manageable subtasks. These subtasks are then handled by distinct "expert" instances of the same LM, each operating under specific, tailored instructions. Central to this process is the LM itself, in its role as the conductor, which ensures seamless communication and effective integration of the outputs from these expert models. It additionally employs its inherent critical thinking and robust verification processes to refine and authenticate the end result. This collaborative prompting approach empowers a single LM to simultaneously act as a comprehensive orchestrator and a panel of diverse experts, significantly enhancing its performance across a wide array of tasks. The zero-shot, task-agnostic nature of meta-prompting greatly simplifies user interaction by obviating the need for detailed, task-specific instructions. Furthermore, our research demonstrates the seamless integration of external tools, such as a Python interpreter, into the meta-prompting framework, thereby broadening its applicability and utility. Through rigorous experimentation with GPT-4, we establish the superiority of meta-prompting over conventional scaffolding methods: When averaged across all tasks, including the Game of 24, Checkmate-in-One, and Python Programming Puzzles, meta-prompting, augmented with a Python interpreter functionality, surpasses standard prompting by 17.1\%, expert (dynamic) prompting by 17.3\%, and multipersona prompting by 15.2\%.},
	language = {en},
	urldate = {2024-10-21},
	publisher = {arXiv},
	author = {Suzgun, Mirac and Kalai, Adam Tauman},
	month = jan,
	year = {2024},
	note = {arXiv:2401.12954 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
}

@misc{noauthor_improving_nodate,
	title = {Improving {LLM}’s {Reasoning} {In} {Production} - {The} {Structured} {Approach} {Artificial} {Intelligence} and {MLOps} {Consulting} company focused on increasing revenue.},
	url = {https://www.mlopsaudits.com/blog/improving-llms-reasoning-in-production-the-structured-approach},
	abstract = {This guide on achieving better reasoning performance of LLMs intends to complement theguide on prompt engineering by programmatic and systematic to increase flexibility while keeping the amount of operational variability to a minimum.},
	urldate = {2024-10-15},
}

@misc{yao_tree_2023,
	title = {Tree of {Thoughts}: {Deliberate} {Problem} {Solving} with {Large} {Language} {Models}},
	shorttitle = {Tree of {Thoughts}},
	url = {http://arxiv.org/abs/2305.10601},
	abstract = {Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, “Tree of Thoughts” (ToT), which generalizes over the popular “Chain of Thought” approach to prompting language models, and enables exploration over coherent units of text (“thoughts”) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models’ problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4\% of tasks, our method achieved a success rate of 74\%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.},
	language = {en},
	urldate = {2024-10-09},
	publisher = {arXiv},
	author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L. and Cao, Yuan and Narasimhan, Karthik},
	month = dec,
	year = {2023},
	note = {arXiv:2305.10601 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{wang_survey_2024,
	title = {A survey on large language model based autonomous agents},
	volume = {18},
	issn = {2095-2228, 2095-2236},
	url = {https://link.springer.com/10.1007/s11704-024-40231-1},
	doi = {10.1007/s11704-024-40231-1},
	abstract = {Abstract
            Autonomous agents have long been a research focus in academic and industry communities. Previous research often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of Web knowledge, large language models (LLMs) have shown potential in human-level intelligence, leading to a surge in research on LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of LLM-based autonomous agents from a holistic perspective. We first discuss the construction of LLM-based autonomous agents, proposing a unified framework that encompasses much of previous work. Then, we present a overview of the diverse applications of LLM-based autonomous agents in social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field.},
	language = {en},
	number = {6},
	urldate = {2024-09-27},
	journal = {Frontiers of Computer Science},
	author = {Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and Zhao, Wayne Xin and Wei, Zhewei and Wen, Jirong},
	month = dec,
	year = {2024},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	pages = {186345},
}

@article{wang_public_2024,
	title = {Public {Behavior} and {Emotion} {Correlation} {Mining} {Driven} by {Aspect} {From} {News} {Corpus}},
	volume = {PP},
	issn = {2162-2388},
	doi = {10.1109/TNNLS.2024.3441011},
	abstract = {Emotion motivates behavior. Investigating the correlation between behavior and emotion, an often overlooked perspective, plays a significant role in uncovering the underlying motives behind behaviors and the intrinsic cause-effects of social events. This article proposes a methodology for mining the correlation between public behavior and emotion using daily news data. Initially, aspect-emotion-reaction (A-E-R) triplets are extracted and generalized, encompassing both explicit and implicit patterns. Then, a knowledge representation model based on hypothetical context (KRHC) with a self-reflection mechanism is proposed to uncover implicit relationships between emotion and behavior through attention mechanisms. By combining rule-based methods for explicit relationships and deep learning for implicit ones, an understanding of emotion-behavior patterns is achieved. In this study, the behaviors are divided into three categories of prosocial, antisocial, and normal behaviors with ten secondary types. Seven categories of emotions are adopted. The proposed deep learning model KRHC is validated on A-E-R datasets and public KINSHIP datasets. The experiment results are concluded; for example, when "fear", "sad", and "surprise" emotions appear, it drives behavior "panic" with most probability. These findings could provide insights for both human-computer interaction and public safety management applications.},
	language = {eng},
	journal = {IEEE transactions on neural networks and learning systems},
	author = {Wang, Xinzhi and Chang, Yudong and Kou, Luyao and Luo, Xiangfeng and Zhang, Hui},
	month = aug,
	year = {2024},
	pmid = {39178079},
	keywords = {Behavior reasoning, Cognition, Computer science, Correlation, Emotion recognition, Feature extraction, Organisms, Psychology, emotion attribution, social event management, text mining},
}

@article{dobber_social_2024,
	title = {The {Social} {Media} {Comment} {Section} as an {Unruly} {Public} {Arena}: {How} {Comment} {Reading} {Erodes} {Trust} in {News} {Media}},
	issn = {1931-2431, 1931-244X},
	shorttitle = {The {Social} {Media} {Comment} {Section} as an {Unruly} {Public} {Arena}},
	url = {https://journals.sagepub.com/doi/10.1177/19312431241268011},
	doi = {10.1177/19312431241268011},
	abstract = {The comment section accompanying news stories on social media offers an important interactive context for news, but may also afford the possibility to spread anti-establishment, trust-eroding comments. Exposure to such comments may affect social media users’ trust in news media. However, evidence of over-time effects is scarce. This study draws upon cultivation theory and uses a three-wave panel survey in the Netherlands ( N = 906). Findings indicate that increased comment reading at T2 exacerbates and deepens news media mistrust at T3, suggesting that exposure to the comment section accompanying news stories posted on social media indeed has negative ramifications for news media trust. News media seem to be closing off their own comment section and delegating commenting to social media in part to create a buffer between themselves and trust-eroding comments. Our findings suggest this buffer is not as solid as news media might hope.},
	language = {en},
	urldate = {2024-09-25},
	journal = {Electronic News},
	author = {Dobber, Tom and Hameleers, Michael},
	month = aug,
	year = {2024},
	pages = {19312431241268011},
}

@misc{noauthor_digital_nodate,
	title = {Digital {News} {Report} 2024 {\textbar} {Reuters} {Institute} for the {Study} of {Journalism}},
	url = {https://reutersinstitute.politics.ox.ac.uk/digital-news-report/2024},
	language = {en},
	urldate = {2024-09-25},
}

@article{moreo_lexicon-based_2012,
	title = {Lexicon-based {Comments}-oriented {News} {Sentiment} {Analyzer} system},
	volume = {39},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417412003016},
	doi = {10.1016/j.eswa.2012.02.057},
	abstract = {Thanks to the technological revolution that has accompanied the Web 2.0, users are able to interact intensively on the Internet, as reflected in social networks, blogs, forums, etc. In these scenarios, users can speak freely on any relevant topic. However, the high volume of user-generated content makes a manual analysis of this discourse unviable. Consequently, automatic analysis techniques are needed to extract the opinions expressed in users’ comments, given that these opinions are an implicit barometer of unquestionable interest for a wide variety of companies, agencies, and organisms. We thus propose a lexicon-based Comments-oriented News Sentiment Analyzer (LCN-SA), which is able to deal with the following: (a) the tendency of many users to express their views in non-standard language; (b) the detection of the target of users’ opinions in a multi-domain scenario; (c) the design of a linguistic modularized knowledge model with low-cost adaptability. The system proposed consists of an automatic Focus Detection Module and a Sentiment Analysis Module capable of assessing user opinions of topics in news items. These modules use a taxonomy-lexicon specifically designed for news analysis. Experiments show that the results obtained thus far are extremely promising.},
	number = {10},
	urldate = {2024-09-27},
	journal = {Expert Systems with Applications},
	author = {Moreo, A. and Romero, M. and Castro, J. L. and Zurita, J. M.},
	month = aug,
	year = {2012},
	keywords = {Feature mining, Focus detection, Lexicon-based, News analysis, Sentiment Analysis},
	pages = {9166--9180},
}

@inproceedings{wang_unified_2016,
	title = {A {Unified} {Framework} for {Fine}-{Grained} {Opinion} {Mining} from {Online} {Reviews}},
	url = {https://ieeexplore.ieee.org/document/7427323/?arnumber=7427323},
	doi = {10.1109/HICSS.2016.144},
	abstract = {Extracting opinion words and opinion targets from online reviews is an important task for fine-grained opinion mining. Usually, traditional extraction methods under the pipeline-based framework have higher precision but lower recall, while methods in the propagation-based framework possess greater recall but poorer precision. To achieve better performance both in precision and recall, this paper proposes a unified framework for fine-grained opinion mining, combining propagation with refinement in a dynamic and iterative process. In the propagation process, syntactic patterns are chosen as opinion relations to extract new opinion words and targets. Besides, syntactic patterns are further generalized to make them more flexible and scalable. In the refinement process, a three-layer opinion relations graph (ORG) model is constructed based on three types of candidates: opinion word candidates, opinion target candidates and syntactic pattern candidates. A sorting algorithm based on ORG model is proposed to rank all the candidates in their own type, and low-rank candidates are removed from candidate datasets. Repeat propagation and refinement until the syntactic pattern candidate set reaches stable. Experimental results on both English and Chinese online reviews demonstrate the effectiveness of proposed framework and its methods, comparing with the-state-of-the-art methods.},
	urldate = {2024-09-27},
	booktitle = {2016 49th {Hawaii} {International} {Conference} on {System} {Sciences} ({HICSS})},
	author = {Wang, Hao and Zhang, Chen and Yin, Hongzhi and Wang, Wei and Zhang, Jun and Xu, Fanjiang},
	month = jan,
	year = {2016},
	note = {ISSN: 1530-1605},
	keywords = {Data mining, Feature extraction, Heuristic algorithms, Hidden Markov models, Pattern matching, Pipelines, Syntactics, opinion mining, opinion relations graph, sentiment analysis, syntactic pattern},
	pages = {1134--1143},
}

@article{abas_deep_2020,
	title = {Deep {Learning} {Model} for {Fine}-{Grained} {Aspect}-{Based} {Opinion} {Mining}},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9139221/?arnumber=9139221},
	doi = {10.1109/ACCESS.2020.3008824},
	abstract = {Despite the great manufactures' efforts to achieve customer satisfaction and improve their performance, social media opinion mining is still on the fly a big challenge. Current opinion mining requires sophisticated feature engineering and syntactic word embedding without considering semantic interaction between aspect term and opinionated features, which degrade the performance of most of opinion mining tasks, especially those that are designed for smart manufacturing. Research on intelligent aspect level opinion mining (AOM) follows the fast proliferation of user-generated data through social media for industrial manufacturing purposes. Google's pre-trained language model, Bidirectional Encoder Representations from Transformers (BERT) widely overcomes existing methods in eleven natural language processing (NLP) tasks, which makes it the standard way for semantic text representation. In this paper, we introduce a novel deep learning model for fine-grained aspect-based opinion mining, named as FGAOM. First, we train the BERT model on three specific domain corpora for domain adaption, then use adjusted BERT as embedding layer for concurrent extraction of local and global context features. Then, we propose Multi-head Self-Attention (MSHA) to effectively fuse internal semantic text representation and take advantage of convolutional layers to model aspect term interaction with surrounding sentiment features. Finally, the performance of the proposed model is evaluated via extensive experiments on three public datasets. Results show that performance of the proposed model outperforms performances of recent the-of-the-art models.},
	urldate = {2024-09-27},
	journal = {IEEE Access},
	author = {Abas, Ahmed R. and El-Henawy, Ibrahim and Mohamed, Hossam and Abdellatif, Amr},
	year = {2020},
	note = {Conference Name: IEEE Access},
	keywords = {Adaptation models, Bit error rate, Context modeling, Deep learning, Feature extraction, Semantics, Sentiment analysis, opinion mining, sentiment analysis, social media analytics},
	pages = {128845--128855},
}

@misc{munkhdalai_leave_2024,
	title = {Leave {No} {Context} {Behind}: {Efficient} {Infinite} {Context} {Transformers} with {Infini}-attention},
	shorttitle = {Leave {No} {Context} {Behind}},
	url = {http://arxiv.org/abs/2404.07143},
	abstract = {This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.},
	language = {en},
	urldate = {2024-10-09},
	publisher = {arXiv},
	author = {Munkhdalai, Tsendsuren and Faruqui, Manaal and Gopal, Siddharth},
	month = aug,
	year = {2024},
	note = {arXiv:2404.07143 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@incollection{plutchik_chapter_1980,
	title = {Chapter 1 - {A} {GENERAL} {PSYCHOEVOLUTIONARY} {THEORY} {OF} {EMOTION}},
	isbn = {978-0-12-558701-3},
	url = {https://www.sciencedirect.com/science/article/pii/B9780125587013500077},
	abstract = {The general psychoevolutionary theory of emotion that is presented here has a number of important characteristics. First, it provides a broad evolutionary foundation for conceptualizing the domain of emotion as seen in animals and humans. Second, it provides a structural model which describes the interrelations among emotions. Third, it has demonstrated both theoretical and empirical relations among a number of derivative domains including personality traits, diagnoses, and ego defenses. Fourth, it has provided a theoretical rationale for the construction of tests and scales for the measurement of key dimensions within these various domains. Fifth, it has stimulated a good deal of empirical research using these tools and concepts. Finally, the theory provides useful insights into the relationships among emotions, adaptations, and evolution.},
	urldate = {2024-10-09},
	booktitle = {Theories of {Emotion}},
	publisher = {Academic Press},
	author = {Plutchik, ROBERT},
	editor = {Plutchik, Robert and Kellerman, Henry},
	month = jan,
	year = {1980},
	doi = {10.1016/B978-0-12-558701-3.50007-7},
	pages = {3--33},
}

@article{barrett_solving_2006,
	title = {Solving the {Emotion} {Paradox}: {Categorization} and the {Experience} of {Emotion}},
	volume = {10},
	copyright = {https://journals.sagepub.com/page/policies/text-and-data-mining-license},
	issn = {1088-8683, 1532-7957},
	shorttitle = {Solving the {Emotion} {Paradox}},
	url = {https://journals.sagepub.com/doi/10.1207/s15327957pspr1001_2},
	doi = {10.1207/s15327957pspr1001_2},
	abstract = {In this article, I introduce an emotion paradox: People believe that they know an emotion when they see it, and as a consequence assume that emotions are discrete events that can be recognized with some degree of accuracy, but scientists have yet to produce a set of clear and consistent criteria for indicating when an emotion is present and when it is not. I propose one solution to this paradox: People experience an emotion when they conceptualize an instance of affective feeling. In this view, the experience of emotion is an act of categorization, guided by embodied knowledge about emotion. The result is a model of emotion experience that has much in common with the social psychological literature on person perception and with literature on embodied conceptual knowledge as it has recently been applied to social psychology.},
	language = {en},
	number = {1},
	urldate = {2024-10-02},
	journal = {Personality and Social Psychology Review},
	author = {Barrett, Lisa Feldman},
	month = feb,
	year = {2006},
	pages = {20--46},
}

@misc{zhang_meta_2024,
	title = {Meta {Prompting} for {AI} {Systems}},
	url = {http://arxiv.org/abs/2311.11482},
	doi = {10.48550/arXiv.2311.11482},
	abstract = {In this work, we present a comprehensive study of Meta Prompting (MP), an innovative technique reshaping the utilization of language models (LMs) and AI systems in problem-solving and data interaction. Grounded in type theory and category theory, Meta Prompting emphasizes the structure and syntax of information over traditional content-centric methods. The paper explores the formal definitions of Meta Prompting, sets it apart from few-shot prompting, and underlines its effectiveness in various AI applications. A key focus is applying Meta Prompting for complex reasoning tasks, showing how it effectively deconstructs intricate problems into simpler sub-problems, enhancing token efficiency, and enabling more equitable problem-solving comparisons, especially against few-shot prompting methods. Additionally, the paper introduces Meta Prompting for prompting tasks, allowing LLMs to self-generate new prompts in a recursive, metaprogramming-like manner. Empirical experiments, including using a Qwen-72B base language model equipped with meta prompt without instruction-tuning to solve MATH problems with accuracy at 46.3\%, which surpass the supervised fine-tuned counterpart trained with extensive mathematical QA instruction pairs and even the initial version of GPT-4, solving GSM8K problems with 83.5\% accuracy with zero-shot meta-prompted Qwen-72B base language model, and solving the Game of 24 tasks with a 100\% success rate using GPT-4, demonstrate the meta prompting's efficacy in achieving high accuracy and efficiency, showcasing Meta Prompting's transformative impact on AI problem-solving The code is available at https://github.com/meta-prompting/meta-prompting.},
	urldate = {2024-10-02},
	publisher = {arXiv},
	author = {Zhang, Yifan and Yuan, Yang and Yao, Andrew Chi-Chih},
	month = jun,
	year = {2024},
	note = {arXiv:2311.11482 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{greg_kamradt_gregkamradt_how_2024,
	type = {Tweet},
	title = {How structured outputs work under the hood (via breakout at {OpenAI} {DevDay}) {Guess} why the first structured output request is slow, but the 2nd+ is fast? {Engineering}: * {Unconstrained} token decoding isn't good. {The} model could pick any token. * {Limiting} which tokens can be https://t.co/{pMhuzYwaLr}},
	url = {https://x.com/GregKamradt/status/1841187546912735248},
	language = {en},
	urldate = {2024-10-02},
	journal = {Twitter},
	author = {{Greg Kamradt [@GregKamradt]}},
	month = oct,
	year = {2024},
}

@misc{noauthor_pdf_nodate,
	title = {[{PDF}] {A} {Survey} on {Large} {Language} {Model} based {Autonomous} {Agents} {\textbar} {Semantic} {Scholar}},
	url = {https://www.semanticscholar.org/reader/28c6ac721f54544162865f41c5692e70d61bccab},
	urldate = {2024-09-27},
}

@inproceedings{liu_fine-grained_2015,
	address = {Lisbon, Portugal},
	title = {Fine-grained {Opinion} {Mining} with {Recurrent} {Neural} {Networks} and {Word} {Embeddings}},
	url = {https://aclanthology.org/D15-1168},
	doi = {10.18653/v1/D15-1168},
	urldate = {2024-09-27},
	booktitle = {Proceedings of the 2015 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Pengfei and Joty, Shafiq and Meng, Helen},
	editor = {Màrquez, Lluís and Callison-Burch, Chris and Su, Jian},
	month = sep,
	year = {2015},
	pages = {1433--1443},
}

@inproceedings{mukwazvure_hybrid_2015,
	title = {A hybrid approach to sentiment analysis of news comments},
	url = {https://ieeexplore.ieee.org/document/7359282/?arnumber=7359282},
	doi = {10.1109/ICRITO.2015.7359282},
	abstract = {Today, the web hosts quite a voluminous amount of information. Among such information is user generated content which plays an important role in analyzing different business aspects. Sentiment analysis therefore becomes an effective way of understanding public opinions. Businesses, particularly in ecommerce, stock market, social networks and also political entities can use sentiment analysis for decision making. Traditional methods of opinion gathering involved the use of questioners and interviews which solely depend on the good will of the people to be interviewed. Most research on sentiment analysis focused on social networks, product reviews and also on the stock market. Less research has been covered on analysis of news comments. This research embarks on a hybrid approach to sentiment analysis of news comments which involves using sentiment lexicon for polarity detection (polarity will be classified as positive, negative and neutral). The results from the lexicon based method are then used to train machine learning algorithms. Two algorithms employed in this research are the Support Vector Machine (SVM) and K-Nearest Neighbour (kNN). Experimental results show that SVM performs better than kNN on news comments.},
	urldate = {2024-09-27},
	booktitle = {2015 4th {International} {Conference} on {Reliability}, {Infocom} {Technologies} and {Optimization} ({ICRITO}) ({Trends} and {Future} {Directions})},
	author = {Mukwazvure, Addlight and Supreethi, K.P},
	month = sep,
	year = {2015},
	keywords = {Classification algorithms, Dictionaries, Machine learning algorithms, Market research, SVM, Sentiment analysis, Support vector machines, User generated content, kNN, polarity, sentiment analysis, sentiment lexicon},
	pages = {1--6},
}

@article{chen_ai_2010,
	title = {{AI} and {Opinion} {Mining}},
	volume = {25},
	issn = {1941-1294},
	url = {https://ieeexplore.ieee.org/document/5475086/?arnumber=5475086},
	doi = {10.1109/MIS.2010.75},
	abstract = {The advent of Web 2.0 and social media content has stirred much excitement and created abundant opportunities for understanding the opinions of the general public and consumers toward social events, political movements, company strategies, marketing campaigns, and product preferences. Many new and exciting social, geopolitical, and business-related research questions can be answered by analyzing the thousands, even millions, of comments and responses expressed in various blogs (such as the blogosphere), forums (such as Yahoo Forums), social media and social network sites (including YouTube, Facebook, and Flikr), virtual worlds (such as Second Life), and tweets (Twitter). Opinion mining, a subdiscipline within data mining and computational linguistics, refers to the computational techniques for extracting, classifying, understanding, and assessing the opinions expressed in various online news sources, social media comments, and other user-generated content. Sentiment analysis is often used in opinion mining to identify sentiment, affect, subjectivity, and other emotional states in online text.},
	number = {3},
	urldate = {2024-09-27},
	journal = {IEEE Intelligent Systems},
	author = {Chen, Hsinchun and Zimbra, David},
	month = may,
	year = {2010},
	note = {Conference Name: IEEE Intelligent Systems},
	keywords = {Artificial intelligence, Blogs, Companies, Computational linguistics, Data mining, Facebook, Second Life, Social network services, Twitter, Wal-Mart, Web 2.0, YouTube, intelligent systems, opinion mining, sentiment analysis},
	pages = {74--80},
}

@article{uszak_aspirations_2018,
	title = {Aspirations and {Motivations} for {Facebook} {Use} {Through} {Sem} {Modeling}},
	volume = {62},
	issn = {2169-5067, 1071-1813},
	url = {http://journals.sagepub.com/doi/10.1177/1541931218621281},
	doi = {10.1177/1541931218621281},
	abstract = {Facebook is one of the most pervasive social networking sites in the world. Growing out of a humble Harvard cataloging project, Facebook has over 2 billion monthly active users (Facebook Newsroom, 2017). This paper will explore the motivations for such widespread use of this technology. The study of Facebook user motivations is not a new endeavor (E.g. Quan-Haase \& Young, 2010; Knowles, Haycock, and Shaikh, 2015; Kross et al.,2013). Generally, there seems to be a consensus that Facebook is able to provide a social outlet and platform of potential connection for users. However, these explicit self-reported motives beg the question of what are the underlying implicit needs or desires driving the action of joining Facebook? What do people actually get out of maintaining a digital presence there?
            Uses and Gratifications Theory (U\&G) considers how people benefit from mass communication (Katz, Blumer, \& Gurevitch (1974). Contrary to early theories where users were just passive consumers of whatever a particular media put in front of them, Quan-Haase \& Young, 2010 found six factors of Facebook use gratifications; these factors were pastime, affection, fashion, share problems, sociability, and social information. Self-Determination Theory (SD-Theory) is a meta-framework for defining and classifying different types of motivations in relation to individual differences and social context (Deci \& Ryan,1985, 2000). SD-Theory postulates that evolutionarily, humans have three basic psychological needs. According to this view, we have the need for competency, autonomy, and relatedness. Besides psychological needs, another measured component of SD-Theory is life goals, also known as aspirations (Kasser \& Ryan, 1993;1996).
            Aspirations are life goals one has, based on the values one holds; and are normally divided into two categories. Intrinsic aspirations (IA) are such things as meaningful relationships, individual growth, and community contributions. Extrinsic aspirations (EA) encompass wealth, fame, and image; they are beholden to the judgements of other people and tend to be more of a hoop to jump through rather than an end goal (Kasser \& Ryan, 1996).
            The Two-Process Model (TPM) ties Self-Determination Theory together with personal goal and motive disposition constructs (Sheldon \& Schüler, 2011). According to TPM all humans have basic needs, but there will be individual differences in motives. A weak experience of competency, autonomy, or/and relatedness (i.e., strong need requirements) was found to provoke motivation to improve the situation (Sheldon and Gunz, 2009). Sheldon, Abad, and Hinsch (2011) built upon these findings by measuring connection and disconnection in Facebook use. The purpose for this study was to investigate whether different types of aspirations (intrinsic or extrinsic) predicted connection and disconnection, and whether Facebook motives mediate this relationship.
            Participants were 424 college students from a large university in central Florida. Participant gender is 255 female and 167 male, with two not indicating a gender. Participants age range was from 18 to 59 (M=20.8, SD=4.9) years. Three surveys were administered through an online Qualtrics system.
            Surveys measured aspirations, social connection, and Facebook use. Structural equation modeling analysis was then conducted to produce several models.
            Direct effects were found between EA and connectedness, as well as IA and connectedness. IA and EA were both shown to predict Facebook motives. However, Facebook motives were found not to mediate the relationship between aspirations and connectedness.},
	language = {en},
	number = {1},
	urldate = {2024-09-25},
	journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
	author = {Uszak, Nicolas and Szalma, James},
	month = sep,
	year = {2018},
	pages = {1226--1227},
}

@misc{noauthor_public_nodate,
	title = {Public {Behavior} and {Emotion} {Correlation} {Mining} {Driven} by {Aspect} {From} {News} {Corpus}. {\textbar} {Semantic} {Scholar}},
	url = {https://www.semanticscholar.org/paper/Public-Behavior-and-Emotion-Correlation-Mining-by-Wang-Chang/fa581fef6d6d4ea94864ff6ab16ccb59a66aabe4},
	urldate = {2024-09-25},
}

@article{mizgajski_affective_2019,
	title = {Affective recommender systems in online news industry: how emotions influence reading choices},
	volume = {29},
	issn = {0924-1868, 1573-1391},
	shorttitle = {Affective recommender systems in online news industry},
	url = {http://link.springer.com/10.1007/s11257-018-9213-x},
	doi = {10.1007/s11257-018-9213-x},
	abstract = {Recommender systems have become ubiquitous over the last decade, providing users with personalized search results, video streams, news excerpts, and purchasing hints. Human emotions are widely regarded as important predictors of behavior and preference. They are a crucial factor in decision making, but until recently, relatively little has been known about the effectiveness of using human emotions in personalizing real-world recommender systems. In this paper we introduce the Emotion Aware Recommender System (EARS), a large scale system for recommending news items using user’s self-assessed emotional reactions. Our original contribution includes the formulation of a multi-dimensional model of emotions for news item recommendations, introduction of affective item features that can be used to describe recommended items, construction of affective similarity measures, and validation of the EARS on a large corpus of real-world Web traffic. We collect over 13,000,000 page views from 2,700,000 unique users of two news sites and we gather over 160,000 emotional reactions to 85,000 news articles. We discover that incorporating pleasant emotions into collaborative filtering recommendations consistently outperforms all other algorithms. We also find that targeting recommendations by selected emotional reactions presents a promising direction for further research. As an additional contribution we share our experiences in designing and developing a real-world emotion-based recommendation engine, pointing to various challenges posed by the practical aspects of deploying emotion-based recommenders.},
	language = {en},
	number = {2},
	urldate = {2024-09-25},
	journal = {User Modeling and User-Adapted Interaction},
	author = {Mizgajski, Jan and Morzy, Mikołaj},
	month = apr,
	year = {2019},
	pages = {345--379},
}

@misc{noauthor_bfcl_nodate,
	title = {{BFCL} {V3} • {Multi}-{Turn} \& {Multi}-{Step} {Function} {Calling}},
	url = {https://gorilla.cs.berkeley.edu/blogs/13_bfcl_v3_multi_turn.html},
	urldate = {2024-09-25},
}

@misc{dziri_evaluating_2020,
	title = {Evaluating {Coherence} in {Dialogue} {Systems} using {Entailment}},
	url = {http://arxiv.org/abs/1904.03371},
	abstract = {Evaluating open-domain dialogue systems is difﬁcult due to the diversity of possible correct answers. Automatic metrics such as BLEU correlate weakly with human annotations, resulting in a signiﬁcant bias across different models and datasets. Some researchers resort to human judgment experimentation for assessing response quality, which is expensive, time consuming, and not scalable. Moreover, judges tend to evaluate a small number of dialogues, meaning that minor differences in evaluation conﬁguration may lead to dissimilar results. In this paper, we present interpretable metrics for evaluating topic coherence by making use of distributed sentence representations. Furthermore, we introduce calculable approximations of human judgment based on conversational coherence by adopting state-of-the-art entailment techniques. Results show that our metrics can be used as a surrogate for human judgment, making it easy to evaluate dialogue systems on large-scale datasets and allowing an unbiased estimate for the quality of the responses.},
	language = {en},
	urldate = {2024-09-20},
	publisher = {arXiv},
	author = {Dziri, Nouha and Kamalloo, Ehsan and Mathewson, Kory W. and Zaiane, Osmar},
	month = mar,
	year = {2020},
	note = {arXiv:1904.03371 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{sheng_repeval_2024,
	title = {{RepEval}: {Effective} {Text} {Evaluation} with {LLM} {Representation}},
	shorttitle = {{RepEval}},
	url = {http://arxiv.org/abs/2404.19563},
	doi = {10.48550/arXiv.2404.19563},
	abstract = {Automatic evaluation metrics for generated texts play an important role in the NLG field, especially with the rapid growth of LLMs. However, existing metrics are often limited to specific scenarios, making it challenging to meet the evaluation requirements of expanding LLM applications. Therefore, there is a demand for new, flexible, and effective metrics. In this study, we introduce RepEval, the first metric leveraging the projection of LLM representations for evaluation. RepEval requires minimal sample pairs for training, and through simple prompt modifications, it can easily transition to various tasks. Results on ten datasets from three tasks demonstrate the high effectiveness of our method, which exhibits stronger correlations with human judgments compared to previous metrics, even outperforming GPT-4. Our work underscores the richness of information regarding text quality embedded within LLM representations, offering insights for the development of new metrics.},
	urldate = {2024-09-20},
	publisher = {arXiv},
	author = {Sheng, Shuqian and Xu, Yi and Zhang, Tianhang and Shen, Zanwei and Fu, Luoyi and Ding, Jiaxin and Zhou, Lei and Wang, Xinbing and Zhou, Chenghu},
	month = apr,
	year = {2024},
	note = {arXiv:2404.19563 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{awasthi_humanely_2023,
	title = {{HumanELY}: {Human} evaluation of {LLM} yield, using a novel web-based evaluation tool},
	copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
	shorttitle = {{HumanELY}},
	url = {https://www.medrxiv.org/content/10.1101/2023.12.22.23300458v2},
	doi = {10.1101/2023.12.22.23300458},
	abstract = {Large language models (LLMs) have caught the imagination of researchers,developers and public in general the world over with their potential for transformation. Vast amounts of research and development resources are being provided to implement these models in all facets of life. Trained using billions of parameters, various measures of their accuracy and performance have been proposed and used in recent times. While many of the automated natural language assessment parameters measure LLM output performance for use of language, contextual outputs are still hard to measure and quantify. Hence, human evaluation is still an important measure of LLM performance,even though it has been applied variably and inconsistently due to lack of guidance and resource limitations.
To provide a structured way to perform comprehensive human evaluation of LLM output, we propose the first guidance and tool called HumanELY. Our approach and tool built using prior knowledge helps perform evaluation of LLM outputs in a comprehensive, consistent, measurable and comparable manner. HumanELY comprises of five key evaluation metrics: relevance, coverage, coherence, harm and comparison. Additional submetrics within these five key metrics provide for Likert scale based human evaluation of LLM outputs. Our related webtool uses this HumanELY guidance to enable LLM evaluation and provide data for comparison against different users performing human evaluation. While all metrics may not be relevant and pertinent to all outputs, it is important to assess and address their use.
Lastly, we demonstrate comparison of metrics used in HumanELY against some of the recent publications in the healthcare domain. We focused on the healthcare domain due to the need to demonstrate highest levels of accuracy and lowest levels of harm in a comprehensive manner. We anticipate our guidance and tool to be used for any domain where LLMs find an use case.
Link to the HumanELY Tool https://www.brainxai.com/humanely},
	language = {en},
	urldate = {2024-09-20},
	publisher = {medRxiv},
	author = {Awasthi, Raghav and Mishra, Shreya and Mahapatra, Dwarikanath and Khanna, Ashish and Maheshwari, Kamal and Cywinski, Jacek and Papay, Frank and Mathur, Piyush},
	month = dec,
	year = {2023},
	note = {Pages: 2023.12.22.23300458},
}

@misc{wang_self-consistency_2023,
	title = {Self-{Consistency} {Improves} {Chain} of {Thought} {Reasoning} in {Language} {Models}},
	url = {http://arxiv.org/abs/2203.11171},
	doi = {10.48550/arXiv.2203.11171},
	abstract = {Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9\%), SVAMP (+11.0\%), AQuA (+12.2\%), StrategyQA (+6.4\%) and ARC-challenge (+3.9\%).},
	urldate = {2024-09-20},
	publisher = {arXiv},
	author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
	month = mar,
	year = {2023},
	note = {arXiv:2203.11171 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{chang_survey_2024,
	title = {A {Survey} on {Evaluation} of {Large} {Language} {Models}},
	volume = {15},
	issn = {2157-6904},
	url = {https://dl.acm.org/doi/10.1145/3641289},
	doi = {10.1145/3641289},
	abstract = {Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, education, natural and social sciences, agent applications, and other areas. Secondly, we answer the ‘where’ and ‘how’ questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing the performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at:},
	number = {3},
	urldate = {2024-08-21},
	journal = {ACM Trans. Intell. Syst. Technol.},
	author = {Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and Ye, Wei and Zhang, Yue and Chang, Yi and Yu, Philip S. and Yang, Qiang and Xie, Xing},
	month = mar,
	year = {2024},
	keywords = {Evaluation},
	pages = {39:1--39:45},
}

@misc{hashimoto_unifying_2019,
	title = {Unifying {Human} and {Statistical} {Evaluation} for {Natural} {Language} {Generation}},
	url = {http://arxiv.org/abs/1904.02792},
	doi = {10.48550/arXiv.1904.02792},
	abstract = {How can we measure whether a natural language generation system produces both high quality and diverse outputs? Human evaluation captures quality but not diversity, as it does not catch models that simply plagiarize from the training set. On the other hand, statistical evaluation (i.e., perplexity) captures diversity but not quality, as models that occasionally emit low quality samples would be insufficiently penalized. In this paper, we propose a unified framework which evaluates both diversity and quality, based on the optimal error rate of predicting whether a sentence is human- or machine-generated. We demonstrate that this error rate can be efficiently estimated by combining human and statistical evaluation, using an evaluation metric which we call HUSE. On summarization and chit-chat dialogue, we show that (i) HUSE detects diversity defects which fool pure human evaluation and that (ii) techniques such as annealing for improving quality actually decrease HUSE due to decreased diversity.},
	urldate = {2024-09-20},
	publisher = {arXiv},
	author = {Hashimoto, Tatsunori B. and Zhang, Hugh and Liang, Percy},
	month = apr,
	year = {2019},
	note = {arXiv:1904.02792 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Statistics - Machine Learning},
}

@inproceedings{van_der_lee_best_2019,
	address = {Tokyo, Japan},
	title = {Best practices for the human evaluation of automatically generated text},
	url = {https://aclanthology.org/W19-8643},
	doi = {10.18653/v1/W19-8643},
	abstract = {Currently, there is little agreement as to how Natural Language Generation (NLG) systems should be evaluated. While there is some agreement regarding automatic metrics, there is a high degree of variation in the way that human evaluation is carried out. This paper provides an overview of how human evaluation is currently conducted, and presents a set of best practices, grounded in the literature. With this paper, we hope to contribute to the quality and consistency of human evaluations in NLG.},
	urldate = {2024-09-20},
	booktitle = {Proceedings of the 12th {International} {Conference} on {Natural} {Language} {Generation}},
	publisher = {Association for Computational Linguistics},
	author = {van der Lee, Chris and Gatt, Albert and van Miltenburg, Emiel and Wubben, Sander and Krahmer, Emiel},
	editor = {van Deemter, Kees and Lin, Chenghua and Takamura, Hiroya},
	month = oct,
	year = {2019},
	pages = {355--368},
}

@inproceedings{desmond_evalullm_2024,
	address = {New York, NY, USA},
	series = {{IUI} '24 {Companion}},
	title = {{EvaluLLM}: {LLM} assisted evaluation of generative outputs},
	isbn = {9798400705090},
	shorttitle = {{EvaluLLM}},
	url = {https://dl.acm.org/doi/10.1145/3640544.3645216},
	doi = {10.1145/3640544.3645216},
	abstract = {With the rapid improvement in large language model (LLM) capabilities, its becoming more difficult to measure the quality of outputs generated by natural language generation (NLG) systems. Conventional metrics such as BLEU and ROUGE are bound to reference data, and are generally unsuitable for tasks that require creative or diverse outputs. Human evaluation is an option, but manually evaluating generated text is difficult to do well, and expensive to scale and repeat as requirements and quality criteria change. Recent work has focused on the use of LLMs as customize-able NLG evaluators, and initial results are promising. In this demonstration we present EvaluLLM, an application designed to help practitioners setup, run and review evaluation over sets of NLG outputs, using an LLM as a custom evaluator. Evaluation is formulated as a series of choices between pairs of generated outputs conditioned on a user provided evaluation criteria. This approach simplifies the evaluation task and obviates the need for complex scoring algorithms. The system can be applied to general evaluation, human assisted evaluation, and model selection problems.},
	urldate = {2024-09-20},
	booktitle = {Companion {Proceedings} of the 29th {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author = {Desmond, Michael and Ashktorab, Zahra and Pan, Qian and Dugan, Casey and Johnson, James M.},
	month = apr,
	year = {2024},
	pages = {30--32},
}

@misc{askell_general_2021,
	title = {A {General} {Language} {Assistant} as a {Laboratory} for {Alignment}},
	url = {http://arxiv.org/abs/2112.00861},
	doi = {10.48550/arXiv.2112.00861},
	abstract = {Given the broad capabilities of large language models, it should be possible to work towards a general-purpose, text-based assistant that is aligned with human values, meaning that it is helpful, honest, and harmless. As an initial foray in this direction we study simple baseline techniques and evaluations, such as prompting. We find that the benefits from modest interventions increase with model size, generalize to a variety of alignment evaluations, and do not compromise the performance of large models. Next we investigate scaling trends for several training objectives relevant to alignment, comparing imitation learning, binary discrimination, and ranked preference modeling. We find that ranked preference modeling performs much better than imitation learning, and often scales more favorably with model size. In contrast, binary discrimination typically performs and scales very similarly to imitation learning. Finally we study a `preference model pre-training' stage of training, with the goal of improving sample efficiency when finetuning on human preferences.},
	urldate = {2024-09-20},
	publisher = {arXiv},
	author = {Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and Elhage, Nelson and Hatfield-Dodds, Zac and Hernandez, Danny and Kernion, Jackson and Ndousse, Kamal and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Kaplan, Jared},
	month = dec,
	year = {2021},
	note = {arXiv:2112.00861 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{zhong_towards_2022,
	title = {Towards a {Unified} {Multi}-{Dimensional} {Evaluator} for {Text} {Generation}},
	url = {http://arxiv.org/abs/2210.07197},
	doi = {10.48550/arXiv.2210.07197},
	abstract = {Multi-dimensional evaluation is the dominant paradigm for human evaluation in Natural Language Generation (NLG), i.e., evaluating the generated text from multiple explainable dimensions, such as coherence and fluency. However, automatic evaluation in NLG is still dominated by similarity-based metrics, and we lack a reliable framework for a more comprehensive evaluation of advanced models. In this paper, we propose a unified multi-dimensional evaluator UniEval for NLG. We re-frame NLG evaluation as a Boolean Question Answering (QA) task, and by guiding the model with different questions, we can use one evaluator to evaluate from multiple dimensions. Furthermore, thanks to the unified Boolean QA format, we are able to introduce an intermediate learning phase that enables UniEval to incorporate external knowledge from multiple related tasks and gain further improvement. Experiments on three typical NLG tasks show that UniEval correlates substantially better with human judgments than existing metrics. Specifically, compared to the top-performing unified evaluators, UniEval achieves a 23\% higher correlation on text summarization, and over 43\% on dialogue response generation. Also, UniEval demonstrates a strong zero-shot learning ability for unseen evaluation dimensions and tasks. Source code, data and all pre-trained evaluators are available on our GitHub repository (https://github.com/maszhongming/UniEval).},
	urldate = {2024-09-20},
	publisher = {arXiv},
	author = {Zhong, Ming and Liu, Yang and Yin, Da and Mao, Yuning and Jiao, Yizhu and Liu, Pengfei and Zhu, Chenguang and Ji, Heng and Han, Jiawei},
	month = oct,
	year = {2022},
	note = {arXiv:2210.07197 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{wu_reasoning_2024,
	title = {Reasoning or {Reciting}? {Exploring} the {Capabilities} and {Limitations} of {Language} {Models} {Through} {Counterfactual} {Tasks}},
	shorttitle = {Reasoning or {Reciting}?},
	url = {http://arxiv.org/abs/2307.02477},
	doi = {10.48550/arXiv.2307.02477},
	abstract = {The impressive performance of recent language models across a wide range of tasks suggests that they possess a degree of abstract reasoning skills. Are these skills general and transferable, or specialized to specific tasks seen during pretraining? To disentangle these effects, we propose an evaluation framework based on "counterfactual" task variants that deviate from the default assumptions underlying standard tasks. Across a suite of 11 tasks, we observe nontrivial performance on the counterfactual variants, but nevertheless find that performance substantially and consistently degrades compared to the default conditions. This suggests that while current LMs may possess abstract task-solving skills to an extent, they often also rely on narrow, non-transferable procedures for task-solving. These results motivate a more careful interpretation of language model performance that teases apart these aspects of behavior.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Wu, Zhaofeng and Qiu, Linlu and Ross, Alexis and Akyürek, Ekin and Chen, Boyuan and Wang, Bailin and Kim, Najoung and Andreas, Jacob and Kim, Yoon},
	month = mar,
	year = {2024},
	note = {arXiv:2307.02477 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{lievin_can_2024,
	title = {Can large language models reason about medical questions?},
	volume = {5},
	issn = {2666-3899},
	url = {https://www.cell.com/patterns/abstract/S2666-3899(24)00042-4},
	doi = {10.1016/j.patter.2024.100943},
	language = {English},
	number = {3},
	urldate = {2024-09-19},
	journal = {Patterns},
	author = {Liévin, Valentin and Hother, Christoffer Egeberg and Motzfeldt, Andreas Geert and Winther, Ole},
	month = mar,
	year = {2024},
	note = {Publisher: Elsevier},
	keywords = {GPT-3.5, Llama 2, MedQA, large language models, machine learning, medical, open source, prompt engineering, question answering, uncertainty quantification},
}

@misc{gendron_large_2024,
	title = {Large {Language} {Models} {Are} {Not} {Strong} {Abstract} {Reasoners}},
	url = {http://arxiv.org/abs/2305.19555},
	abstract = {Large Language Models have shown tremendous performance on a large variety of natural language processing tasks, ranging from text comprehension to common sense reasoning. However, the mechanisms responsible for this success remain opaque, and it is unclear whether LLMs can achieve human-like cognitive capabilities or whether these models are still fundamentally circumscribed. Abstract reasoning is a fundamental task for cognition, consisting of ﬁnding and applying a general pattern from few data. Evaluating deep neural architectures on this task could give insight into their potential limitations regarding reasoning and their broad generalisation abilities, yet this is currently an under-explored area. In this paper, we introduce a new benchmark for evaluating language models beyond memorization on abstract reasoning tasks. We perform extensive evaluations of state-of-the-art LLMs, showing that they currently achieve very limited performance in contrast with other natural language tasks, even when applying techniques that have been shown to improve performance on other NLP tasks. We argue that guiding LLM generation to follow causal paths could help improve the generalisation and reasoning abilities of LLMs.},
	language = {en},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Gendron, Gaël and Bao, Qiming and Witbrock, Michael and Dobbie, Gillian},
	month = jan,
	year = {2024},
	note = {arXiv:2305.19555 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, I.2.2, I.2.3, I.2.7, I.5.1},
}

@book{coustaty_document_2023,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Document {Analysis} and {Recognition} – {ICDAR} 2023 {Workshops}: {San} {José}, {CA}, {USA}, {August} 24–26, 2023, {Proceedings}, {Part} {I}},
	volume = {14193},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-3-031-41497-8 978-3-031-41498-5},
	shorttitle = {Document {Analysis} and {Recognition} – {ICDAR} 2023 {Workshops}},
	url = {https://link.springer.com/10.1007/978-3-031-41498-5},
	language = {en},
	urldate = {2024-09-19},
	publisher = {Springer Nature Switzerland},
	editor = {Coustaty, Mickael and Fornés, Alicia},
	year = {2023},
	doi = {10.1007/978-3-031-41498-5},
}

@misc{zhang_llm_2024,
	title = {{LLM} as a {Mastermind}: {A} {Survey} of {Strategic} {Reasoning} with {Large} {Language} {Models}},
	shorttitle = {{LLM} as a {Mastermind}},
	url = {http://arxiv.org/abs/2404.01230},
	abstract = {This paper presents a comprehensive survey of the current status and opportunities for Large Language Models (LLMs) in strategic reasoning, a sophisticated form of reasoning that necessitates understanding and predicting adversary actions in multi-agent settings while adjusting strategies accordingly. Strategic reasoning is distinguished by its focus on the dynamic and uncertain nature of interactions among multi-agents, where comprehending the environment and anticipating the behavior of others is crucial. We explore the scopes, applications, methodologies, and evaluation metrics related to strategic reasoning with LLMs, highlighting the burgeoning development in this area and the interdisciplinary approaches enhancing their decision-making performance. It aims to systematize and clarify the scattered literature on this subject, providing a systematic review that underscores the importance of strategic reasoning as a critical cognitive capability and offers insights into future research directions and potential improvements.},
	language = {en},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Zhang, Yadong and Mao, Shaoguang and Ge, Tao and Wang, Xun and de Wynter, Adrian and Xia, Yan and Wu, Wenshan and Song, Ting and Lan, Man and Wei, Furu},
	month = apr,
	year = {2024},
	note = {arXiv:2404.01230 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{gretz_large-scale_2020,
	title = {A {Large}-{Scale} {Dataset} for {Argument} {Quality} {Ranking}: {Construction} and {Analysis}},
	volume = {34},
	copyright = {https://www.aaai.org},
	issn = {2374-3468, 2159-5399},
	shorttitle = {A {Large}-{Scale} {Dataset} for {Argument} {Quality} {Ranking}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6285},
	doi = {10.1609/aaai.v34i05.6285},
	abstract = {Identifying the quality of free-text arguments has become an important task in the rapidly expanding ﬁeld of computational argumentation. In this work, we explore the challenging task of argument quality ranking. To this end, we created a corpus of 30,497 arguments carefully annotated for point-wise quality, released as part of this work. To the best of our knowledge, this is the largest dataset annotated for point-wise argument quality, larger by a factor of ﬁve than previously released datasets. Moreover, we address the core issue of inducing a labeled score from crowd annotations by performing a comprehensive evaluation of different approaches to this problem. In addition, we analyze the quality dimensions that characterize this dataset. Finally, we present a neural method for argument quality ranking, which outperforms several baselines on our own dataset, as well as previous methods published for another dataset.},
	language = {en},
	number = {05},
	urldate = {2024-09-19},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Gretz, Shai and Friedman, Roni and Cohen-Karlik, Edo and Toledo, Assaf and Lahav, Dan and Aharonov, Ranit and Slonim, Noam},
	month = apr,
	year = {2020},
	pages = {7805--7813},
}

@inproceedings{trichelair_how_2019,
	title = {How {Reasonable} are {Common}-{Sense} {Reasoning} {Tasks}: {A} {Case}-{Study} on the {Winograd} {Schema} {Challenge} and {SWAG}},
	shorttitle = {How {Reasonable} are {Common}-{Sense} {Reasoning} {Tasks}},
	url = {http://arxiv.org/abs/1811.01778},
	doi = {10.18653/v1/D19-1335},
	abstract = {Recent studies have significantly improved the state-of-the-art on common-sense reasoning (CSR) benchmarks like the Winograd Schema Challenge (WSC) and SWAG. The question we ask in this paper is whether improved performance on these benchmarks represents genuine progress towards common-sense-enabled systems. We make case studies of both benchmarks and design protocols that clarify and qualify the results of previous work by analyzing threats to the validity of previous experimental designs. Our protocols account for several properties prevalent in common-sense benchmarks including size limitations, structural regularities, and variable instance difficulty.},
	urldate = {2024-09-19},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	author = {Trichelair, Paul and Emami, Ali and Trischler, Adam and Suleman, Kaheer and Cheung, Jackie Chi Kit},
	year = {2019},
	note = {arXiv:1811.01778 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {3380--3385},
}

@inproceedings{ribeiro_why_2016,
	address = {San Francisco California USA},
	title = {"{Why} {Should} {I} {Trust} {You}?": {Explaining} the {Predictions} of {Any} {Classifier}},
	isbn = {978-1-4503-4232-2},
	shorttitle = {"{Why} {Should} {I} {Trust} {You}?},
	url = {https://dl.acm.org/doi/10.1145/2939672.2939778},
	doi = {10.1145/2939672.2939778},
	language = {en},
	urldate = {2024-09-19},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	month = aug,
	year = {2016},
	pages = {1135--1144},
}

@inproceedings{liao_questioning_2020,
	address = {Honolulu HI USA},
	title = {Questioning the {AI}: {Informing} {Design} {Practices} for {Explainable} {AI} {User} {Experiences}},
	isbn = {978-1-4503-6708-0},
	shorttitle = {Questioning the {AI}},
	url = {https://dl.acm.org/doi/10.1145/3313831.3376590},
	doi = {10.1145/3313831.3376590},
	language = {en},
	urldate = {2024-09-19},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Liao, Q. Vera and Gruen, Daniel and Miller, Sarah},
	month = apr,
	year = {2020},
	pages = {1--15},
}

@misc{noauthor_questioning_nodate,
	title = {Questioning the {AI}: {Informing} {Design} {Practices} for {Explainable} {AI} {User} {Experiences} {\textbar} {Proceedings} of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	url = {https://dl.acm.org/doi/abs/10.1145/3313831.3376590?casa_token=g17W95Kpgw8AAAAA:r36S2R1NJ8t7SgMeYhU5qBU-uT-ml6xoX--9bGCjJdlI9GAVRO2ebls3CRSNVUe9YxtNK9WmzyEh},
	urldate = {2024-09-19},
}

@article{mohseni_multidisciplinary_2021,
	title = {A {Multidisciplinary} {Survey} and {Framework} for {Design} and {Evaluation} of {Explainable} {AI} {Systems}},
	volume = {11},
	issn = {2160-6455, 2160-6463},
	url = {https://dl.acm.org/doi/10.1145/3387166},
	doi = {10.1145/3387166},
	abstract = {The need for interpretable and accountable intelligent systems grows along with the prevalence of
              artificial intelligence
              (
              AI
              ) applications used in everyday life.
              Explainable AI
              (
              XAI
              ) systems are intended to self-explain the reasoning behind system decisions and predictions. Researchers from different disciplines work together to define, design, and evaluate explainable systems. However, scholars from different disciplines focus on different objectives and fairly independent topics of XAI research, which poses challenges for identifying appropriate design and evaluation methodology and consolidating knowledge across efforts. To this end, this article presents a survey and framework intended to share knowledge and experiences of XAI design and evaluation methods across multiple disciplines. Aiming to support diverse design goals and evaluation methods in XAI research, after a thorough review of XAI related papers in the fields of machine learning, visualization, and human-computer interaction, we present a categorization of XAI design goals and evaluation methods. Our categorization presents the mapping between design goals for different XAI user groups and their evaluation methods. From our findings, we develop a framework with step-by-step design guidelines paired with evaluation methods to close the iterative design and evaluation cycles in multidisciplinary XAI teams. Further, we provide summarized ready-to-use tables of evaluation methods and recommendations for different goals in XAI research.},
	language = {en},
	number = {3-4},
	urldate = {2024-09-19},
	journal = {ACM Transactions on Interactive Intelligent Systems},
	author = {Mohseni, Sina and Zarei, Niloofar and Ragan, Eric D.},
	month = dec,
	year = {2021},
	pages = {1--45},
}

@misc{noauthor_multidisciplinary_nodate,
	title = {A {Multidisciplinary} {Survey} and {Framework} for {Design} and {Evaluation} of {Explainable} {AI} {Systems} {\textbar} {ACM} {Transactions} on {Interactive} {Intelligent} {Systems}},
	url = {https://dl.acm.org/doi/abs/10.1145/3387166},
	urldate = {2024-09-19},
}

@misc{hoffman_metrics_2019,
	title = {Metrics for {Explainable} {AI}: {Challenges} and {Prospects}},
	shorttitle = {Metrics for {Explainable} {AI}},
	url = {http://arxiv.org/abs/1812.04608},
	doi = {10.48550/arXiv.1812.04608},
	abstract = {The question addressed in this paper is: If we present to a user an AI system that explains how it works, how do we know whether the explanation works and the user has achieved a pragmatic understanding of the AI? In other words, how do we know that an explanainable AI system (XAI) is any good? Our focus is on the key concepts of measurement. We discuss specific methods for evaluating: (1) the goodness of explanations, (2) whether users are satisfied by explanations, (3) how well users understand the AI systems, (4) how curiosity motivates the search for explanations, (5) whether the user's trust and reliance on the AI are appropriate, and finally, (6) how the human-XAI work system performs. The recommendations we present derive from our integration of extensive research literatures and our own psychometric evaluations.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Hoffman, Robert R. and Mueller, Shane T. and Klein, Gary and Litman, Jordan},
	month = feb,
	year = {2019},
	note = {arXiv:1812.04608 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{lindquist_brain_2012,
	title = {The brain basis of emotion: a meta-analytic review},
	volume = {35},
	shorttitle = {The brain basis of emotion},
	url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/brain-basis-of-emotion-a-metaanalytic-review/80F95F093305C76BA2C66BBA48D4BC8A},
	number = {3},
	urldate = {2024-04-04},
	journal = {Behavioral and brain sciences},
	author = {Lindquist, Kristen A. and Wager, Tor D. and Kober, Hedy and Bliss-Moreau, Eliza and Barrett, Lisa Feldman},
	year = {2012},
	note = {Publisher: Cambridge University Press},
	pages = {121--143},
}

@article{coppini_experiments_2023,
	title = {Experiments on real-life emotions challenge {Ekman}'s model},
	volume = {13},
	copyright = {2023 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-023-36201-5},
	doi = {10.1038/s41598-023-36201-5},
	abstract = {Ekman's emotions (1992) are defined as universal basic emotions. Over the years, alternative models have emerged (e.g. Greene and Haidt 2002; Barrett 2017) describing emotions as social and linguistic constructions. The variety of models existing today raises the question of whether the abstraction provided by such models is sufficient as a descriptive/predictive tool for representing real-life emotional situations. Our study presents a social inquiry to test whether traditional models are sufficient to capture the complexity of daily life emotions, reported in a textual context. The intent of the study is to establish the human-subject agreement rate in an annotated corpus based on Ekman's theory (Entity-Level Tweets Emotional Analysis) and the human-subject agreement rate when using Ekman's emotions to annotate sentences that don’t respect the Ekman’s model (The Dictionary of Obscure Sorrows). Furthermore, we investigated how much alexithymia can influence the human ability to detect and categorise emotions. On a total sample of 114 subjects, our results show low within subjects agreement rates for both datasets, particularly for subjects with low levels of alexithymia; low levels of agreement with the original annotations; frequent use of emotions based on Ekman model, particularly negative one, in people with high levels of alexithymia.},
	language = {en},
	number = {1},
	urldate = {2024-04-19},
	journal = {Scientific Reports},
	author = {Coppini, Sara and Lucifora, Chiara and Vicario, Carmelo M. and Gangemi, Aldo},
	month = jun,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Human behaviour, Psychology, ekman},
	pages = {9511},
}

@book{werthner_introduction_2024,
	address = {Cham},
	title = {Introduction to {Digital} {Humanism}: {A} {Textbook}},
	copyright = {https://creativecommons.org/licenses/by/4.0},
	isbn = {978-3-031-45303-8 978-3-031-45304-5},
	shorttitle = {Introduction to {Digital} {Humanism}},
	url = {https://link.springer.com/10.1007/978-3-031-45304-5},
	language = {en},
	urldate = {2024-04-19},
	publisher = {Springer Nature Switzerland},
	editor = {Werthner, Hannes and Ghezzi, Carlo and Kramer, Jeff and Nida-Rümelin, Julian and Nuseibeh, Bashar and Prem, Erich and Stanger, Allison},
	year = {2024},
	doi = {10.1007/978-3-031-45304-5},
}

@inproceedings{peffers_design_2012,
	address = {Berlin, Heidelberg},
	title = {Design {Science} {Research} {Evaluation}},
	isbn = {978-3-642-29863-9},
	doi = {10.1007/978-3-642-29863-9_29},
	abstract = {The consensus view is that the rigorous evaluation of design science (DS) artifacts is essential. There are many types of DS artifacts and many forms of evaluation; what is missing is guidance for how to perform the evaluation, more specifically, what evaluation methods to use with specific DS research outputs. Here we find and review 148 DS research articles published in a selected set of information systems (IS), computer science (CS) and engineering journals. We analyze the articles to develop taxonomies of DS artifact types and artifact evaluation methods; we apply these taxonomies to determine which evaluation methods are associated in the literature with particular artifacts. We show that there are several popular “artifact - evaluation method” combinations in the literature. The results inform DS researchers of usual and customary combinations of research artifacts and evaluation methods, potentially providing them with rationale and justification for an evaluation method selection.},
	language = {en},
	booktitle = {Design {Science} {Research} in {Information} {Systems}. {Advances} in {Theory} and {Practice}},
	publisher = {Springer},
	author = {Peffers, Ken and Rothenberger, Marcus and Tuunanen, Tuure and Vaezi, Reza},
	editor = {Peffers, Ken and Rothenberger, Marcus and Kuechler, Bill},
	year = {2012},
	keywords = {Design Science, artifacts, evaluation},
	pages = {398--410},
}

@misc{yao_react_2023,
	title = {{ReAct}: {Synergizing} {Reasoning} and {Acting} in {Language} {Models}},
	shorttitle = {{ReAct}},
	url = {http://arxiv.org/abs/2210.03629},
	doi = {10.48550/arXiv.2210.03629},
	abstract = {While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io},
	urldate = {2024-06-17},
	publisher = {arXiv},
	author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
	month = mar,
	year = {2023},
	note = {arXiv:2210.03629 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{li_more_2024,
	title = {More {Agents} {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/2402.05120},
	abstract = {We find that, simply via a sampling-and-voting method, the performance of large language models (LLMs) scales with the number of agents instantiated. Also, this method is orthogonal to existing complicated methods to further enhance LLMs, while the degree of enhancement is correlated to the task difficulty. We conduct comprehensive experiments on a wide range of LLM benchmarks to verify the presence of our finding, and to study the properties that can facilitate its occurrence. Our code is publicly available at: Git.},
	language = {en},
	urldate = {2024-04-08},
	publisher = {arXiv},
	author = {Li, Junyou and Zhang, Qin and Yu, Yangbin and Fu, Qiang and Ye, Deheng},
	month = feb,
	year = {2024},
	note = {arXiv:2402.05120 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{heitzinger_large_nodate,
	title = {Large {Language} {Models} ({Lecture} {Notes} for {VU} {Generative} {AI})},
	language = {en},
	author = {Heitzinger, Clemens},
}

@inproceedings{di_palma_retrieval-augmented_2023,
	address = {Singapore Singapore},
	title = {Retrieval-augmented {Recommender} {System}: {Enhancing} {Recommender} {Systems} with {Large} {Language} {Models}},
	isbn = {9798400702419},
	shorttitle = {Retrieval-augmented {Recommender} {System}},
	url = {https://dl.acm.org/doi/10.1145/3604915.3608889},
	doi = {10.1145/3604915.3608889},
	abstract = {Recommender Systems (RSs) play a pivotal role in delivering personalized recommendations across various domains, from e-commerce to content streaming platforms. Recent advancements in natural language processing have introduced Large Language Models (LLMs) that exhibit remarkable capabilities in understanding and generating human-like text. RS are renowned for their effectiveness and proficiency within clearly defined domains; nevertheless, they are limited in adaptability and incapable of providing recommendations for unexplored data. Conversely, LLMs exhibit contextual awareness and strong adaptability to unseen data. Combining these technologies creates a powerful tool for delivering contextual and relevant recommendations, even in cold scenarios characterized by high data sparsity. The proposal aims to explore the possibilities of integrating LLMs into RS, introducing a novel approach called Retrieval-augmented Recommender Systems, which combines the strengths of retrieval-based and generation-based models to enhance the ability of RSs to provide relevant suggestions.},
	language = {en},
	urldate = {2023-11-18},
	booktitle = {Proceedings of the 17th {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {Di Palma, Dario},
	month = sep,
	year = {2023},
	pages = {1369--1373},
}

@misc{openai_chatgpt_2023,
	title = {{ChatGPT} - {Homepage}},
	shorttitle = {{ChatGPT} - {Homepage}},
	url = {https://chat.openai.com},
	abstract = {A conversational AI system that listens, learns, and challenges},
	language = {en-US},
	urldate = {2023-11-18},
	author = {OpenAi},
	month = nov,
	year = {2023},
}

@misc{packer_memgpt_2023,
	title = {{MemGPT}: {Towards} {LLMs} as {Operating} {Systems}},
	shorttitle = {{MemGPT}},
	url = {http://arxiv.org/abs/2310.08560},
	abstract = {Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.},
	urldate = {2023-12-05},
	publisher = {arXiv},
	author = {Packer, Charles and Fang, Vivian and Patil, Shishir G. and Lin, Kevin and Wooders, Sarah and Gonzalez, Joseph E.},
	month = oct,
	year = {2023},
	note = {arXiv:2310.08560 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{hu_lora_2021,
	title = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
	shorttitle = {{LoRA}},
	url = {http://arxiv.org/abs/2106.09685},
	abstract = {An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full ﬁne-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example – deploying independent instances of ﬁne-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B ﬁne-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than ﬁnetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deﬁciency in language model adaptation, which sheds light on the efﬁcacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
	language = {en},
	urldate = {2023-11-29},
	publisher = {arXiv},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	month = oct,
	year = {2021},
	note = {arXiv:2106.09685 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{nasr_scalable_2023,
	title = {Scalable {Extraction} of {Training} {Data} from ({Production}) {Language} {Models}},
	url = {http://arxiv.org/abs/2311.17035},
	abstract = {This paper studies extractable memorization: training data that an adversary can efficiently extract by querying a machine learning model without prior knowledge of the training dataset. We show an adversary can extract gigabytes of training data from open-source language models like Pythia or GPT-Neo, semi-open models like LLaMA or Falcon, and closed models like ChatGPT. Existing techniques from the literature suffice to attack unaligned models; in order to attack the aligned ChatGPT, we develop a new divergence attack that causes the model to diverge from its chatbot-style generations and emit training data at a rate 150× higher than when behaving properly. Our methods show practical attacks can recover far more data than previously thought, and reveal that current alignment techniques do not eliminate memorization.},
	language = {en},
	urldate = {2023-12-05},
	publisher = {arXiv},
	author = {Nasr, Milad and Carlini, Nicholas and Hayase, Jonathan and Jagielski, Matthew and Cooper, A. Feder and Ippolito, Daphne and Choquette-Choo, Christopher A. and Wallace, Eric and Tramèr, Florian and Lee, Katherine},
	month = nov,
	year = {2023},
	note = {arXiv:2311.17035 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{noauthor_building_nodate,
	title = {Building {RAG} from {Scratch} ({Lower}-{Level}) - {LlamaIndex} 🦙 0.9.11.post1},
	url = {https://docs.llamaindex.ai/en/stable/optimizing/building_rag_from_scratch.html},
	urldate = {2023-12-05},
}

@misc{wu_autogen_2023,
	title = {{AutoGen}: {Enabling} {Next}-{Gen} {LLM} {Applications} via {Multi}-{Agent} {Conversation}},
	shorttitle = {{AutoGen}},
	url = {http://arxiv.org/abs/2308.08155},
	abstract = {AutoGen2 is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic framework for building diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.},
	language = {en},
	urldate = {2023-12-06},
	publisher = {arXiv},
	author = {Wu, Qingyun and Bansal, Gagan and Zhang, Jieyu and Wu, Yiran and Li, Beibin and Zhu, Erkang and Jiang, Li and Zhang, Xiaoyun and Zhang, Shaokun and Liu, Jiale and Awadallah, Ahmed Hassan and White, Ryen W. and Burger, Doug and Wang, Chi},
	month = oct,
	year = {2023},
	note = {Issue: arXiv:2308.08155
arXiv:2308.08155 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{touvron_llama_2023,
	title = {Llama 2: {Open} {Foundation} and {Fine}-{Tuned} {Chat} {Models}},
	shorttitle = {Llama 2},
	url = {http://arxiv.org/abs/2307.09288},
	abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
	language = {en},
	urldate = {2023-12-11},
	publisher = {arXiv},
	author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
	month = jul,
	year = {2023},
	note = {arXiv:2307.09288 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{alizadeh_llm_2023,
	title = {{LLM} in a flash: {Efficient} {Large} {Language} {Model} {Inference} with {Limited} {Memory}},
	shorttitle = {{LLM} in a flash},
	url = {http://arxiv.org/abs/2312.11514},
	doi = {10.48550/arXiv.2312.11514},
	abstract = {Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their intensive computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters on flash memory but bringing them on demand to DRAM. Our method involves constructing an inference cost model that harmonizes with the flash memory behavior, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this flash memory-informed framework, we introduce two principal techniques. First, "windowing'" strategically reduces data transfer by reusing previously activated neurons, and second, "row-column bundling", tailored to the sequential data access strengths of flash memory, increases the size of data chunks read from flash memory. These methods collectively enable running models up to twice the size of the available DRAM, with a 4-5x and 20-25x increase in inference speed compared to naive loading approaches in CPU and GPU, respectively. Our integration of sparsity awareness, context-adaptive loading, and a hardware-oriented design paves the way for effective inference of LLMs on devices with limited memory.},
	urldate = {2023-12-26},
	publisher = {arXiv},
	author = {Alizadeh, Keivan and Mirzadeh, Iman and Belenko, Dmitry and Khatamifard, Karen and Cho, Minsik and Del Mundo, Carlo C. and Rastegari, Mohammad and Farajtabar, Mehrdad},
	month = dec,
	year = {2023},
	note = {arXiv:2312.11514 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{noauthor_ai_2023,
	title = {{AI} {Injections}: {Direct} and {Indirect} {Prompt} {Injections} and {Their} {Implications} · {Embrace} {The} {Red}},
	shorttitle = {{AI} {Injections}},
	url = {https://embracethered.com/blog/posts/2023/ai-injections-direct-and-indirect-prompt-injection-basics/},
	language = {en-us},
	urldate = {2024-01-02},
	journal = {Embrace The Red},
	month = mar,
	year = {2023},
}

@article{griffith_policy_nodate,
	title = {Policy {Shaping}: {Integrating} {Human} {Feedback} with {Reinforcement} {Learning}},
	abstract = {A long term goal of Interactive Reinforcement Learning is to incorporate nonexpert human feedback to solve complex tasks. Some state-of-the-art methods have approached this problem by mapping human information to rewards and values and iterating over them to compute better control policies. In this paper we argue for an alternate, more effective characterization of human feedback: Policy Shaping. We introduce Advise, a Bayesian approach that attempts to maximize the information gained from human feedback by utilizing it as direct policy labels. We compare Advise to state-of-the-art approaches and show that it can outperform them and is robust to infrequent and inconsistent human feedback.},
	language = {en},
	author = {Griffith, Shane and Subramanian, Kaushik and Scholz, Jonathan and Isbell, Charles L and Thomaz, Andrea},
}

@misc{bubeck_sparks_2023,
	title = {Sparks of {Artificial} {General} {Intelligence}: {Early} experiments with {GPT}-4},
	shorttitle = {Sparks of {Artificial} {General} {Intelligence}},
	url = {http://arxiv.org/abs/2303.12712},
	doi = {10.48550/arXiv.2303.12712},
	abstract = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
	urldate = {2024-01-03},
	publisher = {arXiv},
	author = {Bubeck, Sébastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
	month = apr,
	year = {2023},
	note = {arXiv:2303.12712 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{jiang_longllmlingua_2023,
	title = {{LongLLMLingua}: {Accelerating} and {Enhancing} {LLMs} in {Long} {Context} {Scenarios} via {Prompt} {Compression}},
	shorttitle = {{LongLLMLingua}},
	url = {http://arxiv.org/abs/2310.06839},
	doi = {10.48550/arXiv.2310.06839},
	abstract = {In long context scenarios, large language models (LLMs) face three main challenges: higher computational/financial cost, longer latency, and inferior performance. Some studies reveal that the performance of LLMs depends on both the density and the position of the key information (question relevant) in the input prompt. Inspired by these findings, we propose LongLLMLingua for prompt compression towards improving LLMs' perception of the key information to simultaneously address the three challenges. We conduct evaluation on a wide range of long context scenarios including single-/multi-document QA, few-shot learning, summarization, synthetic tasks, and code completion. The experimental results show that LongLLMLingua compressed prompt can derive higher performance with much less cost. The latency of the end-to-end system is also reduced. For example, on NaturalQuestions benchmark, LongLLMLingua gains a performance boost of up to 17.1\% over the original prompt with {\textasciitilde}4x fewer tokens as input to GPT-3.5-Turbo. It can derive cost savings of {\textbackslash}\$28.5 and {\textbackslash}\$27.4 per 1,000 samples from the LongBench and ZeroScrolls benchmark, respectively. Additionally, when compressing prompts of {\textasciitilde}10k tokens at a compression rate of 2x-10x, LongLLMLingua can speed up the end-to-end latency by 1.4x-3.8x. Our code is available at https://aka.ms/LLMLingua.},
	urldate = {2024-01-17},
	publisher = {arXiv},
	author = {Jiang, Huiqiang and Wu, Qianhui and Luo, Xufang and Li, Dongsheng and Lin, Chin-Yew and Yang, Yuqing and Qiu, Lili},
	month = oct,
	year = {2023},
	note = {arXiv:2310.06839 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{noauthor_gemma_2024,
	title = {Gemma: {Introducing} new state-of-the-art open models},
	shorttitle = {Gemma},
	url = {https://blog.google/technology/developers/gemma-open-models/},
	abstract = {Gemma is a family of lightweight, state{\textbackslash}u002Dof{\textbackslash}u002Dthe art open models built from the same research and technology used to create the Gemini models.},
	language = {en-us},
	urldate = {2024-03-18},
	journal = {Google},
	month = feb,
	year = {2024},
}

@misc{shinn_reflexion_2023,
	title = {Reflexion: {Language} {Agents} with {Verbal} {Reinforcement} {Learning}},
	shorttitle = {Reflexion},
	url = {http://arxiv.org/abs/2303.11366},
	abstract = {Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91\% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80\%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance.},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Shinn, Noah and Cassano, Federico and Berman, Edward and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
	month = oct,
	year = {2023},
	note = {arXiv:2303.11366 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{xuan-quy_evaluation_2023,
	title = {Evaluation of {ChatGPT} and {Microsoft} {Bing} {AI} {Chat} {Performances} on {Physics} {Exams} of {Vietnamese} {National} {High} {School} {Graduation} {Examination}},
	url = {http://arxiv.org/abs/2306.04538},
	abstract = {The promise and difficulties of language model-based approaches for physics teaching were assessed in this study. This study evaluates how well ChatGPT and BingChat, two state-of-the-art (SOTA) large language models (LLMs), perform when answering high school physics questions on Vietnamese exams from 2019 to 2023. When we compared the results of the LLMs with the scores of Vietnamese students, we discovered that ChatGPT and BingChat both perform worse than Vietnamese students, proving that LLMs are not yet capable of fully replacing human intellect in the field of physics teaching. The outcomes also showed that neither LLM is capable of responding to questions at the high application levels. In terms of accuracy, BingChat typically surpassed ChatGPT, although ChatGPT showed more stability. Our research suggests that LLMs can help students and teachers during learning and teaching activities, particularly by offering immediate feedback and individualized learning experiences.},
	language = {en},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Xuan-Quy, Dao and Ngoc-Bich, Le and Xuan-Dung, Phan and Bac-Bien, Ngo and The-Duy, Vo},
	month = jun,
	year = {2023},
	note = {Issue: arXiv:2306.04538
arXiv:2306.04538 [physics]},
	keywords = {Physics - Physics Education},
}

@misc{xu_chatgpt_2023,
	title = {{ChatGPT} vs. {Google}: {A} {Comparative} {Study} of {Search} {Performance} and {User} {Experience}},
	shorttitle = {{ChatGPT} vs. {Google}},
	url = {http://arxiv.org/abs/2307.01135},
	doi = {10.48550/arXiv.2307.01135},
	abstract = {The advent of ChatGPT, a large language model-powered chatbot, has prompted questions about its potential implications for traditional search engines. In this study, we investigate the differences in user behavior when employing search engines and chatbot tools for information-seeking tasks. We carry out a randomized online experiment, dividing participants into two groups: one using a ChatGPT-like tool and the other using a Google Search-like tool. Our findings reveal that the ChatGPT group consistently spends less time on all tasks, with no significant difference in overall task performance between the groups. Notably, ChatGPT levels user search performance across different education levels and excels in answering straightforward questions and providing general solutions but falls short in fact-checking tasks. Users perceive ChatGPT's responses as having higher information quality compared to Google Search, despite displaying a similar level of trust in both tools. Furthermore, participants using ChatGPT report significantly better user experiences in terms of usefulness, enjoyment, and satisfaction, while perceived ease of use remains comparable between the two tools. However, ChatGPT may also lead to overreliance and generate or replicate misinformation, yielding inconsistent results. Our study offers valuable insights for search engine management and highlights opportunities for integrating chatbot technologies into search engine designs.},
	urldate = {2024-04-29},
	publisher = {arXiv},
	author = {Xu, Ruiyun and Feng, Yue and Chen, Hailiang},
	month = jul,
	year = {2023},
	note = {arXiv:2307.01135 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Information Retrieval},
}

@misc{dibia_multi-agent_2023,
	title = {Multi-{Agent} {LLM} {Applications} {\textbar} {A} {Review} of {Current} {Research}, {Tools}, and {Challenges}},
	url = {https://newsletter.victordibia.com/p/multi-agent-llm-applications-a-review},
	abstract = {Issue \# 16 {\textbar} Autonomous Agents are the next step in the evolution of LLM Apps. What is the state of the art, applications and current limitations?},
	language = {en},
	urldate = {2024-05-08},
	author = {Dibia, Victor},
	month = feb,
	year = {2023},
	keywords = {Multi-Agent},
}

@misc{teo_how_2024,
	title = {How {I} {Won} {Singapore}’s {GPT}-4 {Prompt} {Engineering} {Competition}},
	url = {https://towardsdatascience.com/how-i-won-singapores-gpt-4-prompt-engineering-competition-34c195a93d41},
	abstract = {A deep dive into the strategies I learned for harnessing the power of Large Language Models (LLMs)},
	language = {en},
	urldate = {2024-05-16},
	journal = {Towards Data Science},
	author = {Teo, Sheila},
	month = apr,
	year = {2024},
}

@misc{liu_calibrating_2023,
	title = {Calibrating {LLM}-{Based} {Evaluator}},
	url = {http://arxiv.org/abs/2309.13308},
	doi = {10.48550/arXiv.2309.13308},
	abstract = {Recent advancements in large language models (LLMs) on language modeling and emergent capabilities make them a promising reference-free evaluator of natural language generation quality, and a competent alternative to human evaluation. However, hindered by the closed-source or high computational demand to host and tune, there is a lack of practice to further calibrate an off-the-shelf LLM-based evaluator towards better human alignment. In this work, we propose AutoCalibrate, a multi-stage, gradient-free approach to automatically calibrate and align an LLM-based evaluator toward human preference. Instead of explicitly modeling human preferences, we first implicitly encompass them within a set of human labels. Then, an initial set of scoring criteria is drafted by the language model itself, leveraging in-context learning on different few-shot examples. To further calibrate this set of criteria, we select the best performers and re-draft them with self-refinement. Our experiments on multiple text quality evaluation datasets illustrate a significant improvement in correlation with expert evaluation through calibration. Our comprehensive qualitative analysis conveys insightful intuitions and observations on the essence of effective scoring criteria.},
	urldate = {2024-05-16},
	publisher = {arXiv},
	author = {Liu, Yuxuan and Yang, Tianchi and Huang, Shaohan and Zhang, Zihan and Huang, Haizhen and Wei, Furu and Deng, Weiwei and Sun, Feng and Zhang, Qi},
	month = sep,
	year = {2023},
	note = {arXiv:2309.13308 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{chan_chateval_2023,
	title = {{ChatEval}: {Towards} {Better} {LLM}-based {Evaluators} through {Multi}-{Agent} {Debate}},
	shorttitle = {{ChatEval}},
	url = {http://arxiv.org/abs/2308.07201},
	doi = {10.48550/arXiv.2308.07201},
	abstract = {Text evaluation has historically posed significant challenges, often demanding substantial labor and time cost. With the emergence of large language models (LLMs), researchers have explored LLMs' potential as alternatives for human evaluation. While these single-agent-based approaches show promise, experimental results suggest that further advancements are needed to bridge the gap between their current effectiveness and human-level evaluation quality. Recognizing that best practices of human evaluation processes often involve multiple human annotators collaborating in the evaluation, we resort to a multi-agent debate framework, moving beyond single-agent prompting strategies. The multi-agent-based approach enables a group of LLMs to synergize with an array of intelligent counterparts, harnessing their distinct capabilities and expertise to enhance efficiency and effectiveness in handling intricate tasks. In this paper, we construct a multi-agent referee team called ChatEval to autonomously discuss and evaluate the quality of generated responses from different models on open-ended questions and traditional natural language generation (NLG) tasks. Our analysis shows that ChatEval transcends mere textual scoring, offering a human-mimicking evaluation process for reliable assessments. Our code is available at https://github.com/chanchimin/ChatEval.},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Chan, Chi-Min and Chen, Weize and Su, Yusheng and Yu, Jianxuan and Xue, Wei and Zhang, Shanghang and Fu, Jie and Liu, Zhiyuan},
	month = aug,
	year = {2023},
	note = {arXiv:2308.07201 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{hada_are_2024,
	title = {Are {Large} {Language} {Model}-based {Evaluators} the {Solution} to {Scaling} {Up} {Multilingual} {Evaluation}?},
	url = {http://arxiv.org/abs/2309.07462},
	doi = {10.48550/arXiv.2309.07462},
	abstract = {Large Language Models (LLMs) excel in various Natural Language Processing (NLP) tasks, yet their evaluation, particularly in languages beyond the top \$20\$, remains inadequate due to existing benchmarks and metrics limitations. Employing LLMs as evaluators to rank or score other models' outputs emerges as a viable solution, addressing the constraints tied to human annotators and established benchmarks. In this study, we explore the potential of LLM-based evaluators, specifically GPT-4 in enhancing multilingual evaluation by calibrating them against \$20\$K human judgments across three text-generation tasks, five metrics, and eight languages. Our analysis reveals a bias in GPT4-based evaluators towards higher scores, underscoring the necessity of calibration with native speaker judgments, especially in low-resource and non-Latin script languages, to ensure accurate evaluation of LLM performance across diverse languages.},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Hada, Rishav and Gumma, Varun and de Wynter, Adrian and Diddee, Harshita and Ahmed, Mohamed and Choudhury, Monojit and Bali, Kalika and Sitaram, Sunayana},
	month = feb,
	year = {2024},
	note = {arXiv:2309.07462 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{kim_prometheus_2024,
	title = {Prometheus 2: {An} {Open} {Source} {Language} {Model} {Specialized} in {Evaluating} {Other} {Language} {Models}},
	shorttitle = {Prometheus 2},
	url = {http://arxiv.org/abs/2405.01535},
	doi = {10.48550/arXiv.2405.01535},
	abstract = {Proprietary LMs such as GPT-4 are often employed to assess the quality of responses from various LMs. However, concerns including transparency, controllability, and affordability strongly motivate the development of open-source LMs specialized in evaluations. On the other hand, existing open evaluator LMs exhibit critical shortcomings: 1) they issue scores that significantly diverge from those assigned by humans, and 2) they lack the flexibility to perform both direct assessment and pairwise ranking, the two most prevalent forms of assessment. Additionally, they do not possess the ability to evaluate based on custom evaluation criteria, focusing instead on general attributes like helpfulness and harmlessness. To address these issues, we introduce Prometheus 2, a more powerful evaluator LM than its predecessor that closely mirrors human and GPT-4 judgements. Moreover, it is capable of processing both direct assessment and pair-wise ranking formats grouped with a user-defined evaluation criteria. On four direct assessment benchmarks and four pairwise ranking benchmarks, Prometheus 2 scores the highest correlation and agreement with humans and proprietary LM judges among all tested open evaluator LMs. Our models, code, and data are all publicly available at https://github.com/prometheus-eval/prometheus-eval.},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Kim, Seungone and Suk, Juyoung and Longpre, Shayne and Lin, Bill Yuchen and Shin, Jamin and Welleck, Sean and Neubig, Graham and Lee, Moontae and Lee, Kyungjae and Seo, Minjoon},
	month = may,
	year = {2024},
	note = {arXiv:2405.01535 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{noauthor_anthropic_nodate,
	title = {Anthropic {Console}},
	url = {https://console.anthropic.com/workbench/c2a68cc3-c628-4165-bb83-6d55fe0adce0},
	urldate = {2024-05-28},
}

@misc{gao_retrieval-augmented_2024,
	title = {Retrieval-{Augmented} {Generation} for {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Retrieval-{Augmented} {Generation} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2312.10997},
	abstract = {Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domainspecific information. RAG synergistically merges LLMs’ intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-theart technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development 1.},
	language = {en},
	urldate = {2024-06-20},
	publisher = {arXiv},
	author = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Meng and Wang, Haofen},
	month = mar,
	year = {2024},
	note = {arXiv:2312.10997 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{huang_position_2024,
	title = {Position {Paper}: {Agent} {AI} {Towards} a {Holistic} {Intelligence}},
	shorttitle = {Position {Paper}},
	url = {http://arxiv.org/abs/2403.00833},
	doi = {10.48550/arXiv.2403.00833},
	abstract = {Recent advancements in large foundation models have remarkably enhanced our understanding of sensory information in open-world environments. In leveraging the power of foundation models, it is crucial for AI research to pivot away from excessive reductionism and toward an emphasis on systems that function as cohesive wholes. Specifically, we emphasize developing Agent AI -- an embodied system that integrates large foundation models into agent actions. The emerging field of Agent AI spans a wide range of existing embodied and agent-based multimodal interactions, including robotics, gaming, and healthcare systems, etc. In this paper, we propose a novel large action model to achieve embodied intelligent behavior, the Agent Foundation Model. On top of this idea, we discuss how agent AI exhibits remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Furthermore, we discuss the potential of Agent AI from an interdisciplinary perspective, underscoring AI cognition and consciousness within scientific discourse. We believe that those discussions serve as a basis for future research directions and encourage broader societal engagement.},
	urldate = {2024-07-17},
	publisher = {arXiv},
	author = {Huang, Qiuyuan and Wake, Naoki and Sarkar, Bidipta and Durante, Zane and Gong, Ran and Taori, Rohan and Noda, Yusuke and Terzopoulos, Demetri and Kuno, Noboru and Famoti, Ade and Llorens, Ashley and Langford, John and Vo, Hoi and Fei-Fei, Li and Ikeuchi, Katsu and Gao, Jianfeng},
	month = feb,
	year = {2024},
	note = {arXiv:2403.00833 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{salemi_evaluating_2024,
	title = {Evaluating {Retrieval} {Quality} in {Retrieval}-{Augmented} {Generation}},
	url = {http://arxiv.org/abs/2404.13781},
	abstract = {Evaluating retrieval-augmented generation (RAG) presents challenges, particularly for retrieval models within these systems. Traditional end-to-end evaluation methods are computationally expensive. Furthermore, evaluation of the retrieval model’s performance based on query-document relevance labels shows a small correlation with the RAG system’s downstream performance. We propose a novel evaluation approach, eRAG, where each document in the retrieval list is individually utilized by the large language model within the RAG system. The output generated for each document is then evaluated based on the downstream task ground truth labels. In this manner, the downstream performance for each document serves as its relevance label. We employ various downstream task metrics to obtain document-level annotations and aggregate them using set-based or ranking metrics. Extensive experiments on a wide range of datasets demonstrate that eRAG achieves a higher correlation with downstream RAG performance compared to baseline methods, with improvements in Kendall’s 𝜏 correlation ranging from 0.168 to 0.494. Additionally, eRAG offers significant computational advantages, improving runtime and consuming up to 50 times less GPU memory than end-to-end evaluation.},
	language = {en},
	urldate = {2024-06-20},
	publisher = {arXiv},
	author = {Salemi, Alireza and Zamani, Hamed},
	month = apr,
	year = {2024},
	note = {arXiv:2404.13781 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
}

@misc{sun_utilizing_2019,
	title = {Utilizing {BERT} for {Aspect}-{Based} {Sentiment} {Analysis} via {Constructing} {Auxiliary} {Sentence}},
	url = {http://arxiv.org/abs/1903.09588},
	doi = {10.48550/arXiv.1903.09588},
	abstract = {Aspect-based sentiment analysis (ABSA), which aims to identify fine-grained opinion polarity towards a specific aspect, is a challenging subtask of sentiment analysis (SA). In this paper, we construct an auxiliary sentence from the aspect and convert ABSA to a sentence-pair classification task, such as question answering (QA) and natural language inference (NLI). We fine-tune the pre-trained model from BERT and achieve new state-of-the-art results on SentiHood and SemEval-2014 Task 4 datasets.},
	urldate = {2024-09-12},
	publisher = {arXiv},
	author = {Sun, Chi and Huang, Luyao and Qiu, Xipeng},
	month = mar,
	year = {2019},
	note = {arXiv:1903.09588 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{noauthor_samloweroberta-base-go_emotions_2024,
	title = {{SamLowe}/roberta-base-go\_emotions · {Hugging} {Face}},
	url = {https://huggingface.co/SamLowe/roberta-base-go_emotions},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-09-12},
	month = jan,
	year = {2024},
}

@misc{noauthor_avichrhebert_sentiment_analysis_nodate,
	title = {avichr/{heBERT}\_sentiment\_analysis · {Hugging} {Face}},
	url = {https://huggingface.co/avichr/heBERT_sentiment_analysis},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-09-12},
}

@misc{chase_langgraph_nodate,
	title = {🦜🕸️{LangGraph}},
	url = {https://langchain-ai.github.io/langgraph/},
	urldate = {2024-09-12},
	author = {Chase, Harrison},
}

@misc{noauthor_langchain_nodate,
	title = {{LangChain}},
	url = {https://www.langchain.com/},
	abstract = {LangChain’s suite of products supports developers along each step of their development journey.},
	language = {en},
	urldate = {2024-09-12},
}

@article{jazyah_multimodal_2018,
	title = {Multimodal {Sentiment} {Analysis}: {A} {Comparison} {Study}},
	volume = {14},
	issn = {1549-3636},
	shorttitle = {Multimodal {Sentiment} {Analysis}},
	url = {http://thescipub.com/abstract/10.3844/jcssp.2018.804.818},
	doi = {10.3844/jcssp.2018.804.818},
	abstract = {Sentiments and emotions play a pivotal role in our daily lives. They assist decision making, learning, communication and situation awareness in human environments. Sentiment analysis is mainly focused on the automatic recognition of opinions’ polarity, as positive or negative. Nowadays, sentiment analysis is replacing the old web based survey and traditional survey methods that conducted by deferent companies for finding public opinion about entities like products and services in order to improve their marketing strategy and product of advertisement, at the same time sentiment analysis improves customer service. Large number of videos is being uploaded online every day. Video files contain text, visual and audio features that complement each other. Multimodality is defined by analyzing more than one modality, Multimodal Sentiment Analysis refers to the combination of two or more input models in order to improve the performance of the analysis; a combination of text and audio-visual inputs is an example. The automatic analysis of multimodal opinion involves a deep understanding of natural languages, audio and video processing, whereas researchers are continuing to improve them. This paper focuses on multimodal sentiment analysis as text, audio and video, by giving a complete image of it and related dataset available and providing brief details for each type, in addition to that present the recent trend of researches in the multimodal sentiment analysis and its related fields will be explored.},
	language = {en},
	number = {6},
	urldate = {2024-09-12},
	journal = {Journal of Computer Science},
	author = {Jazyah, Yahia Hasan and Hussien, Intisar O.},
	month = jun,
	year = {2018},
	pages = {804--818},
}

@book{barrett_psychological_2014,
	title = {The {Psychological} {Construction} of {Emotion}},
	isbn = {978-1-4625-1697-1},
	abstract = {This volume presents cutting-edge theory and research on emotions as constructed events rather than fixed, essential entities. It provides a thorough introduction to the assumptions, hypotheses, and scientific methods that embody psychological constructionist approaches. Leading scholars examine the neurobiological, cognitive/perceptual, and social processes that give rise to the experiences Western cultures call sadness, anger, fear, and so on. The book explores such compelling questions as how the brain creates emotional experiences, whether the \&quot;ingredients\&quot; of emotions also give rise to other mental states, and how to define what is or is not an emotion. Introductory and concluding chapters by the editors identify key themes and controversies and compare psychological construction to other theories of emotion.},
	language = {en},
	publisher = {Guilford Publications},
	author = {Barrett, Lisa Feldman and Russell, James A.},
	month = oct,
	year = {2014},
	note = {Google-Books-ID: j361BQAAQBAJ},
	keywords = {Medical / Neuroscience, Psychology / Personality, Psychology / Social Psychology},
}

@article{kojima_large_2022,
	title = {Large {Language} {Models} are {Zero}-{Shot} {Reasoners}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html},
	language = {en},
	urldate = {2024-09-12},
	journal = {Advances in Neural Information Processing Systems},
	author = {Kojima, Takeshi and Gu, Shixiang (Shane) and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
	month = dec,
	year = {2022},
	pages = {22199--22213},
}

@inproceedings{kosti_emotion_2017,
	address = {Honolulu, HI},
	title = {Emotion {Recognition} in {Context}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8099695/},
	doi = {10.1109/CVPR.2017.212},
	abstract = {Understanding what a person is experiencing from her frame of reference is essential in our everyday life. For this reason, one can think that machines with this type of ability would interact better with people. However, there are no current systems capable of understanding in detail people’s emotional states. Previous research on computer vision to recognize emotions has mainly focused on analyzing the facial expression, usually classifying it into the 6 basic emotions [11]. However, the context plays an important role in emotion perception, and when the context is incorporated, we can infer more emotional states. In this paper we present the “Emotions in Context Database” (EMOTIC), a dataset of images containing people in context in non-controlled environments. In these images, people are annotated with 26 emotional categories and also with the continuous dimensions valence, arousal, and dominance [21]. With the EMOTIC dataset, we trained a Convolutional Neural Network model that jointly analyses the person and the whole scene to recognize rich information about emotional states. With this, we show the importance of considering the context for recognizing people’s emotions in images, and provide a benchmark in the task of emotion recognition in visual context.},
	language = {en},
	urldate = {2024-09-12},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Kosti, Ronak and Alvarez, Jose M. and Recasens, Adria and Lapedriza, Agata},
	month = jul,
	year = {2017},
	pages = {1960--1968},
}

@article{noauthor_experiments_nodate,
	title = {Experiments on real-life emotions challenge {Ekman}'s model},
	url = {https://www.nature.com/articles/s41598-023-36201-5},
	urldate = {2024-09-12},
}

@article{ekman_argument_1992,
	title = {An argument for basic emotions},
	volume = {6},
	issn = {0269-9931, 1464-0600},
	url = {https://www.tandfonline.com/doi/full/10.1080/02699939208411068},
	doi = {10.1080/02699939208411068},
	language = {en},
	number = {3-4},
	urldate = {2024-09-12},
	journal = {Cognition and Emotion},
	author = {Ekman, Paul},
	month = may,
	year = {1992},
	pages = {169--200},
}

@book{dalgleish_handbook_2000,
	title = {Handbook of {Cognition} and {Emotion}},
	isbn = {978-0-470-84221-8},
	abstract = {Namhafte Fachleute präsentieren in dieser Monographie neueste Forschungsergebnisse auf den Gebieten der kognitiven Prozesse wie Gedächtnis, Entscheidungsfindung und logisches Denken und der Emotionen wie Ärger, Angst, Trauer und Eifersucht. Neben historischen Hintergründen und Aspekten der philosophischen Debatte werden jeweils auch die Verknüpfungen mit anderen Bereichen der Psychologie dargestellt. (12/98)},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Dalgleish, Tim and Power, Mick},
	month = nov,
	year = {2000},
	note = {Google-Books-ID: vsLvrhohXhAC},
	keywords = {Psychology / Clinical Psychology, Psychology / General},
}

@article{ekman_constants_1971,
	title = {Constants across cultures in the face and emotion},
	volume = {17},
	issn = {1939-1315},
	doi = {10.1037/h0030377},
	abstract = {Investigated the question of whether any facial expressions of emotion are universal. Recent studies showing that members of literate cultures associated the same emotion concepts with the same facial behaviors could not demonstrate that at least some facial expressions of emotion are universal; the cultures compared had all been exposed to some of the same mass media presentations of facial expression, and these may have taught the people in each culture to recognize the unique facial expressions of other cultures. To show that members of a preliterate culture who had minimal exposure to literate cultures would associate the same emotion concepts with the same facial behaviors as do members of Western and Eastern literate cultures, data were gathered in New Guinea by telling 342 Ss a story, showing them a set of 3 faces, and asking them to select the face which showed the emotion appropriate to the story. Ss were members of the Fore linguistic-cultural group, which up until 12 yr. ago was an isolated, Neolithic, material culture. Results provide evidence in support of the hypothesis. (30 ref.) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {2},
	journal = {Journal of Personality and Social Psychology},
	author = {Ekman, Paul and Friesen, Wallace V.},
	year = {1971},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Countries, Culture (Anthropological), Emotions, Social Perception},
	pages = {124--129},
}

@article{majumder_dialoguernn_2019,
	title = {{DialogueRNN}: {An} {Attentive} {RNN} for {Emotion} {Detection} in {Conversations}},
	volume = {33},
	copyright = {https://www.aaai.org},
	issn = {2374-3468, 2159-5399},
	shorttitle = {{DialogueRNN}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4657},
	doi = {10.1609/aaai.v33i01.33016818},
	abstract = {Emotion detection in conversations is a necessary step for a number of applications, including opinion mining over chat history, social media threads, debates, argumentation mining, understanding consumer feedback in live conversations, and so on. Currently systems do not treat the parties in the conversation individually by adapting to the speaker of each utterance. In this paper, we describe a new method based on recurrent neural networks that keeps track of the individual party states throughout the conversation and uses this information for emotion classification. Our model outperforms the state-of-the-art by a significant margin on two different datasets.},
	number = {01},
	urldate = {2024-09-12},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Majumder, Navonil and Poria, Soujanya and Hazarika, Devamanyu and Mihalcea, Rada and Gelbukh, Alexander and Cambria, Erik},
	month = jul,
	year = {2019},
	pages = {6818--6825},
}

@article{poria_multimodal_2018,
	title = {Multimodal {Sentiment} {Analysis}: {Addressing} {Key} {Issues} and {Setting} {Up} the {Baselines}},
	volume = {33},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1541-1672, 1941-1294},
	shorttitle = {Multimodal {Sentiment} {Analysis}},
	url = {https://ieeexplore.ieee.org/document/8636432/},
	doi = {10.1109/MIS.2018.2882362},
	language = {en},
	number = {6},
	urldate = {2024-09-12},
	journal = {IEEE Intelligent Systems},
	author = {Poria, Soujanya and Majumder, Navonil and Hazarika, Devamanyu and Cambria, Erik and Gelbukh, Alexander and Hussain, Amir},
	month = nov,
	year = {2018},
	pages = {17--25},
}

@article{barrett_experience_2007,
	title = {The {Experience} of {Emotion}},
	volume = {58},
	issn = {0066-4308, 1545-2085},
	url = {https://www.annualreviews.org/content/journals/10.1146/annurev.psych.58.110405.085709},
	doi = {10.1146/annurev.psych.58.110405.085709},
	abstract = {Abstract Experiences of emotion are content-rich events that emerge at the level of psychological description, but must be causally constituted by neurobiological processes. This chapter outlines an emerging scientific agenda for understanding what these experiences feel like and how they arise. We review the available answers to what is felt (i.e., the content that makes up an experience of emotion) and how neurobiological processes instantiate these properties of experience. These answers are then integrated into a broad framework that describes, in psychological terms, how the experience of emotion emerges from more basic processes. We then discuss the role of such experiences in the economy of the mind and behavior.},
	language = {en},
	number = {Volume 58, 2007},
	urldate = {2024-09-12},
	journal = {Annual Review of Psychology},
	author = {Barrett, Lisa Feldman and Mesquita, Batja and Ochsner, Kevin N. and Gross, James J.},
	month = jan,
	year = {2007},
	note = {Publisher: Annual Reviews},
	pages = {373--403},
}

@misc{noauthor_lmsys_nodate,
	title = {{LMSys} {Chatbot} {Arena} {Leaderboard} - a {Hugging} {Face} {Space} by lmsys},
	url = {https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard},
	abstract = {Discover amazing ML apps made by the community},
	urldate = {2024-09-05},
}

@misc{shanahan_role-play_2023,
	title = {Role-{Play} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2305.16367},
	doi = {10.48550/arXiv.2305.16367},
	abstract = {As dialogue agents become increasingly human-like in their performance, it is imperative that we develop effective ways to describe their behaviour in high-level terms without falling into the trap of anthropomorphism. In this paper, we foreground the concept of role-play. Casting dialogue agent behaviour in terms of role-play allows us to draw on familiar folk psychological terms, without ascribing human characteristics to language models they in fact lack. Two important cases of dialogue agent behaviour are addressed this way, namely (apparent) deception and (apparent) self-awareness.},
	urldate = {2024-08-30},
	publisher = {arXiv},
	author = {Shanahan, Murray and McDonell, Kyle and Reynolds, Laria},
	month = may,
	year = {2023},
	note = {arXiv:2305.16367 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{kong_better_2024,
	title = {Better {Zero}-{Shot} {Reasoning} with {Role}-{Play} {Prompting}},
	url = {http://arxiv.org/abs/2308.07702},
	doi = {10.48550/arXiv.2308.07702},
	abstract = {Modern large language models (LLMs) exhibit a remarkable capacity for role-playing, enabling them to embody not only human characters but also non-human entities. This versatility allows them to simulate complex human-like interactions and behaviors within various contexts, as well as to emulate specific objects or systems. While these capabilities have enhanced user engagement and introduced novel modes of interaction, the influence of role-playing on LLMs' reasoning abilities remains underexplored. In this study, we introduce a strategically designed role-play prompting methodology and assess its performance under the zero-shot setting across twelve diverse reasoning benchmarks. Our empirical results illustrate that role-play prompting consistently surpasses the standard zero-shot approach across most datasets. Notably, in experiments conducted using ChatGPT, accuracy on AQuA rises from 53.5\% to 63.8\%, and on Last Letter from 23.8\% to 84.2\%.Upon further comparison with the Zero-Shot-CoT technique, which prompts the model to "think step by step", our study demonstrates that role-play prompting acts as a more effective trigger for the CoT process. This highlights its potential to augment the reasoning capabilities of LLMs. We release our code at https://github.com/NKU-HLT/Role-Play-Prompting.},
	urldate = {2024-08-29},
	publisher = {arXiv},
	author = {Kong, Aobo and Zhao, Shiwan and Chen, Hao and Li, Qicheng and Qin, Yong and Sun, Ruiqi and Zhou, Xin and Wang, Enzhi and Dong, Xiaohang},
	month = mar,
	year = {2024},
	note = {arXiv:2308.07702 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{noauthor_sign_nodate,
	title = {Sign in to {MSV}},
	url = {https://login.rewe-group.com/auth/realms/msv2/protocol/openid-connect/auth?scope=openid+profile+email+rewe-all-in&state=k3UKxvZmFGpsQYpCY29TUXY4e4siiKM5QwrxhIif3UE.kx4izMG-osA.ANKSUwU1QlWdJcbFjvPxjw&response_type=code&client_id=iam-keycloak-realm&redirect_uri=https%3A%2F%2Flogin.rewe-group.com%2Fauth%2Frealms%2Fglobal%2Fbroker%2Frewe-digi-msv2%2Fendpoint&nonce=c_OKPy7tPAVt4CSfL-njug},
	urldate = {2024-08-29},
}

@misc{noauthor_reasoning_nodate,
	title = {Reasoning techniques - {Improving} {LLM}’s {Reasoning} {In} {Production} - {The} {Structured} {Approach} {Artificial} {Intelligence} and {MLOps} {Consulting} company focused on increasing revenue.},
	url = {https://mlopsaudits.com/blog/improving-llms-reasoning-in-production-the-structured-approach},
	abstract = {This guide on achieving better reasoning performance of LLMs intends to complement theguide on prompt engineering by programmatic and systematic to increase flexibility while keeping the amount of operational variability to a minimum.},
	urldate = {2024-08-28},
	keywords = {reasoning},
}

@misc{noauthor_how_nodate,
	title = {How to {System} {Prompts} with {Claude}},
	url = {https://docs.anthropic.com/en/release-notes/system-prompts},
	abstract = {See updates to the default system prompt for text-based conversations on [Claude.ai](https://www.claude.ai) and the Claude [iOS](http://anthropic.com/ios) and [Android](http://anthropic.com/android) apps.},
	language = {en},
	urldate = {2024-08-28},
	journal = {Anthropic},
}

@misc{lee_language_2024,
	title = {Language {Models} {Show} {Stable} {Value} {Orientations} {Across} {Diverse} {Role}-{Plays}},
	url = {http://arxiv.org/abs/2408.09049},
	abstract = {We demonstrate that large language models (LLMs) exhibit consistent value orientations despite adopting diverse personas, revealing a persistent inertia in their responses that remains stable across the variety of roles they are prompted to assume. To systematically explore this phenomenon, we introduce the role-playat-scale methodology, which involves prompting LLMs with randomized, diverse personas and analyzing the macroscopic trend of their responses. Unlike previous works that simply feed these questions to LLMs as if testing human subjects, our role-play-at-scale methodology diagnoses inherent tendencies in a systematic and scalable manner by: (1) prompting the model to act in different random personas and (2) asking the same question multiple times for each random persona. This approach reveals consistent patterns in LLM responses across diverse role-play scenarios, indicating deeply encoded inherent tendencies. Our findings contribute to the discourse on value alignment in foundation models and demonstrate the efficacy of role-play-at-scale as a diagnostic tool for uncovering encoded biases in LLMs. Our research code is at github.com/brucewlee/moral-value-bias.},
	language = {en},
	urldate = {2024-08-21},
	publisher = {arXiv},
	author = {Lee, Bruce W. and Lee, Yeongheon and Cho, Hyunsoo},
	month = aug,
	year = {2024},
	note = {arXiv:2408.09049 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
}

@misc{lim_large_2024,
	title = {Large {Language} {Model}-based {Role}-{Playing} for {Personalized} {Medical} {Jargon} {Extraction}},
	url = {http://arxiv.org/abs/2408.05555},
	abstract = {Objective: Studies reveal that Electronic Health Records (EHR), which have been widely adopted in the U.S. to allow patients to access their personal medical information, do not have high readability to patients due to the prevalence of medical jargon. Tailoring medical notes to individual comprehension by identifying jargon that is difficult for each person will enhance the utility of generative models. In this study, we investigate whether role-playing in a large language model, ChatGPT, can improve performance on customized medical term extraction based on sociodemographic groups. Materials and Methods: We gathered data on extracting medical terms from 270 Mechanical Turk workers, two open-source biomedical NER systems (SciSpacy and MedJEx), and ChatGPT, using 20 sentences selected from medical notes. Each medical term was labeled based on the consensus of the 270 Mechanical Turk respondents. The performance of ChatGPT in extracting medical terms was compared to that of earlier models. Ultimately, we assessed the effect of role-playing with and without in-context learning in ChatGPT quantitatively with 14 different socio-demographic groups and varying temperatures between 0.0 and 1.0 and calculated the F1 scores.
Results: Without in-context learning, F1 scores improved in 133 out of 140 cases with role-playing. Wilcoxon signed-rank test further shows that the F1 scores follow distinct distributions. The application of in-context learning, with four examples each, resulted in significant performance improvements while being less affected by role-playing. Particularly, GPT4, with in-context learning and role-playing, gave a macro-averaged F1 score of 51.28, showing better performance than the previous state-of-the-art model, MedJEx, which scored 50.92.
Conclusion and Future Work: The results show that role-playing in ChatGPT can give personalized results for medical term extraction. We intend to explore experiments further to harness the potential of role-playing in ChatGPT for practical applications such as EHR note personalization, medical concept linking systems, or chatbot-based selfdiagnosis systems.},
	language = {en},
	urldate = {2024-08-21},
	publisher = {arXiv},
	author = {Lim, Jung Hoon and Kwon, Sunjae and Yao, Zonghai and Lalor, John P. and Yu, Hong},
	month = aug,
	year = {2024},
	note = {arXiv:2408.05555 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{thelwall_evaluating_nodate,
	title = {Evaluating {Research} {Quality} with {Large} {Language} {Models}: {An} {Analysis} of {ChatGPT}’s {Effectiveness} with {Different} {Settings} and {Inputs}},
	language = {en},
	author = {Thelwall, Mike},
}

@misc{yu_beyond_2024,
	title = {{BEYOND} {DIALOGUE}: {A} {Profile}-{Dialogue} {Alignment} {Framework} {Towards} {General} {Role}-{Playing} {Language} {Model}},
	shorttitle = {{BEYOND} {DIALOGUE}},
	url = {http://arxiv.org/abs/2408.10903},
	abstract = {The rapid advancement of large language models (LLMs) has revolutionized role-playing, enabling the development of general role-playing models. However, current role-playing training has two significant issues: (I) Using a predefined role profile to prompt dialogue training for specific scenarios usually leads to inconsistencies and even conflicts between the dialogue and the profile, resulting in training biases. (II) The model learns to imitate the role based solely on the profile, neglecting profile-dialogue alignment at the sentence level. In this work, we propose a simple yet effective framework called BEYOND DIALOGUE, designed to overcome these hurdles. This framework innovatively introduces “beyond dialogue” tasks to align dialogue with profile traits based on each specific scenario, thereby eliminating biases during training. Furthermore, by adopting an innovative prompting mechanism that generates reasoning outcomes for training, the framework allows the model to achieve finegrained alignment between profile and dialogue at the sentence level. The aforementioned methods are fully automated and low-cost. Additionally, the integration of automated dialogue and objective evaluation methods forms a comprehensive framework, paving the way for general role-playing. Experimental results demonstrate that our model excels in adhering to and reflecting various dimensions of role profiles, outperforming most proprietary general and specialized role-playing baselines. All code and datasets are available at https://github.com/yuyouyu32/BeyondDialogue.},
	language = {en},
	urldate = {2024-08-21},
	publisher = {arXiv},
	author = {Yu, Yeyong and Yu, Rusheng and Wei, Haojie and Zhang, Zhanqiu and Qian, Quan},
	month = aug,
	year = {2024},
	note = {arXiv:2408.10903 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
}

@misc{herbold_large_2024,
	title = {Large {Language} {Models} can impersonate politicians and other public figures},
	url = {http://arxiv.org/abs/2407.12855},
	doi = {10.48550/arXiv.2407.12855},
	abstract = {Modern AI technology like Large language models (LLMs) has the potential to pollute the public information sphere with made-up content, which poses a significant threat to the cohesion of societies at large. A wide range of research has shown that LLMs are capable of generating text of impressive quality, including persuasive political speech, text with a pre-defined style, and role-specific content. But there is a crucial gap in the literature: We lack large-scale and systematic studies of how capable LLMs are in impersonating political and societal representatives and how the general public judges these impersonations in terms of authenticity, relevance and coherence. We present the results of a study based on a cross-section of British society that shows that LLMs are able to generate responses to debate questions that were part of a broadcast political debate programme in the UK. The impersonated responses are judged to be more authentic and relevant than the original responses given by people who were impersonated. This shows two things: (1) LLMs can be made to contribute meaningfully to the public political debate and (2) there is a dire need to inform the general public of the potential harm this can have on society.},
	urldate = {2024-08-21},
	publisher = {arXiv},
	author = {Herbold, Steffen and Trautsch, Alexander and Kikteva, Zlata and Hautli-Janisz, Annette},
	month = jul,
	year = {2024},
	note = {arXiv:2407.12855 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{novikova_why_2017,
	title = {Why {We} {Need} {New} {Evaluation} {Metrics} for {NLG}},
	url = {http://arxiv.org/abs/1707.06875},
	doi = {10.18653/v1/D17-1237},
	abstract = {The majority of NLG evaluation relies on automatic metrics, such as BLEU . In this paper, we motivate the need for novel, system- and data-independent automatic evaluation methods: We investigate a wide range of metrics, including state-of-the-art word-based and novel grammar-based ones, and demonstrate that they only weakly reflect human judgements of system outputs as generated by data-driven, end-to-end NLG. We also show that metric performance is data- and system-specific. Nevertheless, our results also suggest that automatic metrics perform reliably at system-level and can support system development by finding cases where a system performs poorly.},
	urldate = {2024-08-21},
	author = {Novikova, Jekaterina and Dušek, Ondřej and Curry, Amanda Cercas and Rieser, Verena},
	month = jul,
	year = {2017},
	note = {arXiv:1707.06875 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{noauthor_planning_2024,
	title = {Planning for {Agents}},
	url = {https://blog.langchain.dev/planning-for-agents/},
	abstract = {The fourth installment in our "In the Loop Series," in which we talk about what planning means for an agent and how to improve it.},
	language = {en},
	urldate = {2024-08-19},
	journal = {LangChain Blog},
	month = jul,
	year = {2024},
	keywords = {proposal},
}

@misc{noauthor_what_2024,
	title = {What is a "cognitive architecture"?},
	url = {https://blog.langchain.dev/what-is-a-cognitive-architecture/},
	abstract = {The second installment in our "In the Loop" series, focusing on what cognitive architecture means.},
	language = {en},
	urldate = {2024-08-19},
	journal = {LangChain Blog},
	month = jul,
	year = {2024},
	keywords = {explanable ai},
}

@misc{noauthor_berkeley_nodate,
	title = {Berkeley {Function} {Calling} {Leaderboard} (aka {Berkeley} {Tool} {Calling} {Leaderboard})},
	url = {https://gorilla.cs.berkeley.edu/leaderboard.html?ref=blog.langchain.dev},
	urldate = {2024-08-19},
	keywords = {Evaluation},
}

@misc{noauthor_linkedin_nodate,
	title = {{LinkedIn} {Login}, {Sign} in},
	url = {https://www.linkedin.com},
	abstract = {Login to LinkedIn to keep in touch with people you know, share ideas, and build your career.},
	language = {en-US},
	urldate = {2024-08-01},
	journal = {LinkedIn},
}

@misc{rabbit_ai_2024,
	title = {{AI} {In} {Games}: {Complicated} {Characters}},
	shorttitle = {{AI} {In} {Games}},
	url = {https://medium.com/curiouserinstitute/ai-in-games-complicated-characters-6c47797d997a},
	abstract = {Connecting LLM Driven Characters To The Game State},
	language = {en},
	urldate = {2024-08-01},
	journal = {curiouserinstitute},
	author = {Rabbit, Rabbit},
	month = jan,
	year = {2024},
}

@misc{shao_character-llm_2023,
	title = {Character-{LLM}: {A} {Trainable} {Agent} for {Role}-{Playing}},
	shorttitle = {Character-{LLM}},
	url = {http://arxiv.org/abs/2310.10158},
	doi = {10.48550/arXiv.2310.10158},
	abstract = {Large language models (LLMs) can be used to serve as agents to simulate human behaviors, given the powerful ability to understand human instructions and provide high-quality generated texts. Such ability stimulates us to wonder whether LLMs can simulate a person in a higher form than simple human behaviors. Therefore, we aim to train an agent with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API. In this work, we introduce Character-LLM that teach LLMs to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar, etc. Our method focuses on editing profiles as experiences of a certain character and training models to be personal simulacra with these experiences. To assess the effectiveness of our approach, we build a test playground that interviews trained agents and evaluates whether the agents {\textbackslash}textit\{memorize\} their characters and experiences. Experimental results show interesting observations that help build future simulacra of humankind.},
	urldate = {2024-08-01},
	publisher = {arXiv},
	author = {Shao, Yunfan and Li, Linyang and Dai, Junqi and Qiu, Xipeng},
	month = dec,
	year = {2023},
	note = {arXiv:2310.10158 [cs]
version: 2},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{liao_towards_2024,
	title = {Towards {Automated} {Data} {Sciences} with {Natural} {Language} and {SageCopilot}: {Practices} and {Lessons} {Learned}},
	shorttitle = {Towards {Automated} {Data} {Sciences} with {Natural} {Language} and {SageCopilot}},
	url = {http://arxiv.org/abs/2407.21040},
	doi = {10.48550/arXiv.2407.21040},
	abstract = {While the field of NL2SQL has made significant advancements in translating natural language instructions into executable SQL scripts for data querying and processing, achieving full automation within the broader data science pipeline - encompassing data querying, analysis, visualization, and reporting - remains a complex challenge. This study introduces SageCopilot, an advanced, industry-grade system system that automates the data science pipeline by integrating Large Language Models (LLMs), Autonomous Agents (AutoAgents), and Language User Interfaces (LUIs). Specifically, SageCopilot incorporates a two-phase design: an online component refining users' inputs into executable scripts through In-Context Learning (ICL) and running the scripts for results reporting \& visualization, and an offline preparing demonstrations requested by ICL in the online phase. A list of trending strategies such as Chain-of-Thought and prompt-tuning have been used to augment SageCopilot for enhanced performance. Through rigorous testing and comparative analysis against prompt-based solutions, SageCopilot has been empirically validated to achieve superior end-to-end performance in generating or executing scripts and offering results with visualization, backed by real-world datasets. Our in-depth ablation studies highlight the individual contributions of various components and strategies used by SageCopilot to the end-to-end correctness for data sciences.},
	urldate = {2024-08-01},
	publisher = {arXiv},
	author = {Liao, Yuan and Bian, Jiang and Yun, Yuhui and Wang, Shuo and Zhang, Yubo and Chu, Jiaming and Wang, Tao and Li, Kewei and Li, Yuchen and Li, Xuhong and Ji, Shilei and Xiong, Haoyi},
	month = jul,
	year = {2024},
	note = {arXiv:2407.21040 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Databases, Computer Science - Software Engineering, use case},
}

@misc{wang_plan-and-solve_2023,
	title = {Plan-and-{Solve} {Prompting}: {Improving} {Zero}-{Shot} {Chain}-of-{Thought} {Reasoning} by {Large} {Language} {Models}},
	shorttitle = {Plan-and-{Solve} {Prompting}},
	url = {http://arxiv.org/abs/2305.04091},
	doi = {10.48550/arXiv.2305.04091},
	abstract = {Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual effort, Zero-shot-CoT concatenates the target problem statement with "Let's think step by step" as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.},
	urldate = {2024-06-14},
	publisher = {arXiv},
	author = {Wang, Lei and Xu, Wanyu and Lan, Yihuai and Hu, Zhiqiang and Lan, Yunshi and Lee, Roy Ka-Wei and Lim, Ee-Peng},
	month = may,
	year = {2023},
	note = {arXiv:2305.04091 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{han_llm_2024,
	title = {{LLM} {Multi}-{Agent} {Systems}: {Challenges} and {Open} {Problems}},
	shorttitle = {{LLM} {Multi}-{Agent} {Systems}},
	url = {http://arxiv.org/abs/2402.03578},
	abstract = {This paper explores existing works of multi-agent systems and identify challenges that remain inadequately addressed. By leveraging the diverse capabilities and roles of individual agents within a multi-agent system, these systems can tackle complex tasks through collaboration. We discuss optimizing task allocation, fostering robust reasoning through iterative debates, managing complex and layered context information, and enhancing memory management to support the intricate interactions within multi-agent systems. We also explore the potential application of multi-agent systems in blockchain systems to shed light on their future development and application in real-world distributed systems. multi-agent systems far exceeds the progress made to date.},
	language = {en},
	urldate = {2024-05-08},
	publisher = {arXiv},
	author = {Han, Shanshan and Zhang, Qifan and Yao, Yuhang and Jin, Weizhao and Xu, Zhaozhuo and He, Chaoyang},
	month = feb,
	year = {2024},
	note = {arXiv:2402.03578 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems, Multi-Agent},
}

@misc{beurer-kellner_prompt_2023,
	title = {Prompt {Sketching} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2311.04954},
	doi = {10.48550/arXiv.2311.04954},
	abstract = {Many recent prompting strategies for large language models (LLMs) query the model multiple times sequentially -- first to produce intermediate results and then the final answer. However, using these methods, both decoder and model are unaware of potential follow-up prompts, leading to disconnected and undesirably wordy intermediate responses. In this work, we address this issue by proposing prompt sketching, a new prompting paradigm in which an LLM does not only respond by completing a prompt, but by predicting values for multiple variables in a template. This way, sketching grants users more control over the generation process, e.g., by providing a reasoning framework via intermediate instructions, leading to better overall results. The key idea enabling sketching with existing, autoregressive models is to adapt the decoding procedure to also score follow-up instructions during text generation, thus optimizing overall template likelihood in inference. Our experiments show that in a zero-shot setting, prompt sketching outperforms existing, sequential prompting schemes such as direct asking or chain-of-thought on 7 out of 8 LLM benchmarking tasks, including state tracking, arithmetic reasoning, and general question answering. To facilitate future use, we release a number of generic, yet effective sketches applicable to many tasks, and an open source library called dclib, powering our sketch-aware decoders.},
	urldate = {2024-05-16},
	publisher = {arXiv},
	author = {Beurer-Kellner, Luca and Müller, Mark Niklas and Fischer, Marc and Vechev, Martin},
	month = nov,
	year = {2023},
	note = {arXiv:2311.04954 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, LMQL, guided generation},
}

@misc{yang_buffer_2024,
	title = {Buffer of {Thoughts}: {Thought}-{Augmented} {Reasoning} with {Large} {Language} {Models}},
	shorttitle = {Buffer of {Thoughts}},
	url = {http://arxiv.org/abs/2406.04271},
	doi = {10.48550/arXiv.2406.04271},
	abstract = {We introduce Buffer of Thoughts (BoT), a novel and versatile thought-augmented reasoning approach for enhancing accuracy, efficiency and robustness of large language models (LLMs). Specifically, we propose meta-buffer to store a series of informative high-level thoughts, namely thought-template, distilled from the problem-solving processes across various tasks. Then for each problem, we retrieve a relevant thought-template and adaptively instantiate it with specific reasoning structures to conduct efficient reasoning. To guarantee the scalability and stability, we further propose buffer-manager to dynamically update the meta-buffer, thus enhancing the capacity of meta-buffer as more tasks are solved. We conduct extensive experiments on 10 challenging reasoning-intensive tasks, and achieve significant performance improvements over previous SOTA methods: 11\% on Game of 24, 20\% on Geometric Shapes and 51\% on Checkmate-in-One. Further analysis demonstrate the superior generalization ability and model robustness of our BoT, while requiring only 12\% of the cost of multi-query prompting methods (e.g., tree/graph of thoughts) on average. Notably, we find that our Llama3-8B+BoT has the potential to surpass Llama3-70B model. Our project is available at: https://github.com/YangLing0818/buffer-of-thought-llm},
	urldate = {2024-06-14},
	publisher = {arXiv},
	author = {Yang, Ling and Yu, Zhaochen and Zhang, Tianjun and Cao, Shiyi and Xu, Minkai and Zhang, Wentao and Gonzalez, Joseph E. and Cui, Bin},
	month = jun,
	year = {2024},
	note = {arXiv:2406.04271 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2024-06-06},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{noauthor_what_2024-1,
	title = {What is a long context window?},
	url = {https://blog.google/technology/ai/long-context-window-ai-models/},
	abstract = {Gemini 1.5 Pro brings big improvements to speed and efficiency, but one of its innovations is its long context window, which measures how many tokens that the model can …},
	language = {en-us},
	urldate = {2024-06-06},
	journal = {Google},
	month = feb,
	year = {2024},
}

@misc{noauthor_big_2024,
	title = {The {Big} {Benchmarks} {Collection} - a open-llm-leaderboard {Collection}},
	url = {https://huggingface.co/collections/open-llm-leaderboard/the-big-benchmarks-collection-64faca6335a7fc7d4ffe974a},
	abstract = {Gathering benchmark spaces on the hub (beyond the Open LLM Leaderboard)},
	urldate = {2024-06-06},
	month = may,
	year = {2024},
}

@book{klapach_comparative_2023,
	title = {The {Comparative} {Emotional} {Capabilities} of {Five} {Popular} {Large} {Language} {Models}},
	author = {Klapach, Nathan},
	month = oct,
	year = {2023},
	doi = {10.58445/rars.645},
}

@inproceedings{devlin_bert_2019,
	address = {Minneapolis, Minnesota},
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {https://aclanthology.org/N19-1423},
	doi = {10.18653/v1/N19-1423},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2024-06-05},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
	month = jun,
	year = {2019},
	pages = {4171--4186},
}

@misc{howard_universal_2018,
	title = {Universal {Language} {Model} {Fine}-tuning for {Text} {Classification}},
	url = {http://arxiv.org/abs/1801.06146},
	doi = {10.48550/arXiv.1801.06146},
	abstract = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24\% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.},
	urldate = {2024-06-05},
	publisher = {arXiv},
	author = {Howard, Jeremy and Ruder, Sebastian},
	month = may,
	year = {2018},
	note = {arXiv:1801.06146 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{abburi_generative_2023,
	title = {Generative {AI} {Text} {Classification} using {Ensemble} {LLM} {Approaches}},
	url = {http://arxiv.org/abs/2309.07755},
	doi = {10.48550/arXiv.2309.07755},
	abstract = {Large Language Models (LLMs) have shown impressive performance across a variety of Artificial Intelligence (AI) and natural language processing tasks, such as content creation, report generation, etc. However, unregulated malign application of these models can create undesirable consequences such as generation of fake news, plagiarism, etc. As a result, accurate detection of AI-generated language can be crucial in responsible usage of LLMs. In this work, we explore 1) whether a certain body of text is AI generated or written by human, and 2) attribution of a specific language model in generating a body of text. Texts in both English and Spanish are considered. The datasets used in this study are provided as part of the Automated Text Identification (AuTexTification) shared task. For each of the research objectives stated above, we propose an ensemble neural model that generates probabilities from different pre-trained LLMs which are used as features to a Traditional Machine Learning (TML) classifier following it. For the first task of distinguishing between AI and human generated text, our model ranked in fifth and thirteenth place (with macro \$F1\$ scores of 0.733 and 0.649) for English and Spanish texts, respectively. For the second task on model attribution, our model ranked in first place with macro \$F1\$ scores of 0.625 and 0.653 for English and Spanish texts, respectively.},
	urldate = {2024-06-05},
	publisher = {arXiv},
	author = {Abburi, Harika and Suesserman, Michael and Pudota, Nirmala and Veeramani, Balaji and Bowen, Edward and Bhattacharya, Sanmitra},
	month = sep,
	year = {2023},
	note = {arXiv:2309.07755 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{venable_comprehensive_2012,
	address = {Berlin, Heidelberg},
	title = {A {Comprehensive} {Framework} for {Evaluation} in {Design} {Science} {Research}},
	isbn = {978-3-642-29863-9},
	doi = {10.1007/978-3-642-29863-9_31},
	abstract = {Evaluation is a central and essential activity in conducting rigorous Design Science Research (DSR), yet there is surprisingly little guidance about designing the DSR evaluation activity beyond suggesting possible methods that could be used for evaluation. This paper extends the notable exception of the existing framework of Pries-Heje et al [11] to address this problem. The paper proposes an extended DSR evaluation framework together with a DSR evaluation design method that can guide DSR researchers in choosing an appropriate strategy for evaluation of the design artifacts and design theories that form the output from DSR. The extended DSR evaluation framework asks the DSR researcher to consider (as input to the choice of the DSR evaluation strategy) contextual factors of goals, conditions, and constraints on the DSR evaluation, e.g. the type and level of desired rigor, the type of artifact, the need to support formative development of the designed artifacts, the properties of the artifact to be evaluated, and the constraints on resources available, such as time, labor, facilities, expertise, and access to research subjects. The framework and method support matching these in the first instance to one or more DSR evaluation strategies, including the choice of ex ante (prior to artifact construction) versus ex post evaluation (after artifact construction) and naturalistic (e.g., field setting) versus artificial evaluation (e.g., laboratory setting). Based on the recommended evaluation strategy(ies), guidance is provided concerning what methodologies might be appropriate within the chosen strategy(ies).},
	language = {en},
	booktitle = {Design {Science} {Research} in {Information} {Systems}. {Advances} in {Theory} and {Practice}},
	publisher = {Springer},
	author = {Venable, John and Pries-Heje, Jan and Baskerville, Richard},
	editor = {Peffers, Ken and Rothenberger, Marcus and Kuechler, Bill},
	year = {2012},
	keywords = {Design Science Research, Evaluation Method, Evaluation Strategy, Information Systems Evaluation, Research Methodology},
	pages = {423--438},
}

@incollection{hevner_design_2010,
	address = {Boston, MA},
	title = {Design {Science} {Research} in {Information} {Systems}},
	isbn = {978-1-4419-5653-8},
	url = {https://doi.org/10.1007/978-1-4419-5653-8_2},
	abstract = {Design activities are central to most applied disciplines. Research in design has a long history in many fields including architecture, engineering, education, psychology, and the fine arts (Cross 2001). The computing and information technology (CIT) field since its advent in the late 1940s has appropriated many of the ideas, concepts, and methods of design science that have originated in these other disciplines. However, information systems (IS) as composed of inherently mutable and adaptable hardware, software, and human interfaces provide many unique and challenging design problems that call for new and creative ideas.},
	language = {en},
	urldate = {2024-06-05},
	booktitle = {Design {Research} in {Information} {Systems}: {Theory} and {Practice}},
	publisher = {Springer US},
	author = {Hevner, Alan and Chatterjee, Samir},
	editor = {Hevner, Alan and Chatterjee, Samir},
	year = {2010},
	doi = {10.1007/978-1-4419-5653-8_2},
	pages = {9--22},
}

@article{fan_review_2021,
	title = {A {Review} on {Data} {Preprocessing} {Techniques} {Toward} {Efficient} and {Reliable} {Knowledge} {Discovery} {From} {Building} {Operational} {Data}},
	volume = {9},
	issn = {2296-598X},
	url = {https://www.frontiersin.org/articles/10.3389/fenrg.2021.652801},
	doi = {10.3389/fenrg.2021.652801},
	abstract = {The rapid development in data science and the increasing availability of building operational data have provided great opportunities for developing data-driven solutions for intelligent building energy management. Data preprocessing serves as the foundation for valid data analyses. It is an indispensable step in building operational data analysis considering the intrinsic complexity of building operations and deficiencies in data quality. Data preprocessing refers to a set of techniques for enhancing the quality of the raw data, such as outlier removal and missing value imputation. This article serves as a comprehensive review of data preprocessing techniques for analysing massive building operational data. A wide variety of data preprocessing techniques are summarised in terms of their applications in missing value imputation, outlier detection, data reduction, data scaling, data transformation, and data partitioning. In addition, three state-of-the-art data science techniques are proposed to tackle practical data challenges in the building field, i.e., data augmentation, transfer learning, and semi-supervised learning. In-depth discussions have been presented to present the pros and cons of existing preprocessing methods, possible directions for future research and potential applications in smart building energy management. The insights obtained are helpful for the development of data-driven research in the building field.},
	language = {English},
	urldate = {2024-06-05},
	journal = {Frontiers in Energy Research},
	author = {Fan, Cheng and Chen, Meiling and Wang, Xinghua and Wang, Jiayuan and Huang, Bufu},
	month = mar,
	year = {2021},
	note = {Publisher: Frontiers},
	keywords = {Building energy management, Building operational data analysis, Data Preprocessing, data science, knowledge discovery},
}

@misc{noauthor_researchgate_nodate,
	title = {{ResearchGate}},
	url = {https://www.researchgate.net/publication/201168946_Design_Science_in_Information_Systems_Research/link/5405d4670cf23d9765a75fc2/download?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6InB1YmxpY2F0aW9uIiwicGFnZSI6InB1YmxpY2F0aW9uIn19},
	urldate = {2024-06-05},
}

@article{gehman_finding_2018,
	title = {Finding {Theory}-{Method} {Fit}: {A} {Comparison} of {Three} {Qualitative} {Approaches} to {Theory} {Building}},
	volume = {27},
	shorttitle = {Finding {Theory}-{Method} {Fit}},
	doi = {10.1177/1056492617706029},
	abstract = {This article, together with a companion video, provides a synthesized summary of a Showcase Symposium held at the 2016 Academy of Management Annual Meeting in which prominent scholars—Denny Gioia, Kathy Eisenhardt, Ann Langley and Kevin Corley—discussed different approaches to theory building with qualitative research. Our goal for the symposium was to increase management scholars’ sensitivity to the importance of theory-method “fit” in qualitative research. We have integrated the panelists’ prepared remarks and interactive discussion into three sections: an introduction by each scholar, who articulates their own approach to qualitative research; their personal reflections on the similarities and differences between approaches to qualitative research, and answers to general questions posed by the audience during the symposium. We conclude by summarizing insights gleaned from the symposium about important distinctions among these three qualitative research approaches and their appropriate usages.

The companion video is available on YouTube: https://youtu.be/\_JdOSCzSpMc},
	journal = {Journal of Management Inquiry},
	author = {Gehman, Joel and Glaser, Vern and Eisenhardt, Kathleen and Gioia, Dennis and Langley, Ann and Corley, Kevin},
	month = jul,
	year = {2018},
	pages = {284--300},
}

@article{peffers_design_2007,
	title = {A {Design} {Science} {Research} {Methodology} for {Information} {Systems} {Research}},
	volume = {24},
	issn = {0742-1222},
	url = {https://doi.org/10.2753/MIS0742-1222240302},
	doi = {10.2753/MIS0742-1222240302},
	abstract = {The paper motivates, presents, demonstrates in use, and evaluates a methodology for conducting design science (DS) research in information systems (IS). DS is of importance in a discipline oriented to the creation of successful artifacts. Several researchers have pioneered DS research in IS, yet over the past 15 years, little DS research has been done within the discipline. The lack of a methodology to serve as a commonly accepted framework for DS research and of a template for its presentation may have contributed to its slow adoption. The design science research methodology (DSRM) presented here incorporates principles, practices, and procedures required to carry out such research and meets three objectives: it is consistent with prior literature, it provides a nominal process model for doing DS research, and it provides a mental model for presenting and evaluating DS research in IS. The DS process includes six steps: problem identification and motivation, definition of the objectives for a solution, design and development, demonstration, evaluation, and communication. We demonstrate and evaluate the methodology by presenting four case studies in terms of the DSRM, including cases that present the design of a database to support health assessment methods, a software reuse measure, an Internet video telephony application, and an IS planning method. The designed methodology effectively satisfies the three objectives and has the potential to help aid the acceptance of DS research in the IS discipline.},
	number = {3},
	urldate = {2024-05-10},
	journal = {Journal of Management Information Systems},
	author = {Peffers, Ken and Tuunanen, Tuure and Rothenberger, Marcus A. and Chatterjee, Samir},
	month = dec,
	year = {2007},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.2753/MIS0742-1222240302},
	keywords = {case study, design science, design science research, design theory, mental model, methodology, process model},
	pages = {45--77},
}

@misc{regan_can_2024,
	title = {Can {Generative} {Agents} {Predict} {Emotion}?},
	url = {http://arxiv.org/abs/2402.04232},
	abstract = {Large Language Models (LLMs) have demonstrated a number of human-like abilities, however the empathic understanding and emotional state of LLMs is yet to be aligned to that of humans. In this work, we investigate how the emotional state of generative LLM agents evolves as they perceive new events, introducing a novel architecture in which new experiences are compared to past memories. Through this comparison, the agent gains the ability to understand new experiences in context, which according to the appraisal theory of emotion is vital in emotion creation. First, the agent perceives new experiences as time series text data. After perceiving each new input, the agent generates a summary of past relevant memories, referred to as the norm, and compares the new experience to this norm. Through this comparison we can analyse how the agent reacts to the new experience in context. The PANAS, a test of affect, is administered to the agent, capturing the emotional state of the agent after the perception of the new event. Finally, the new experience is then added to the agents memory to be used in the creation of future norms. By creating multiple experiences in natural language from emotionally charged situations, we test the proposed architecture on a wide range of scenarios. The mixed results suggests that introducing context can occasionally improve the emotional alignment of the agent, but further study and comparison with human evaluators is necessary. We hope that this paper is another step towards the alignment of generative agents. 1.},
	language = {en},
	urldate = {2024-05-08},
	publisher = {arXiv},
	author = {Regan, Ciaran and Iwahashi, Nanami and Tanaka, Shogo and Oka, Mizuki},
	month = feb,
	year = {2024},
	note = {arXiv:2402.04232 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{li_enhancing_2024,
	title = {Enhancing {Emotional} {Generation} {Capability} of {Large} {Language} {Models} via {Emotional} {Chain}-of-{Thought}},
	url = {http://arxiv.org/abs/2401.06836},
	abstract = {Large Language Models (LLMs) have shown remarkable performance in various emotion recognition tasks, thereby piquing the research community’s curiosity for exploring their potential in emotional intelligence. However, several issues in the field of emotional generation tasks remain unresolved, including human preference alignment and emotional generation assessment. In this paper, we propose the Emotional Chain-of-Thought (ECoT), a plugand-play prompting method that enhances the performance of LLMs on various emotional generation tasks by aligning with human emotional intelligence guidelines. To assess the reliability of ECoT, we propose an automated model-based evaluation method called Emotional Generation Score (EGS). EGS incorporates Goleman’s Emotional Intelligence Theory as a consensus of human experts, providing a new perspective on the evaluation of emotional generation tasks. Extensive experimental results demonstrate the effectiveness of ECoT and EGS. Further, we discuss the promise of LLMs in the field of emotional intelligence and present key insights into the LLMs with the ECoT in emotional generation tasks.},
	language = {en},
	urldate = {2024-05-04},
	publisher = {arXiv},
	author = {Li, Zaijing and Chen, Gongwei and Shao, Rui and Jiang, Dongmei and Nie, Liqiang},
	month = feb,
	year = {2024},
	note = {arXiv:2401.06836 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{zhang_dialoguellm_2024,
	title = {{DialogueLLM}: {Context} and {Emotion} {Knowledge}-{Tuned} {Large} {Language} {Models} for {Emotion} {Recognition} in {Conversations}},
	shorttitle = {{DialogueLLM}},
	url = {http://arxiv.org/abs/2310.11374},
	doi = {10.48550/arXiv.2310.11374},
	abstract = {Large language models (LLMs) and their variants have shown extraordinary efficacy across numerous downstream natural language processing (NLP) tasks, which has presented a new vision for the development of NLP. Despite their remarkable performance in natural language generating (NLG), LLMs lack a distinct focus on the emotion understanding domain. As a result, using LLMs for emotion recognition may lead to suboptimal and inadequate precision. Another limitation of LLMs is that they are typical trained without leveraging multi-modal information. To overcome these limitations, we propose DialogueLLM, a context and emotion knowledge tuned LLM that is obtained by fine-tuning LLaMA models with 13,638 multi-modal (i.e., texts and videos) emotional dialogues. The visual information is considered as the supplementary knowledge to construct high-quality instructions. We offer a comprehensive evaluation of our proposed model on three benchmarking emotion recognition in conversations (ERC) datasets and compare the results against the SOTA baselines and other SOTA LLMs. Additionally, DialogueLLM-7B can be easily trained using LoRA on a 40GB A100 GPU in 5 hours, facilitating reproducibility for other researchers.},
	urldate = {2024-04-27},
	publisher = {arXiv},
	author = {Zhang, Yazhou and Wang, Mengyao and Wu, Youxi and Tiwari, Prayag and Li, Qiuchi and Wang, Benyou and Qin, Jing},
	month = jan,
	year = {2024},
	note = {arXiv:2310.11374 [cs]},
	keywords = {Computer Science - Computation and Language, LLM, approach},
}

@misc{noauthor_unmasking_nodate,
	title = {Unmasking the {Face}: {A} {Guide} to {Recognizing} {Emotions} from {Facial} {Clues} - {Paul} {Ekman}, {Wallace} {V}. {Friesen} - {Google} {Books}},
	url = {https://books.google.at/books?hl=en&lr=&id=TukNoJDgMTUC&oi=fnd&pg=PR3&dq=Ekman,+P.,+%26+Friesen,+W.+V.+Unmasking+the+face.&ots=GXDq8j93fb&sig=4KKk5-RPvPO-AKnJA-A4cS4aOKU&redir_esc=y#v=onepage&q&f=false},
	urldate = {2024-04-27},
	keywords = {display rules},
}

@book{ekman_emotion_2013,
	title = {Emotion in the {Human} {Face}: {Guidelines} for {Research} and an {Integration} of {Findings}},
	isbn = {978-1-4831-4763-5},
	shorttitle = {Emotion in the {Human} {Face}},
	abstract = {Emotion in the Human Face: Guidelines for Research and an Integration of Findings reviews research findings about the link between the face and emotion and provides some guidelines for study of this complicated but intriguing phenomenon. Some of the conceptual ambiguities that have hindered research and the methodological decisions that must be made in planning research on the face and emotion are discussed. How past investigators handled these matters is presented critically, and a set of standards is offered. This book is comprised of 21 chapters and begins with an overview of questions about how the face provides information about emotion, with emphasis on evidence based on scientific research (largely in psychology). The reader is then introduced to conceptual ambiguities and methodological decisions related to research on the face-emotion connection (including sampling), along with some important research findings. In particular, emotion categories and dimensions that observers can judge on the basis of facial behavior are analyzed, and whether such judgments can be accurate. The similarities and differences in facial behavior across cultures are also considered, along with the relative contribution of facial behavior and contextual information to the judgment of emotion. This monograph is intended primarily for students of psychology, anthropology, ethology, sociology, and biology, as well as those planning or already conducting research on the face.},
	language = {en},
	publisher = {Elsevier},
	author = {Ekman, Paul and Friesen, Wallace V. and Ellsworth, Phoebe},
	month = oct,
	year = {2013},
	note = {Google-Books-ID: dOFFBQAAQBAJ},
	keywords = {Psychology / General, Psychology / Physiological Psychology},
}

@misc{noauthor_vadersentimentvadersentimentvader_lexicontxt_nodate,
	title = {{vaderSentiment}/{vaderSentiment}/vader\_lexicon.txt at master · cjhutto/{vaderSentiment}},
	url = {https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vader_lexicon.txt},
	abstract = {VADER Sentiment Analysis. VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social ...},
	language = {en},
	urldate = {2024-04-20},
	journal = {GitHub},
}

@misc{tymann_karstenamfgervader_2024,
	title = {{KarstenAMF}/{GerVADER}},
	copyright = {MIT},
	url = {https://github.com/KarstenAMF/GerVADER},
	abstract = {GerVADER - A German adaptation of the VADER sentiment analysis tool for social media texts},
	urldate = {2024-04-20},
	author = {Tymann, Karsten Michael},
	month = apr,
	year = {2024},
	note = {original-date: 2019-06-05T22:36:20Z},
}

@inproceedings{kosti_emotic_2017,
	address = {Honolulu, HI, USA},
	title = {{EMOTIC}: {Emotions} in {Context} {Dataset}},
	isbn = {978-1-5386-0733-6},
	shorttitle = {{EMOTIC}},
	url = {http://ieeexplore.ieee.org/document/8015019/},
	doi = {10.1109/CVPRW.2017.285},
	abstract = {Recognizing people’s emotions from their frame of reference is very important in our everyday life. This capacity helps us to perceive or predict the subsequent actions of people, interact effectively with them and to be sympathetic and sensitive toward them. Hence, one should expect that a machine needs to have a similar capability of understanding people’s feelings in order to correctly interact with humans. Current research on emotion recognition has focused on the analysis of facial expressions. However, recognizing emotions requires also understanding the scene in which a person is immersed. The unavailability of suitable data to study such a problem has made research in emotion recognition in context difﬁcult. In this paper, we present the EMOTIC database (from EMOTions In Context), a database of images with people in real environments, annotated with their apparent emotions. We deﬁned an extended list of 26 emotion categories to annotate the images, and combined these annotations with three common continuous dimensions: Valence, Arousal, and Dominance. Images in the database are annotated using the Amazon Mechanical Turk (AMT) platform. The resulting set contains 18, 313 images with 23, 788 annotated people. The goal of this paper is to present the EMOTIC database, detailing how it was created and the information available. We expect this dataset can help to open up new horizons on creating systems able of recognizing rich information about people’s apparent emotional states.},
	language = {en},
	urldate = {2024-04-20},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	publisher = {IEEE},
	author = {Kosti, Ronak and Alvarez, Jose M. and Recasens, Adria and Lapedriza, Agata},
	month = jul,
	year = {2017},
	pages = {2309--2317},
}

@misc{xenos_vllms_2024,
	title = {{VLLMs} {Provide} {Better} {Context} for {Emotion} {Understanding} {Through} {Common} {Sense} {Reasoning}},
	url = {http://arxiv.org/abs/2404.07078},
	abstract = {Recognising emotions in context involves identifying the apparent emotions of an individual, taking into account contextual cues from the surrounding scene. Previous approaches to this task have involved the design of explicit scene-encoding architectures or the incorporation of external scene-related information, such as captions. However, these methods often utilise limited contextual information or rely on intricate training pipelines. In this work, we leverage the groundbreaking capabilities of Vision-and-Large-Language Models (VLLMs) to enhance in-context emotion classification without introducing complexity to the training process in a two-stage approach. In the first stage, we propose prompting VLLMs to generate descriptions in natural language of the subject's apparent emotion relative to the visual context. In the second stage, the descriptions are used as contextual information and, along with the image input, are used to train a transformer-based architecture that fuses text and visual features before the final classification task. Our experimental results show that the text and image features have complementary information, and our fused architecture significantly outperforms the individual modalities without any complex training methods. We evaluate our approach on three different datasets, namely, EMOTIC, CAER-S, and BoLD, and achieve state-of-the-art or comparable accuracy across all datasets and metrics compared to much more complex approaches. The code will be made publicly available on github: https://github.com/NickyFot/EmoCommonSense.git},
	urldate = {2024-04-20},
	publisher = {arXiv},
	author = {Xenos, Alexandros and Foteinopoulou, Niki Maria and Ntinou, Ioanna and Patras, Ioannis and Tzimiropoulos, Georgios},
	month = apr,
	year = {2024},
	note = {arXiv:2404.07078 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction, LLM, emotion classification},
}

@misc{li_culture-gen_2024,
	title = {{CULTURE}-{GEN}: {Revealing} {Global} {Cultural} {Perception} in {Language} {Models} through {Natural} {Language} {Prompting}},
	shorttitle = {{CULTURE}-{GEN}},
	url = {http://arxiv.org/abs/2404.10199},
	abstract = {As the utilization of large language models (LLMs) has proliferated worldwide, it is crucial for them to have adequate knowledge and fair representation for diverse global cultures. In this work, we uncover culture perceptions of three SOTA models on 110 countries and regions on 8 culture-related topics through culture-conditioned generations, and extract symbols from these generations that are associated to each culture by the LLM. We discover that culture-conditioned generation consist of linguistic "markers" that distinguish marginalized cultures apart from default cultures. We also discover that LLMs have an uneven degree of diversity in the culture symbols, and that cultures from different geographic regions have different presence in LLMs' culture-agnostic generation. Our findings promote further research in studying the knowledge and fairness of global culture perception in LLMs. Code and Data can be found in: https://github.com/huihanlhh/Culture-Gen/},
	urldate = {2024-04-20},
	publisher = {arXiv},
	author = {Li, Huihan and Jiang, Liwei and Dziri, Nouha and Ren, Xiang and Choi, Yejin},
	month = apr,
	year = {2024},
	note = {arXiv:2404.10199 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, emotion signal: culture, emotion signals},
}

@misc{de_giorgis_efo_2024,
	title = {{EFO}: the {Emotion} {Frame} {Ontology}},
	shorttitle = {{EFO}},
	url = {http://arxiv.org/abs/2401.10751},
	abstract = {Emotions are a subject of intense debate in various disciplines. Despite the proliferation of theories and definitions, there is still no consensus on what emotions are, and how to model the different concepts involved when we talk about - or categorize - them. In this paper, we propose an OWL frame-based ontology of emotions: the Emotion Frames Ontology (EFO). EFO treats emotions as semantic frames, with a set of semantic roles that capture the different aspects of emotional experience. EFO follows pattern-based ontology design, and is aligned to the DOLCE foundational ontology. EFO is used to model multiple emotion theories, which can be cross-linked as modules in an Emotion Ontology Network. In this paper, we exemplify it by modeling Ekman's Basic Emotions (BE) Theory as an EFO-BE module, and demonstrate how to perform automated inferences on the representation of emotion situations. EFO-BE has been evaluated by lexicalizing the BE emotion frames from within the Framester knowledge graph, and implementing a graph-based emotion detector from text. In addition, an EFO integration of multimodal datasets, including emotional speech and emotional face expressions, has been performed to enable further inquiry into crossmodal emotion semantics.},
	urldate = {2024-04-20},
	publisher = {arXiv},
	author = {De Giorgis, Stefano and Gangemi, Aldo},
	month = jan,
	year = {2024},
	note = {arXiv:2401.10751 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Symbolic Computation, Inbox},
}

@misc{valmeekam_planbench_2023,
	title = {{PlanBench}: {An} {Extensible} {Benchmark} for {Evaluating} {Large} {Language} {Models} on {Planning} and {Reasoning} about {Change}},
	shorttitle = {{PlanBench}},
	url = {http://arxiv.org/abs/2206.10498},
	doi = {10.48550/arXiv.2206.10498},
	abstract = {Generating plans of action, and reasoning about change have long been considered a core competence of intelligent agents. It is thus no surprise that evaluating the planning and reasoning capabilities of large language models (LLMs) has become a hot topic of research. Most claims about LLM planning capabilities are however based on common sense tasks-where it becomes hard to tell whether LLMs are planning or merely retrieving from their vast world knowledge. There is a strong need for systematic and extensible planning benchmarks with sufficient diversity to evaluate whether LLMs have innate planning capabilities. Motivated by this, we propose PlanBench, an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change. PlanBench provides sufficient diversity in both the task domains and the specific planning capabilities. Our studies also show that on many critical capabilities-including plan generation-LLM performance falls quite short, even with the SOTA models. PlanBench can thus function as a useful marker of progress of LLMs in planning and reasoning.},
	urldate = {2024-03-18},
	publisher = {arXiv},
	author = {Valmeekam, Karthik and Marquez, Matthew and Olmo, Alberto and Sreedharan, Sarath and Kambhampati, Subbarao},
	month = nov,
	year = {2023},
	note = {arXiv:2206.10498 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{nalis_not_2023,
	address = {New York, NY, USA},
	series = {{UMAP} '23 {Adjunct}},
	title = {Not {Facial} {Expression}, nor {Fingerprint} – {Acknowledging} {Complexity} and {Context} in {Emotion} {Research} for {Human}-{Centered} {Personalization} and {Adaptation}},
	isbn = {978-1-4503-9891-6},
	url = {https://dl.acm.org/doi/10.1145/3563359.3596990},
	doi = {10.1145/3563359.3596990},
	abstract = {While research on emotion has emerged as a crucial area in studying this relationship, the use of classical psychological concepts in human emotion detection and sentiment analysis has been challenged by the cognitive sciences and psychology. This paper argues that the uncritical adoption of concepts that overlook the complexity and context of emotions may hinder progress in this field. To overcome this limitation, the theory of constructed emotion is reviewed, which suggests that emotions are not distinct categories but rather dimensions that require dynamic, rather than static, contextualized models. By prioritizing digital wellbeing in emotion studies and acknowledging complexity and context, future research can develop more effective models for emotion detection and sentiment analysis. The aim is to provide valuable insights for researchers seeking to advance our understanding of the relationship between technology and wellbeing for human centered-adaptation and personalization.},
	urldate = {2024-03-23},
	booktitle = {Adjunct {Proceedings} of the 31st {ACM} {Conference} on {User} {Modeling}, {Adaptation} and {Personalization}},
	publisher = {Association for Computing Machinery},
	author = {Nalis, Irina and Neidhardt, Julia},
	month = jun,
	year = {2023},
	keywords = {cognitive science, emotion detection, interdisciplinarity, sentiment analysis},
	pages = {325--330},
}

@article{ekman_emotions_2004,
	title = {Emotions revealed},
	volume = {328},
	copyright = {© BMJ Publishing Group Ltd 2004},
	issn = {1756-1833},
	url = {https://www.bmj.com/content/328/Suppl_S5/0405184},
	doi = {10.1136/sbmj.0405184},
	abstract = {{\textless}p{\textgreater}In the second of two articles, \textbf{Paul Ekman} discusses how recognising your own emotions can help you communicate better{\textless}/p{\textgreater}},
	language = {en},
	number = {Suppl S5},
	urldate = {2024-03-23},
	journal = {BMJ},
	author = {Ekman, Paul},
	month = may,
	year = {2004},
	note = {Publisher: British Medical Journal Publishing Group
Section: Student},
	pages = {0405184},
}

@article{khare_emotion_2024,
	title = {Emotion recognition and artificial intelligence: {A} systematic review (2014–2023) and research recommendations},
	volume = {102},
	issn = {1566-2535},
	shorttitle = {Emotion recognition and artificial intelligence},
	url = {https://www.sciencedirect.com/science/article/pii/S1566253523003354},
	doi = {10.1016/j.inffus.2023.102019},
	abstract = {Emotion recognition is the ability to precisely infer human emotions from numerous sources and modalities using questionnaires, physical signals, and physiological signals. Recently, emotion recognition has gained attention because of its diverse application areas, like affective computing, healthcare, human–robot interactions, and market research. This paper provides a comprehensive and systematic review of emotion recognition techniques of the current decade. The paper includes emotion recognition using physical and physiological signals. Physical signals involve speech and facial expression, while physiological signals include electroencephalogram, electrocardiogram, galvanic skin response, and eye tracking. The paper provides an introduction to various emotion models, stimuli used for emotion elicitation, and the background of existing automated emotion recognition systems. This paper covers comprehensive searching and scanning of well-known datasets followed by design criteria for review. After a thorough analysis and discussion, we selected 142 journal articles using PRISMA guidelines. The review provides a detailed analysis of existing studies and available datasets of emotion recognition. Our review analysis also presented potential challenges in the existing literature and directions for future research.},
	urldate = {2024-03-18},
	journal = {Information Fusion},
	author = {Khare, Smith K. and Blanes-Vidal, Victoria and Nadimi, Esmaeil S. and Acharya, U. Rajendra},
	month = feb,
	year = {2024},
	keywords = {Artificial intelligence, Deep learning, Electrocardiogram, Electroencephalogram, Emotion recognition, Eye tracking, Facial images, Galvanic skin response, Machine learning, Speech},
	pages = {102019},
}

@article{barrett_context_2022,
	title = {Context reconsidered: {Complex} signal ensembles, relational meaning, and population thinking in psychological science.},
	volume = {77},
	copyright = {http://www.apa.org/pubs/journals/resources/open-access.aspx},
	issn = {1935-990X, 0003-066X},
	shorttitle = {Context reconsidered},
	url = {https://doi.apa.org/doi/10.1037/amp0001054},
	doi = {10.1037/amp0001054},
	abstract = {Psychological science is a set of ideas and practices conditioned on assumptions about what a mind is and how mental events are caused. These assumptions reverberate much further, inﬂuencing medicine, education, industry, and other aspects of public life. This article uses three well-trod methodological debates about emotional expressions as a lens to challenge a particular set of assumptions and consider an alternative that might improve the validity and robustness of psychological science.},
	language = {en},
	number = {8},
	urldate = {2024-04-19},
	journal = {American Psychologist},
	author = {Barrett, Lisa Feldman},
	month = nov,
	year = {2022},
	pages = {894--920},
}

@article{barrett_context_2011,
	title = {Context in {Emotion} {Perception}},
	volume = {20},
	issn = {0963-7214},
	url = {https://doi.org/10.1177/0963721411422522},
	doi = {10.1177/0963721411422522},
	abstract = {We review recent work demonstrating consistent context effects during emotion perception. Visual scenes, voices, bodies, other faces, cultural orientation, and even words shape how emotion is perceived in a face, calling into question the still-common assumption that the emotional state of a person is written on and can be read from the face like words on a page. Incorporating context during emotion perception appears to be routine, efficient, and, to some degree, automatic. This evidence challenges the standard view of emotion perception represented in psychology texts, in the cognitive neuroscience literature, and in the popular media and points to a necessary change in the basic paradigm used in the scientific study of emotion perception.},
	language = {en},
	number = {5},
	urldate = {2024-03-22},
	journal = {Current Directions in Psychological Science},
	author = {Barrett, Lisa Feldman and Mesquita, Batja and Gendron, Maria},
	month = oct,
	year = {2011},
	note = {Number: 5
Publisher: SAGE Publications Inc},
	pages = {286--290},
}

@article{hoemann_assessing_2022,
	title = {Assessing the {Power} of {Words} to {Facilitate} {Emotion} {Category} {Learning}},
	volume = {3},
	issn = {2662-2041, 2662-205X},
	url = {https://link.springer.com/10.1007/s42761-021-00084-4},
	doi = {10.1007/s42761-021-00084-4},
	abstract = {Previous research suggests that labels shape the categorization of emotional stimuli such as facial configurations, yet the strongest evidence of labels’ influence on category learning comes from work on object categories. In particular, Lupyan et al. (Psychol Sci 18(12):1077–1083, 2007) found that novel categories of aliens were learned faster by participants provided with nonsense labels during feedback. We summarize a series of five studies in which we examined whether this wordenhancement effect on learning would extend to novel categories of emotion. These studies were conceptual replications of the paradigm used by Lupyan et al. (Psychol Sci 18(12):1077–1083, 2007) designed so that participants would associate novel expressive behaviors with situated experiences. We hypothesized that participants would learn to categorize exemplars of novel emotion categories over the duration of the task, and that categorization would be facilitated for participants who were presented with category labels during learning. We simultaneously analyzed data from all five studies in an integrative data analysis, allowing us to test the effects of learning over time and label condition with increased statistical power. Across all five studies, we found that, while participant performance did improve over time, in no case was it facilitated by including emotion labels at feedback. These results join others in suggesting that the effect of labels on emotion categorization may be more context-dependent than previously supposed—varying by the type of category learning task as well as the specific categories being learned and their relationship to previously acquired knowledge—such that there may be multiple pathways for emotion category learning.},
	language = {en},
	number = {1},
	urldate = {2024-03-15},
	journal = {Affective Science},
	author = {Hoemann, Katie and Gendron, Maria and Barrett, Lisa Feldman},
	month = mar,
	year = {2022},
	pages = {69--80},
}

@misc{noauthor_apa_nodate,
	title = {{APA} {PsycNet} {FullTextHTML} page},
	url = {https://psycnet.apa.org/fulltext/2023-19650-004.html},
	urldate = {2024-04-19},
}

@article{alslaity_machine_2024,
	title = {Machine learning techniques for emotion detection and sentiment analysis: current state, challenges, and future directions},
	volume = {43},
	issn = {0144-929X},
	shorttitle = {Machine learning techniques for emotion detection and sentiment analysis},
	url = {https://doi.org/10.1080/0144929X.2022.2156387},
	doi = {10.1080/0144929X.2022.2156387},
	abstract = {Emotion detection and Sentiment analysis techniques are used to understand polarity or emotions expressed by people in many cases, especially during interactive systems use. Recognizing users’ emotions is an important topic for human–computer interaction. Computers that recognize emotions would provide more natural interactions. Also, emotion detection helps design human-centred systems that provide adaptable behaviour change interventions based on users’ emotions. The growing capability of machine learning to analyze big data and extract emotions therein has led to a surge in research in this domain. With this increased attention, it becomes essential to investigate this research area and provide a comprehensive review of the current state. In this paper, we conduct a systematic review of 123 papers on machine learning-based emotion detection to investigate research trends along many themes, including machine learning approaches, application domain, data, evaluation, and outcome. The results demonstrate: 1) increasing interest in this domain, 2) supervised machine learning (namely, SVM and Naïve Bayes) are the most popular algorithms, 3) Text datasets in the English language are the most common data source, and 4) most research use Accuracy to evaluate performance. Based on the findings, we suggest future directions and recommendations for developing human-centred systems.},
	number = {1},
	urldate = {2024-03-23},
	journal = {Behaviour \& Information Technology},
	author = {Alslaity, Alaa and Orji, Rita},
	month = jan,
	year = {2024},
	note = {Number: 1
Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/0144929X.2022.2156387},
	keywords = {Emotion detection, deep learning, emotion recognition, machine learning, opinion mining, sentiment analysis},
	pages = {139--164},
}
